[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Creative Coding with Pure Data",
    "section": "",
    "text": "Preface\nThis book was born out of the classroom—but it does not stay there. It is the result of years of teaching creative coding and interactive media at public universities, engaging with students from a wide range of disciplines, backgrounds, and interests. Over time, a common thread emerged: the desire to bridge artistic expression and technical skill, to write code not just as a means to an end, but as a space of exploration, experimentation, and play.\nIn these pages, you’ll find the distilled insights, exercises, and creative strategies that have shaped countless workshops and academic programs. The goal has always been twofold: to equip readers with the tools to build interactive digital systems, and to nurture a mindset where sound, movement, code, and structure can be explored as artistic materials. Whether it’s a generative soundscape, a data-driven artwork, or a custom tool built from scratch, the projects in this book are designed to foster technical growth while encouraging individual expression.\nThis is not a manual in the traditional sense, nor is it a fixed curriculum. Instead, this book invites you to engage with Pure Data on your own terms, navigating its chapters in whatever order best suits your curiosity. The modular structure is intentional: it supports creative detours, sudden insights, and unexpected connections between ideas. You are encouraged to experiment, remix, and stretch the boundaries of the examples presented. Treat the book as both a guide and a sandbox—one where art, sound, code, and interactivity come alive through your engagement.\nWhether you’re an artist exploring new tools, a programmer seeking creative outlets, or a student diving into the world of interactive media, I hope this book helps you discover how code can become a language for imagination.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-am-i",
    "href": "index.html#who-am-i",
    "title": "Creative Coding with Pure Data",
    "section": "Who am I?",
    "text": "Who am I?\nMy name is Ezequiel Abregú, and I am a sound artist, composer, multi-instrumentalist, and researcher originally from Buenos Aires, Argentina. My artistic practice encompasses sound recordings, audio installations, performances, sound sculptures, sound design, and compositions for chamber music, choreography, and theater. I am particularly interested in the interplay between music, performance, sound art, live electronics, auditory and visual perception, interactive media, and the application of technology in art.\n\n\n\n\nDr. Ezequiel Abregú\n\n\n\nI hold a Ph.D. focusing on the relationship between visual and auditory perception in sound art, and a degree in Composition with Electroacoustic Media from the National University of Quilmes (UNQ). My academic career includes teaching positions at several institutions: I am a professor at the University of Quilmes (Computing Applied to Music area), the National University of Arts (Multimedia Arts area), and the University of Tres de Febrero (Electronic Arts area).\nMy passion for programming and digital audio applications has led me to explore various programming languages and tools over the past two decades, including C, C++, Python, and Pure Data. I am an advocate of the open-source philosophy, regularly working with Linux and sharing my projects in publicly accessible repositories. My technical expertise extends to hardware development using microcontrollers and single-board computers, enabling me to adopt a hands-on approach in both my artistic and research endeavors.\n\n\n\n\n\n\nMore information about my work can be found on my personal website ezequielabregu.net.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "Creative Coding with Pure Data",
    "section": "What is this book about?",
    "text": "What is this book about?\nThe goal of this book is to support undergraduate and postgraduate students in exploring the intersection of creativity and technology alongside peers from diverse backgrounds. Building on years of teaching experience at several public universities1, this work encourages the integration of a creative mindset with programming skills to design original tools, algorithms, and artworks. Through this synthesis, the book invites students to engage with sound, interactivity, and control protocols in both technical and expressive dimensions.\nRather than offering a fixed, linear progression, the structure of this book is deliberately open and modular. Readers are encouraged to navigate the content according to their interests, needs, or curiosity. This flexibility supports an experimental approach to learning, where exploration and play are not only welcome but essential.\nBy approaching programming as a creative practice, this book invites you to think, make, and reflect through code. Whether you’re building an interactive sound installation, prototyping a digital instrument, or simply experimenting with new ideas, the goal is to empower you with the tools and concepts to express yourself in the digital domain—and to enjoy the process along the way.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Creative Coding with Pure Data",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book is intended for anyone interested in learning about creative coding using Pure Data. It is suitable for beginners who are new to programming and want to explore the world of interactive media, as well as for experienced programmers looking to expand their skills in sound synthesis and data visualization.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-you-going-to-learn",
    "href": "index.html#what-are-you-going-to-learn",
    "title": "Creative Coding with Pure Data",
    "section": "What are you going to learn?",
    "text": "What are you going to learn?\nIn this book, you will learn how to use Pure Data to create interactive audio-visual projects. You will explore various techniques for sound synthesis, data visualization, and interactive installations. The book will also cover best practices for organizing and managing your Pd projects, as well as tips for debugging and optimizing your code.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-you-dont-going-to-learn",
    "href": "index.html#what-are-you-dont-going-to-learn",
    "title": "Creative Coding with Pure Data",
    "section": "What are you don’t going to learn?",
    "text": "What are you don’t going to learn?\nThis book is not intended to be a comprehensive guide to all aspects of Pure Data. Instead, it focuses on specific topics and projects that are relevant to creative coding. While the book will cover a wide range of techniques and concepts, it will not delve into every detail or aspect of Pure Data. This book is not a substitute for the official Pure Data documentation or other resources. It is meant to complement these resources and provide a practical, hands-on approach to learning Pure Data.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#a-work-in-progress",
    "href": "index.html#a-work-in-progress",
    "title": "Creative Coding with Pure Data",
    "section": "A Work in Progress",
    "text": "A Work in Progress\nBefore we delve deeper into the subject, let it be clear from the outset:\n\n\nThis book is, by design, a work in progress.\n\n\nThe intention is not to present a definitive compendium or a closed collection of recipes. Instead, what I offer here is a growing body of materials, case studies, and conceptual tools that evolve in parallel with the creative and technical challenges faced by artists and researchers working at the intersection of code, sound, and interactivity. The choice of Pure Data as the primary platform reflects both its openness and its suitability for rapid prototyping and conceptual clarity. But beyond that, this book aims to serve as a framework for thinking, not just for coding.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#from-problem-to-algorithm",
    "href": "index.html#from-problem-to-algorithm",
    "title": "Creative Coding with Pure Data",
    "section": "From Problem to Algorithm",
    "text": "From Problem to Algorithm\nOne of the central ideas guiding this book is that programming begins with a problem, not with a tool. In each chapter and example, we begin by carefully defining a problem: this might be technical (how to generate a sequence?), aesthetic (how to modulate a tone?), or conceptual (how do oscillators shape our understanding of time?). From there, we proceed to analyze the problem, understand its inner structure, and design an algorithmic strategy to address it.\nOnly then do we concern ourselves with the details of implementation. The choice of programming language—Pure Data in this case—is important, but always secondary. The clarity of thought and the logic of the algorithm come first. In this spirit, we encourage you to reflect not just on what a patch does, but on why it works the way it does, and how it might be extended, modified, or reimagined.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#backend-first-leaving-the-gui-for-later",
    "href": "index.html#backend-first-leaving-the-gui-for-later",
    "title": "Creative Coding with Pure Data",
    "section": "Backend First: Leaving the GUI for Later",
    "text": "Backend First: Leaving the GUI for Later\nAnother important methodological decision in this book is our emphasis on the backend. We begin by focusing on signal flow, algorithmic thinking, and control structures—those components that shape the inner workings of a patch. In many cases, the graphical interface (GUI) can be a distraction from the deeper mechanics at play.\nBy concentrating first on backend logic, we build a strong foundation that can later support more refined user interactions. GUI design will certainly be addressed, but in future chapters, when we are equipped with a clearer understanding of how our systems behave and how we want them to evolve. In other words, we treat the visual layer as a representation of logic, not a replacement for it.\nThis book is an invitation to approach creative coding not as a set of shortcuts or pre-made solutions, but as a process of inquiry. We will get our hands dirty, make mistakes, and revise along the way. In doing so, we learn not just how to build systems, but how to think with and through them. That is the deeper promise of creative code.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Creative Coding with Pure Data",
    "section": "Contributing",
    "text": "Contributing\nIf you would like to contribute to this book, please feel free to fork the repository and submit a pull request. I welcome any suggestions, corrections, or improvements to the content. You can also report issues or request features by opening an issue in the repository. You can find the source code for this book on GitHub.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Creative Coding with Pure Data",
    "section": "Contact",
    "text": "Contact\nIf you have any questions, comments, or feedback about this book, please feel free to reach out to me at eabregu.dev@gmail.com. I would love to hear from you!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Creative Coding with Pure Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to express my gratitude to the following individuals and organizations for their support and contributions to this book:\n\nPure Data for providing a powerful and flexible platform for creative coding.\nThe Pd community for their invaluable resources, tutorials, and support.\nThe open-source community for their dedication to sharing knowledge and tools for creative coding.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Creative Coding with Pure Data",
    "section": "License",
    "text": "License\nThis book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. You are free to share and adapt the material, provided you give appropriate credit, do not use it for commercial purposes, and distribute your contributions under the same license.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Creative Coding with Pure Data",
    "section": "",
    "text": "Multimedia Arts UNA, Electronic Arts UNTREF, Bachelor of Music and Technology UNQ, Master’s Degree in Sound Art UNQ, Doctorate in Arts UNA↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Pure Data?\nTraditionally, the process of creating software applications has relied heavily on text-based programming languages. Developers would write code manually in text files and execute them later to observe the outcome. While this method is efficient for those trained in programming, it often presents a steep learning curve for artists, musicians, and other creatives without a technical background. The abstract nature of text-based coding can act as a barrier to entry, especially for those more accustomed to working in visual or tactile media.\nPure Data offers a radically different approach by introducing a graphical programming environment tailored to creative exploration. Instead of writing lines of code, users work with visual representations of functions—called objects—that are placed and connected on a canvas. This paradigm allows users to construct interactive programs, known as “patches,” by literally drawing connections between elements. Each object can receive messages that modify its behavior in real time, creating a highly responsive and accessible system for building audio, visual, or multimedia tools.\nThe design of Pure Data echoes the modular synthesis systems of the 20th century, where sound was shaped by routing audio through a network of physical devices linked with patch cables. This historical reference not only grounds Pure Data in a legacy of experimental electronic music but also makes it intuitive for users familiar with analog workflows. By visually “patching” connections, users can simulate and extend these traditional processes in a digital context.\nTo fully engage with the creative potential of Pure Data, one can develop or modify algorithms for digital audio using imaginative coding strategies. For instance, building a real-time audio granulator that reinterprets live microphone input can offer both technical challenge and artistic reward. Through this lens, programming becomes a form of creative expression, blending logical structures with aesthetic decisions. By experimenting with signal flow, modulation techniques, or interactive sensors, users cultivate a mindset that is both inventive and analytical—unlocking new possibilities in digital sound design and performance.\nPure Data (or Pd) is a real-time graphical programming environment for audio, video, and graphical processing. Pure Data is commonly used for live music performance, VeeJaying, sound effects, composition, audio analysis, interfacing with sensors, using cameras, controlling robots or even interacting with websites. Because all of these various media are handled as digital data within the program, many fascinating opportunities for cross-synthesis between them exist. Sound can be used to manipulate video, which could then be streamed over the internet to another computer which might analyze that video and use it to control a motor-driven installation.\nProgramming with Pure Data is a unique interaction that is much closer to the experience of manipulating things in the physical world. The most basic unit of functionality is a box, and the program is formed by connecting these boxes together into diagrams that both represent the flow of data while actually performing the operations mapped out in the diagram. The program itself is always running, there is no separation between writing the program and running the program, and each action takes effect the moment it is completed.\nThe community of users and programmers around Pure Data have created additional functions (called “externals” or “external libraries”) which are used for a wide variety of other purposes, such as video processing, the playback and streaming of MP3s or Quicktime video, the manipulation and display of 3-dimensional objects and the modeling of virtual physical objects. There is a wide range of external libraries available which give Pure Data additional features. Just about any kind of programming is feasible using Pure Data as long as there are externals libraries which provide the most basic units of functionality required.\nThe core of Pure Data written and maintained by Miller S. Puckette and includes the work of many developers, making the whole package very much a community effort.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#why-pure-data",
    "href": "chapters/introduction.html#why-pure-data",
    "title": "1  Introduction",
    "section": "1.2 Why Pure Data?",
    "text": "1.2 Why Pure Data?\nPure Data (Pd) is a powerful, open-source environment for creative coding, offering a uniquely visual approach to programming that is especially suited for artists, musicians, and interactive media designers. Its intuitive interface and modular structure make it a flexible and accessible tool for developing real-time audio and visual projects—from live performances to experimental installations.\nOne of Pure Data’s standout features is its graphical programming interface, which replaces traditional lines of code with visual objects and patch cords. This allows users to construct complex behaviors by connecting elements on screen, making it easier to prototype and refine creative ideas. Real-time processing capabilities mean that audio and visual data can be generated, modified, and responded to instantly—ideal for performances, generative art, or interactive systems that react to sensors or user input.\nBeyond its creative potential, Pure Data offers practical advantages: it is cross-platform, running on Windows, macOS, and Linux, and integrates easily with tools like Arduino, Raspberry Pi, and Max/MSP, enabling hybrid systems that combine digital and physical components. A rich ecosystem of external libraries supports advanced functions like synthesis, visualization, and computer vision. Its open-source nature encourages exploration and collaboration, with a supportive community, extensive documentation, and countless tutorials available. Because it’s free and widely used in education, Pd is not only an effective tool for artistic expression but also a valuable learning resource for developing a strong foundation in programming and multimedia design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#what-is-creative-coding",
    "href": "chapters/introduction.html#what-is-creative-coding",
    "title": "1  Introduction",
    "section": "1.3 What is Creative Coding?",
    "text": "1.3 What is Creative Coding?\nCreative coding is the practice of using programming as a tool for artistic expression. It transforms code from a purely functional medium into a creative one, enabling the development of visual art, music, interactive experiences, and experimental media. This approach encourages artists, designers, and technologists to explore beyond the limits of traditional art forms, embracing code as a flexible and dynamic means of invention and communication.\nThe applications of creative coding are diverse, ranging from generative visuals and algorithmic design to responsive installations and live audiovisual performances. Creators often use languages and environments specifically designed to support creative work, such as Processing, OpenFrameworks, Max/MSP, and Pure Data. These platforms make it easier to manipulate data, control media in real time, and experiment with unconventional interfaces and outputs.\nImportantly, creative coding transcends the boundaries of specific disciplines. It can intersect with visual art, music, dance, theater, architecture, and even narrative writing. What unites these practices is the use of code as an expressive tool—one that invites innovation, play, and conceptual exploration. Closely tied to the values of the open-source movement, creative coding thrives in a culture of sharing, where artists and developers freely exchange code, tutorials, and ideas. This collaborative ecosystem fosters continuous learning and reinvention, empowering creators to expand what is possible through digital technology.\nFor an in-depth exploration of creative coding, consider checking out these resources:\n\nAwesome Creative coding - A curated list of resources, libraries, and tools for creative coding.\nCreative Code Berlin - A collection of creative coding resources, tutorials, and projects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#getting-started",
    "href": "chapters/introduction.html#getting-started",
    "title": "1  Introduction",
    "section": "1.4 Getting Started",
    "text": "1.4 Getting Started\nThis section will guide you through the installation of Pure Data Vanilla, the official and minimal distribution of Pure Data, on Windows, macOS, and Linux systems. The installation process is straightforward and will have you up and running in no time.\n\n1.4.1 Pre-requisites\nTo get the most out of this book, you should have a basic understanding of programming and some familiarity with the Pure Data language.\nIf you are new to Pure Data or programming, there are several free online resources that can help you get started:\n\nPure Data FLOSS Manual – A beginner-friendly and comprehensive guide to Pure Data.\nOfficial Pure Data Documentation – The official manuals and reference materials for Pure Data.\nMiller Puckette’s “Theory and Techniques of Electronic Music” – A comprehensive book by the creator of Pure Data, covering both theory and practical techniques.\nProgramming Electronic Music in Pd - Designed for self-study, principally for composers. It begins with explanations of basic programming and acoustic principles then gradually builds up to the most advanced electronic music processing techniques.\nKunstuniversität Graz / Pure Data – An extensive collection of tutorials and resources for learning Pure Data, including tutorials and example patches.\nPatchstorage - A community-driven platform for sharing and discovering Pure Data patches. It features a wide range of projects, from simple examples to complex installations, and serves as a valuable resource for learning and inspiration.\n\nThese resources can be consulted before or alongside this book to strengthen your foundational knowledge.\n\n\n1.4.2 Recommended Pure Data distributions\nThis book is based on Pure Data Vanilla distribution, which is the most widely used version. You can download it from the PD official website. This version is maintained by the original author, Miller Puckette1, and is the most stable supported version of Pure Data. It includes all the core features and libraries needed for most projects, making it a great starting point for beginners and experienced users alike. Pure Data Vanilla is a free, open-source and available for Windows, macOS, and Linux operating systems.\nIn addition to Pure Data Vanilla, there are several other distributions and versions of Pure Data that you may find useful:\n\nPurr Data – A fork of Pd-l2ork that focuses on usability and accessibility, with a more polished interface and additional features. Purr Data serves the same purpose, but offers a new and much improved graphical user interface and includes many 3rd party plug-ins. Like Pd, it runs on Linux, macOS and Windows, and is open-source throughout.\nPlugdata – plugdata is a plugin wrapper designed for Miller Puckette’s Pure Data (Pd), featuring an enhanced graphical user interface (GUI) created using JUCE, headed by Timothy Schoen. this project is still a work in progress, and may still have some bugs. By default, plugdata comes with the cyclone and ELSE collections of externals and abstractions.\nPD Ceammc - a general purpose Pd distribution and library that is used for performance, sound-design and education purposes in Centre of electroacoustic music of Moscow Conservatory (CEAM).\n\n\n\n1.4.3 Recommended libraries and externals\nThe following libraries are recommended to be used with Pure Data. They are not included in the Pure Data Vanilla distribution, but they can be easily installed and used with it:\n\n\n\n\n\n\n\nLibrary\nDescription & Usefulness\n\n\n\n\ncyclone\nA large collection of Max/MSP-compatible objects. Useful for porting Max patches and advanced DSP tasks.\n\n\nELSE\nExtensive library for audio, control, math, and UI\n\n\niemlib\nCollection of general purpose objects and filters for Pure Data\n\n\nlist-abs\nTools for advanced list manipulation and data processing\n\n\nzexy\nEssential utilities for math, signal, and control operations. Adds many missing core features.\n\n\nGem\nGraphics Environment for Multimedia. Enables real-time visuals and video.\n\n\nceammc\nlibrary used for work and education purposes in Centre of electroacoustic music of Moscow Conservatory (CEAMMC) and ZIL-electro studio\n\n\ncomport\nSerial port communication. Useful for connecting Pd to Arduino, sensors, and hardware\n\n\nmrpeach\nNetworking and OSC support. Essential for interactive and networked Pd projects\n\n\nfreeverb~\nHigh-quality reverb effect. Simple way to add spatialization to audio patches\n\n\njmmmp\nCollection of GUI and utility objects. Enhances user interface design in Pd patches\n\n\nmapping\nTools for mapping data (e.g., sensors to sound). Useful for interactive installations and performances\n\n\n\nThese libraries provide additional objects and features that can enhance your projects and make it easier to work with sound synthesis, data visualization, and interactive installations.\n\n\n1.4.4 Installing Pure Data Vanilla\nThe following is a step-by-step installation of Pure Data Vanilla, the official and minimal distribution of Pure Data, on Windows, macOS, and Linux systems.\n\n\n1.4.5 Installing on Windows\n\n1.4.5.1 Download the Installer\n\nVisit the official Pure Data website: https://puredata.info/downloads/pure-data\nScroll down to the Windows section.\nClick the latest version link (e.g., pd-0.55-2.windows-installer.exe).\n\n\n\n1.4.5.2 Run the Installer\n\nLocate the downloaded .exe file in your Downloads folder.\nDouble-click to run the installer.\nFollow the installation wizard:\n\nChoose the installation location (default is fine).\nAllow the program to create a Start Menu shortcut.\n\n\n\n\n1.4.5.3 Verify Installation\n\nAfter installation, open Pure Data from the Start Menu or desktop shortcut.\nThe main Pd window should appear with a blank patch ready.\n\n\n\n\n\n1.4.6 Installing on macOS\n\n1.4.6.1 Download the Disk Image\n\nVisit: https://msp.ucsd.edu/software.html\nScroll to the macOS section.\nDownload the .dmg file (e.g., Pd-0.54-1.dmg).\n\n\n\n1.4.6.2 Install the Application\n\nOpen the downloaded .dmg file.\nDrag the Pure Data icon into the Applications folder.\n\n\n\n1.4.6.3 Open Pure Data\n\nOpen your Applications folder.\nRight-click (or Ctrl + click) the Pure Data icon and select Open.\n\nThe first time, macOS may warn that the app is from an unidentified developer.\nConfirm to proceed.\n\n\n\n\n1.4.6.4 Optional: Enable Audio Permissions\n\nIf prompted, allow Pure Data to access the microphone.\nOpen System Preferences &gt; Security & Privacy &gt; Microphone and ensure Pure Data is enabled.\n\n\n\n\n\n1.4.7 Installing on Linux\nPure Data is available in the package repositories of most major Linux distributions. Below are instructions for popular systems.\n\n1.4.7.1 Debian/Ubuntu-based Systems\nsudo apt update\nsudo apt install puredata\n\n\n1.4.7.2 Fedora\nsudo dnf install puredata\n\n\n1.4.7.3 Arch Linux\nsudo pacman -S puredata\n\n\n1.4.7.4 Verify Installation\n\nOpen a terminal and type pd.\nThe Pure Data GUI should launch, displaying a blank patch.\nIf you encounter issues, check your package manager or consult the Pure Data community for troubleshooting.\n\n\n\n\n1.4.8 Installing Externals\nPure Data’s functionality can be extended through the use of externals—additional libraries that provide new objects and features. These externals are created by the Pd community and can add everything from new audio effects to advanced data processing tools. Installing externals is straightforward thanks to Pure Data’s built-in package manager.\n\n1.4.8.1 Step 1: Open Pure Data\n\nLaunch Pure Data (Pd) on your computer.\nMake sure you are using the Vanilla version for best compatibility with the package manager.\n\n\n\n1.4.8.2 Step 2: Access the “Find Externals” Tool\n\nIn the Pure Data menu, go to Help → Find Externals…\nThis opens the Deken package manager, which allows you to search for and install externals directly from within Pd.\n\n\n\n1.4.8.3 Step 3: Search for an External\n\nIn the search bar, type the name of the external or library you want to install (for example, zexy, cyclone, or iemlib).\nPress Enter or click the Search button.\nA list of matching externals will appear, showing available versions for your platform.\n\n\n\n1.4.8.4 Step 4: Install the External\n\nClick on the desired external in the list.\nChoose the version that matches your operating system and Pd version.\nClick Install.\nThe external will be downloaded and placed in your Pd externals folder (typically ~/Documents/Pd/externals on macOS and Linux, or C:\\Users\\&lt;YourName&gt;\\Documents\\Pd\\externals on Windows).\n\n\n\n1.4.8.5 Step 5: Add the External to Your Pd Path (if needed)\n\nMost externals are automatically available after installation.\nIf Pd cannot find the external, you may need to add its folder to Pd’s search path:\n\nGo to Preferences → Path…\nClick New and add the path to the external’s folder (for example, ~/Documents/Pd/externals/cyclone).\nClick OK and restart Pd.\n\n\n\n\n1.4.8.6 Step 6: Use the External in Your Patch\n\nIn your patch, create an object with the name of the external’s library, followed by the object you want to use.\nFor example, to use the counter object from the cyclone library:\n[cyclone/counter]\nSome libraries require you to load them with a special object, such as [declare -lib cyclone] at the top of your patch.\n\n\n\n1.4.8.7 Step 7: Verify Installation\n\nIf the object appears without errors (no red box), the external is installed correctly.\nIf you see errors, double-check the installation path and that you are using the correct object/library name.\n\n\n\n1.4.8.8 Manual Installation (Advanced)\nIf you need to install an external manually:\n\nDownload the external from https://deken.puredata.info/ or the developer’s website.\nExtract the files to your Pd externals folder.\nAdd the folder to Pd’s path as described above.\n\n\nTip: Always check the documentation for each external, as installation steps or requirements may vary.\n\nFor more details, see the official Pure Data documentation on externals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Miller Puckette is a computer music researcher and the creator of Pure Data. He has been involved in the development of various software tools for music and multimedia, including Max/MSP and Pure Data. His work has had a significant impact on the field of computer music and interactive media. More information about Miller Puckette can be found on his website.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html",
    "href": "chapters/playrec.html",
    "title": "2  Rec & Play",
    "section": "",
    "text": "2.1 I’m Sitting in a Room – Alvin Lucier\nIn this chapter, we will explore the relationship between recording and playback. We will look at how these two processes can be used to create new sounds and compositions. We will also discuss the technical aspects of recording and playback, including the equipment and techniques used in the process. We will also consider the artistic implications of recording and playback, including how these processes can be used to create new forms of expression and communication.\nI’m Sitting in a Room (Alvin Lucier) consists of a 15-minute and 23-second sound recording. The piece opens with Lucier’s voice as he declares he is sitting in a room different from ours. His voice trembles slightly as he delivers the text, describing what will unfold over the next 15 minutes:\nAs listeners, we know what is going to happen, but we don’t know how it will happen (Hasse 2012). We listen, following Lucier’s recorded voice. Then, Lucier plays the recording into the room and re-records it. This time, we begin to hear more of the room’s acoustic characteristics. He continues to play back and re-record his voice—over and over—until his speech becomes softened, almost dissolved, into the sonic reflections of the room in which the piece was recorded and re-recorded.\nOne effective way to study a piece is to replicate its technical aspects and the devices involved. The aim of this activity is to recreate the technical setup of I’m Sitting in a Room, focusing on the processes of recording, playback, and automation.\nThere are many ways to implement this technical system. Just to mention two environments we’re currently working with: it’s relatively straightforward in Pure Data.\nIn terms of hardware, besides a computer, you’ll need a speaker (mono audio output), a microphone (mono audio input), and a semi-reverberant room.\nThe system should include at least two manual (non-automated) controls:\nChoose a room whose acoustic or musical qualities you’d like to evoke. Connect the microphone to the input of tape recorder #1. From the output of tape recorder #2, connect to an amplifier and speaker. Use the following text, or any other text of any length:\nThe following steps outline the process:\nAll the recorded generations, presented in chronological order, create a tape composition whose duration is determined by the length of the original statement and the number of generations produced. Make versions in which a single statement is recycled through different rooms. Create versions using one or more speakers in different languages and spaces. Try versions where, for each generation, the microphone is moved to different parts of the room(s). You may also develop versions that can be performed in real time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#im-sitting-in-a-room-alvin-lucier",
    "href": "chapters/playrec.html#im-sitting-in-a-room-alvin-lucier",
    "title": "2  Rec & Play",
    "section": "",
    "text": "“…I am recording the sound of my speaking voice and I am going to play it back into the room again and again…”\n\n\n\n\nI am sitting in a room design\n\n\n\n\n\n\n\n\nStart recording\nStop recording\n\n\n\nI am sitting in a room different from the one you are in now. I am recording the sound of my speaking voice, and I am going to play it back into the room again and again until the resonant frequencies of the room reinforce themselves so that any semblance of my speech, with perhaps the exception of rhythm, is destroyed. What you will hear, then, are the natural resonant frequencies of the room articulated by speech. I regard this activity not so much as a demonstration of a physical fact, but more as a way to smooth out any irregularities my speech might have.\n\n\n\nRecord your voice through the microphone into tape recorder #1.\nRewind the tape, transfer it to tape recorder #2, and play it back into the room through the speaker. - Record a second generation of the statement via the microphone back into tape recorder #1.\nRewind this second generation to the beginning and splice it to the end of the original statement in tape recorder #2.\nPlayback only the second generation into the room and record a third generation into tape recorder #1.\nContinue this process through multiple generations.\n\n\n\n2.1.1 Pure Data implementation of I’m sitting in a room\n \n\n\n\nPure Data implementation of I am sitting in a room\n\n\nIn this section, we will break down the Pure Data patch I-am-sitting-in-a-room.pd step by step. This patch is inspired by Alvin Lucier’s iconic piece and demonstrates how to recursively record and play back sound to reveal the resonant frequencies of a room.\n\n2.1.1.1 Conceptual Overview\nThe patch enables you to:\n\nRecord your voice or any sound in a room.\nPlay back the recording into the room and re-record it.\nRepeat this process, so the room’s resonances become more pronounced with each generation.\n\n\n\n\n2.1.2 System Diagram\nThe following diagram illustrates the recursive nature of the recording and playback process, similar to how Lucier’s piece unfolds. Each generation of tape recorders captures the previous one, creating a feedback loop that emphasizes the room’s resonances.\n\n\n\n\n\nflowchart TB\n  %% Top level source\n  Input[Microphone]\n\n  subgraph PLAYBACK RECORDERS\n    direction LR\n    subgraph RecorderA [DEVICE A]\n      RecordA[Record A.wav]\n      PlayA[Play A.wav]\n      RecordA -.-&gt; PlayA\n    end\n    \n    subgraph RecorderB [DEVICE B]\n      RecordB[Record B.wav]\n      PlayB[Play B.wav]\n      RecordB -.-&gt;PlayB\n    end\n  end\n  \n  %% Output at the bottom  \n  Output((Speaker))\n  \n  %% Clear connections showing signal flow\n  start([1-start]) --&gt; RecordA\n  stop([2-stop]) --&gt; RecordA\n  stop([2-stop]) --&gt; RecordB\n  stop([2-stop]) --&gt; PlayA\n\n  Input ==&gt; RecordA\n  Input ==&gt; RecordB\n\n  PlayA ==&gt; Output\n  PlayB ==&gt; Output\n  \n  %% Feedback path showing recursion\n  PlayB -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordA\n  PlayA -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordB\n  Output ==&gt; room[\"ROOM'S&lt;br&gt;REFLECTIONS\"]:::roomStyle ==&gt; Input\n\n  classDef roomStyle fill:#f5f5f5,stroke:#333,stroke-dasharray:5 5 \n\n\n\n\n\n\n \nThe following sequence diagram illustrates the process of recording and playback in the patch. It shows how the audio input is captured, recorded, played back, and re-recorded in a loop, emphasizing the room’s resonances.\n\n\n\n\n\n\nsequenceDiagram\n    participant Start as Start\n    participant Stop as Stop\n    participant Input as Mic\n    participant RecA as Rec A.wav\n    participant PlayA as Play A.wav\n    participant RecB as Rec B.wav\n    participant PlayB as Play B.wav\n    participant Output as Speaker\n    participant Room as Room\n\n    %% Generation 1\n    activate Input\n    Start--&gt;&gt;RecA: Start button\n    activate RecA\n    Input-&gt;&gt;RecA: Mic input\n\n\n    Stop--xRecA: Stop Rec A  \n    deactivate RecA   \n    Stop--&gt;&gt;RecB: Start Rec B\n    activate RecB \n    Input-&gt;&gt;RecB: Mic input          \n    Stop--&gt;&gt;PlayA: Start Play A\n    activate PlayA\n\n    PlayA-&gt;&gt;Output: Playback A.wav\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n    PlayA--xRecB: Trigger Stop Rec B\n    deactivate RecB\n    deactivate PlayA\n\n    PlayA--&gt;&gt;RecA: Trigger Start Rec B\n    activate RecA\n    Input-&gt;&gt;RecA: Mic input \n    PlayA--&gt;&gt;PlayB: Trigger Start Play A\n    activate PlayB\n\n    %% Generation 2\n    PlayB-&gt;&gt;Output: Playback B.wav\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n\n    %% Generation 3 (process repeats)\n    PlayB--xRecA: Trigger Stop Rec A\n    deactivate RecA\n    deactivate PlayB\n    PlayB--&gt;&gt;RecB: Trigger Start Rec B\n    activate RecB \n    Input-&gt;&gt;RecB: Mic input\n    PlayB--&gt;&gt;PlayA: Trigger Start Play A\n    activate PlayA\n    PlayA-&gt;&gt;Output: Playback\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n\n    Note over Input,Room: The process continues, alternating between devices and reinforcing the room's resonances.\n    deactivate Input\n    deactivate PlayA\n    deactivate RecB\n\n\n\n\n\n\n\n\n\n2.1.3 Step 1: Audio Input and Routing\n\nadc~ receives audio from your microphone.\nThe signal is sent to s~ audio.input, making it available to other parts of the patch.\nThis modular routing allows the same input to be used for both recording and playback chains.\n\n\n\n2.1.4 Step 2: Recording Your Voice (record-A.wav)\n\nPress the Start Recording button (bng labeled “Start Recording”).\nThis triggers a message:\nopen record-A.wav, start → writesf~\nThe incoming audio from your microphone is now being recorded to record-A.wav.\nPress the Stop Recording button (bng labeled “Stop Recording”) to send the stop message, ending the recording.\n\n\n\n2.1.5 Step 3: Playback and Re-recording (record-B.wav)\n\nTo play back your recording, another button triggers:\nopen record-A.wav, start → readsf~\nThe playback signal is routed to:\n\nthrow~ audio (so you hear it through the speakers)\nwritesf~ (recording to record-B.wav)\n\nThis means the playback of your first recording is re-recorded, capturing both the original sound and the room’s response.\n\n\n\n2.1.6 Step 4: Recursive Process\n\nYou can repeat the process:\n\nPlay back record-B.wav and record it again, each time reinforcing the room’s resonances.\n\nEach iteration makes the speech less intelligible and the room’s resonant frequencies more prominent, just like in Lucier’s piece.\n\n\n\n2.1.7 Step 5: Output\n\nAll audio routed to throw~ audio is collected by catch~ audio and sent to dac~ for playback through your speakers or headphones.\n\n\n\n2.1.8 Key Objects Table\n\n\n\nObject\nPurpose\n\n\n\n\nadc~\nAudio input from microphone\n\n\nwritesf~\nRecord audio to a file\n\n\nreadsf~\nPlay audio from a file\n\n\nthrow~/catch~\nMix and route audio signals\n\n\ndac~\nAudio output to speakers\n\n\nbng\nButton for triggering actions\n\n\nmsg\nSend commands to objects (open, start, stop)\n\n\n\n\n\n2.1.9 How to Use the Patch\n\nStart Recording:\nClick the “Start Recording” button and speak or make a sound.\nStop Recording:\nClick the “Stop Recording” button to finish.\nPlayback and Re-record:\nUse the playback button to play your recording into the room and simultaneously re-record it.\nRepeat:\nRepeat the playback and re-recording process as many times as you like to hear the room’s resonances emerge.\n\nThis patch is a practical and creative way to explore acoustic phenomena and the transformation of sound through recursive recording, echoing the spirit of Lucier’s original work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#i-am-sitting-in-a-freeverb-patch-walkthrough",
    "href": "chapters/playrec.html#i-am-sitting-in-a-freeverb-patch-walkthrough",
    "title": "2  Rec & Play",
    "section": "2.2 I am sitting in a [freeverb~] — Patch Walkthrough",
    "text": "2.2 I am sitting in a [freeverb~] — Patch Walkthrough\nThe patch I-am-sitting-in-a-room-freeverb.pd extends the original I am sitting in a room concept by introducing a virtual acoustic environment using reverb and allowing for pre-recorded audio input instead of just live microphone input. This adaptation provides more creative flexibility and consistent results compared to using a physical room.\n\n\n\nPure Data implementation of I am sitting in a room with freeverb\n\n\nThis patch is designed to simulate the recursive recording process of Lucier’s piece while allowing for more control over the sound environment. It uses the freeverb~ object to create a virtual room effect, enabling you to manipulate the acoustic characteristics of the sound without relying on a physical space.\n\n2.2.1 Conceptual Overview\nThe patch enables you to: - Use a pre-recorded audio file OR live input as your source material - Add artificial reverb to simulate room characteristics - Follow the same recursive recording process as the original - Achieve more controlled, repeatable results\n\n\n2.2.2 System Diagram\n\n\n\n\n\nflowchart TB\n  %% Top level source\n  AudioFile[Audio File]\n  \n  %% Horizontal arrangement of recorders A and B\n  subgraph PLAYBACK RECORDERS\n    direction LR\n    subgraph RecorderA [DEVICE A]\n      RecordA[writesf~ A.wav]\n      PlayA[readsf~ A.wav]\n      RecordA ==&gt; PlayA\n    end\n    \n    subgraph RecorderB [DEVICE B]\n      RecordB[writesf~ B.wav]\n      PlayB[readsf~ B.wav]\n      RecordB ==&gt; PlayB\n    end\n  end\n  \n  subgraph VIRTUAL ROOM\n  %% Reverb block in the middle\n    VirtualRoom(((freeverb~&lt;br&gt;Virtual Room)))\n  end  \n  \n  %% Output at the bottom  \n  Output[dac~]\n  \n  %% Clear connections showing signal flow\n  AudioFile --&gt; RecordA\n  AudioFile --&gt; Output\n  \n  PlayA ==&gt; VirtualRoom\n  PlayB ==&gt; VirtualRoom\n  \n  VirtualRoom ==&gt; RecordB\n  VirtualRoom ==&gt; RecordA\n  VirtualRoom --&gt; Output\n  \n  %% Feedback path showing recursion\n  PlayB -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordA\n  PlayA -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordB\n\n\n\n\n\n\nThis diagram illustrates the recursive nature of the recording and playback process, similar to how Lucier’s piece unfolds. Each generation of tape recorders captures the previous one, creating a feedback loop that emphasizes the room’s resonances.\n\n\n2.2.3 Unique Features of the [freeverb~] Version\n\n2.2.3.1 1. Audio File Input\nUnlike the original patch that only uses microphone input, this version can:\n\nPlay a pre-recorded audio file (speech.wav) as the initial source\nProcess this file through reverb before recording\nTrigger playback with a button (labeled “2. Playback audio”)\n\n\n\n\n\n\nflowchart LR\n    bng([bng]) --&gt; msg[\"msg open speech.wav, 1\"]\n    msg --&gt; readsf[\"readsf~\"]\n    readsf --&gt; freeverb[\"freeverb~\"]\n    freeverb --&gt; send[\"s~ audio.input\"]\n\n\n\n\n\n\n\n\n2.2.3.2 2. Virtual Acoustic Environment\nInstead of relying on a physical room’s acoustics, this patch uses freeverb~ objects to create a simulated acoustic space:\n\nEvery audio signal (input and playback) passes through a freeverb~ object\nThis creates consistent reverberation that can be adjusted and controlled\nMultiple freeverb~ objects process different stages of the audio for cumulative effects\n\n\n\n2.2.3.3 3. Enhanced Control Flow\nThe patch includes numbered controls for better workflow:\n\n\n\n\n\n\n\n\nButton\nLabel\nFunction\n\n\n\n\n1\nStart recording\nBegins recording the input source to record-A.wav\n\n\n2\nPlayback audio\nPlays the speech.wav file through reverb into the system\n\n\n3\nStop recording\nStops the current recording process\n\n\n\n\n\n2.2.3.4 4. Multiple Send/Receive Paths\nThe patch uses additional send/receive pairs to manage signal routing:\n\ns~/r~ audio.input - Routes input signals to the recording object\ns~/r~ player.A - Routes playback from file A to recorder B\ns~/r~ player.B - Routes playback from file B back to recorder A\nthrow~/catch~ audio - Collects all signals to be sent to the output\n\n\n\n\n2.2.4 Step-by-Step Explanation\n\n2.2.4.1 Step 1: Initial Audio Source Options\nThis patch provides two possible sources for the initial recording:\n\nPre-recorded file input:\n\nClick “2. Playback audio” button to play speech.wav\nThe file is played through readsf~, processed by freeverb~, and sent to s~ audio.input\n\nLive input: (not explicitly shown in this version but could be added using adc~)\n\nAny signal sent to s~ audio.input will be recorded when recording starts\n\n\n\n\n2.2.4.2 Step 2: Recording the First Generation\n\nClick “1. Start recording” to begin recording to record-A.wav\nThe signal from s~ audio.input is captured by r~ audio.input and sent to writesf~\nThe resulting file contains your source material with initial reverb applied\n\n\n\n2.2.4.3 Step 3: Playback and Re-recording\n\nAfter stopping recording with “3. Stop recording”, the patch automatically:\n\nOpens and plays record-A.wav through readsf~\nProcesses it through another freeverb~ for additional reverb\nSends it to both throw~ audio (for monitoring) and s~ player.A\nThe player.A signal is received and recorded to record-B.wav\n\n\n\n\n2.2.4.4 Step 4: Recursive Process\n\nWhen record-B.wav is created, it can be played back through another readsf~\nThis signal is also processed by freeverb~\nThe output is sent to both throw~ audio and s~ player.B\nThe player.B signal could be received and recorded back to record-A.wav\nThis cycle can continue, with each generation accumulating more of the virtual room’s characteristics\n\n\n\n2.2.4.5 Step 5: Output\n\nAll audio signals sent to throw~ audio are collected by catch~ audio\nThe mixed signal is sent to out~ for playback through speakers/headphones\n\n\n\n\n2.2.5 Key Objects Table\n\n\n\n\n\n\n\nObject\nPurpose in This Patch\n\n\n\n\nfreeverb~\nCreates virtual room acoustics by adding reverberation\n\n\nreadsf~\nPlays audio files (source material or previous generations)\n\n\nwritesf~\nRecords audio to file (for each generation)\n\n\ns~/r~\nRoutes audio between different parts of the patch\n\n\nthrow~/catch~\nMixes and outputs all audio signals\n\n\ndel\nAdds small delays to ensure proper sequencing of operations\n\n\nbng\nProvides buttons for user interaction\n\n\nout~\nSends audio to speakers/headphones\n\n\n\n\n\n2.2.6 Creative Applications\n\nExperiment with reverb settings: Try different room sizes, damping, and wet/dry mix\nUse different source materials: Test various speech samples, musical phrases, or sound effects\nCreate hybrid processes: Record the first generation in a real room, then switch to the virtual environment\nBuild compositional sequences: Layer different generations to create evolving textures\nCompare real vs. virtual: Process the same source through both the original and freeverb patches to contrast physical and virtual acoustics\n\nThis virtual approach offers a controlled laboratory for exploring the core concepts of Lucier’s piece, making it accessible even without an ideal acoustic space, while also opening new creative possibilities that wouldn’t be possible in a physical implementation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#references",
    "href": "chapters/playrec.html#references",
    "title": "2  Rec & Play",
    "section": "References",
    "text": "References\n\n\n\n\nHasse, Sarah. 2012. “I Am Sitting in a Room.” Body, Space & Technology 11. https://doi.org/10.16995/bst.71.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html",
    "href": "chapters/sequencer.html",
    "title": "3  Sequencer",
    "section": "",
    "text": "3.1 Arrays and Sequencers\nIn this chapter, we will explore the concept of sequencers and how they can be implemented in Pure Data.\nSequencer is a tool for organizing and controlling the playback of events in a temporally ordered sequence. This sequence consists of a series of discrete steps, where each step represents a regular time interval and can contain information about event activation or deactivation.\nIn addition to controlling musical events such as notes, chords, and percussions, a step sequencer can also manage a variety of other events. This includes parameter changes in virtual instruments or audio/video synthesizers, real-time effect automation, lighting control in live performances or multimedia installations, triggering of samples to create patterns and effect sequences, as well as firing MIDI control events to operate external hardware or software.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#arrays-and-sequencers",
    "href": "chapters/sequencer.html#arrays-and-sequencers",
    "title": "3  Sequencer",
    "section": "",
    "text": "Step\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nValue\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nIndex\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nEach cell represents a step in the sequencer.\n1 to 8 indicate the steps.\nEmpty cells represent steps without events or notes.\nYou can fill each cell with notes or events to represent the desired sequence.\n\n\n3.1.1 Represent an 8-step sequencer using pseudo code \n\n3.1.1.1 Formulation (pseudo code)\nData Structures:\n- Define a data structure to represent each step of the sequence (e.g., array, list).\n\nVariables:\n- Define an array to store the MIDI note values for each step.\n- Initialize the sequencer parameters:\n  - CurrentStep = 0\n  - Tempo\n  - NumberOfSteps = 8\n\nAlgorithm:\n1. Initialization:\n   a. Set CurrentStep to 0.\n   b. Ask the user to input the Tempo.\n   c. Set NumberOfSteps to 8.\n\n2. Loop:\n   a. While true:\n      i. Calculate the time duration for each step based on the Tempo.\n      ii. Play the MIDI note corresponding to the CurrentStep.\n      iii. Increment CurrentStep.\n      iv. If CurrentStep exceeds NumberOfSteps, reset it to 0.\n      v. Wait the calculated duration before moving to the next step.\n\n\n\n3.1.2 Pure Data Implementation of the 8-Step Sequencer\n\n\n\nFig.\n\n\nThis is a simple step sequencer for MIDI. It uses the mido library to send MIDI messages and the time library to control the timing of the sequence.\nThe script starts by importing the necessary libraries and defining some data structures and initial variables. The secuencia list will hold the sequence of notes to be played, and is initially filled with None values. The notas_midi list contains the MIDI note values for each step in the sequence, representing a C Major scale.\nThen, the script initializes some sequencer parameters. The paso_actual variable tracks the current step in the sequence, tempo sets the tempo in beats per minute, and num_pasos stores the total number of steps in the sequence.\nThe script opens a virtual MIDI output port using mido.open_output(). You may need to change the port name ‘IAC Driver Bus 1’ depending on your system setup.\nThe main part of the script is a while loop that continuously plays the sequence. For each step, it calculates the step duration based on the tempo, plays the corresponding MIDI note, increments the current step, and waits for the calculated duration before moving to the next step. If the current step exceeds the number of steps in the sequence, it resets to 0. After each note is played, a note_off MIDI message is sent to stop the note.\n\n\n3.1.3 Random Step Sequencer Implementation\n\n\n\nFig.\n\n\nThis Python script is a modified version of the step-by-step MIDI sequencer from the previous example. The main difference lies in how the current step (paso_actual) is incremented.\nIn the original script, the current step increased sequentially, creating a predictable pattern of notes. In this modified version, the current step is set to a random integer between 0 and 7 (inclusive). This means that the note sequence will be played in a random order, creating a more unpredictable pattern.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#piano-phase-steve-reich",
    "href": "chapters/sequencer.html#piano-phase-steve-reich",
    "title": "3  Sequencer",
    "section": "3.2 Piano Phase (Steve Reich)",
    "text": "3.2 Piano Phase (Steve Reich)\n\n\n\nFig.\n\n\nView Piano Phase Score (S. Reich)\nDesign patterns are abstractions that capture idiomatic tendencies of practices and processes. These abstractions are embedded in live coding languages as functions and syntax, allowing for the reuse of these practices in new projects. (Brown 2023)\nPiano Phase is an example of “music as a gradual process,” as Reich stated in his essay from 1968.(Reich 2002). In it, Reich described his interest in using processes to generate music, particularly noting how the process is perceived by the listener. (Processes are deterministic: a description of the process can describe an entire whole composition.[5] In other words, once the basic pattern and the phase process have been defined, the music consists itself.)\nReich called the unexpected ways change occurred via the process “by-products”, formed by the superimposition of patterns. The superimpositions form sub-melodies, often spontaneously due to echo, resonance, dynamics, and tempo, and the general perception of the listener.(Epstein 1986)\nPiano Phase led to several breakthroughs that would mark Reich’s future compositions. The first is the discovery of using simple but flexible harmonic material, which produces remarkable musical results when phasing occurs. The use of 12-note or 12-division patterns in Piano Phase proved to be successful, and Reich would re-use it in Clapping Music and Music for 18 Musicians. Another novelty is the appearance of rhythmic ambiguity during phasing of a basic pattern. The rhythmic perception during phasing can vary considerably, from being very simple (in-phase), to complex and intricate.\nThe first section of Piano Phase has been the section studied most by musicologists. A property of the first section of phase cycle is that it is symmetric, which results in identical patterns half-way through the phase cycle.(Epstein 1986)\nPiano Phase can be conceived as an algorithm. Starting with two pianos playing the same sequence of notes at the same speed, one of the pianists begins to gradually accelerate their tempo.\nWhen the separation between the notes played by both pianos reaches a fraction of a note’s duration, the phase is shifted and the cycle repeats. This process continues endlessly, creating a hypnotic effect of rhythmic phasing that evolves over time as an infinite sequence of iterations.\n\n\n\nPhase Difference\n\n\n\n3.2.1 Algorithm Formulation (pseudo code)\n1. Initialize two pianists with the same note sequence.\n2. Set an initial tempo for both pianists.\n3. Repeat until the desired phase is reached:\n     a. Gradually increase the second pianist's tempo.\n     b. Compare note positions between both pianists.\n     c. When the separation between the notes reaches a fraction of a note’s duration, invert the phase.\n     d. Continue playback.\n4. Repeat the cycle indefinitely to create a continuous and evolving rhythmic phasing effect.\n\n\n3.2.2 Piano Phase Implementation in PD\n\n\n\nfig\n\n\n\n\n3.2.3 Patch Overview\nThe piano-phase.pd patch implements a minimalist phasing process inspired by Steve Reich’s Piano Phase. It uses two parallel sequencers, each reading from the same melodic sequence but advancing at slightly different rates. This gradual tempo difference causes the two sequences to drift out of phase, creating evolving rhythmic and melodic patterns.\n\n\n\n\n\nflowchart TD\n    Start([Start/Stop]) --&gt; M1[Clock 1 410ms]\n    Start --&gt; M2[Clock 2 406ms]\n    M1 --&gt; C1[Counter 1]\n    M2 --&gt; C2[Counter 2]\n    C1 --&gt; T1[read sequence]\n    C2 --&gt; T2[read sequence]\n    T1 --&gt; MK1[MIDI noteout]\n    T2 --&gt; MK2[MIDI noteout]\n\n\n\n\n\n\n\n\n3.2.4 Sequence Data\n[table sequence 12]\nThe sequence is stored in a table named sequence with 12 MIDI note values:\n64 66 71 73 74 66 64 73 71 66 74 73\nThis pattern is initialized at patch load.\n\n\n3.2.5 Data Flow\nInitialization\n\nThe sequence table is filled with 12 MIDI notes.\nTwo metro objects are set: one at 410 ms (left), one at 406 ms (right).\nBoth sequencers are started/stopped with a single toggle.\n\nPlayback\n\nEach metro triggers its own counter, which advances from 0 to 11 and wraps around.\nThe current index is used to read a note from the sequence table.\nThe note is played via makenote and sent to MIDI output.\nThe current step is visualized with a horizontal radio button (hradio).\n\nPhasing Effect\n\nThe right sequencer is slightly faster, so its step index gradually shifts ahead of the left sequencer.\nOver time, the two sequences drift out of phase, producing new rhythmic and melodic combinations.\nEventually, the faster sequencer laps the slower one, and the process repeats.\n\n\n\n3.2.6 Key Objects and Their Roles\nTwo identical sequences are played in parallel. Slightly different tempos cause the sequences to gradually shift out of phase. Emergent patterns arise from the interaction of the two sequences. Visual feedback helps track the phase relationship. This patch demonstrates how minimalist processes can generate complex musical results through simple, deterministic rules, echoing the core concept of Steve Reich’s Piano Phase.\n\n\n\nObject\nPurpose\n\n\n\n\nmetro\nSets the timing for each sequencer\n\n\nexpr int(60000/$f1)\nConverts BPM to milliseconds\n\n\nf, + 1, mod 12\nAdvances and wraps the step index\n\n\ntabread sequence\nReads the current note from the sequence\n\n\nmakenote\nGenerates MIDI note-on/off with duration\n\n\nnoteout\nSends MIDI notes to the output device\n\n\nhradio\nVisualizes the current step position\n\n\nloadbang\nInitializes sequence and metro intervals",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#random-melody-generator",
    "href": "chapters/sequencer.html#random-melody-generator",
    "title": "3  Sequencer",
    "section": "3.3 Random Melody Generator",
    "text": "3.3 Random Melody Generator\nThis chapter provides a step-by-step explanation of the music-scale-B.pd Pure Data patch. The patch demonstrates how to generate a melody using a musical scale, store it in an array, and play it back using MIDI.\n\n\n\nRandom Melody Generator\n\n\n\n3.3.1 Patch Overview\nThe patch is organized into several functional blocks:\n\nScale and Melody Generation\nMelody Storage\nPlayback Control\nMIDI Output\n\nBelow is a simplified diagram of the main data flow:\n\n\n\n\n\nflowchart TD\n    A[Scale Input] --&gt; B[Melody Generation]\n    B --&gt; C[Store in Array]\n    C --&gt; D[Playback Control]\n    D --&gt; E[MIDI Output]\n\n\n\n\n\n\n\n\n3.3.2 Scale and Melody Generation\n\n3.3.2.1 Scale Definition\nThe patch starts with a message box containing the scale intervals:\n0 2 4 5 7 9 11\nFor example, this represents a major scale in semitones accordding to the following mapping:\n\n\n\n\nNote\nInterval\n\n\n\n\nC\n0\n\n\nC# / Db\n1\n\n\nD\n2\n\n\nD# / Eb\n3\n\n\nE\n4\n\n\nF\n5\n\n\nF# / Gb\n6\n\n\nG\n7\n\n\nG# / Ab\n8\n\n\nA\n9\n\n\nA# / Bb\n10\n\n\nB\n11\n\n\n\n\n \nThe scale is appended to a list and processed to determine its length.\n\n\n3.3.2.2 Random Note Selection\nA random number between 0 and 47 is generated (random 48), then shifted up by 60 to get a MIDI note in a typical range.\nThe note is then mapped to the scale using modulo operations and list indexing.\n\n\n3.3.2.3 Melody Construction\nThe patch uses a loop (until, i, + 1) to generate a sequence of notes.\nEach note is calculated based on the scale and stored in a list.\n\n\n\n3.3.3 Melody Storage\nThe generated melody is stored in a Pure Data array called melody.\nThe array is visualized in the patch for reference.\n\n\n\n\n\ngraph LR\n    MelodyList --&gt;|tabwrite| MelodyArray\n\n\n\n\n\n\n\n\n3.3.4 Playback Control\nA metro object (metronome) triggers playback at a tempo set by a horizontal slider.\nEach bang from the metronome advances an index, which reads the next note from the melody array.\n\n\n3.3.5 MIDI Output\nThe note value is sent to a makenote object, which creates a MIDI note with velocity and duration.\nThe note is then sent to the noteout object, which outputs the MIDI note to your system’s MIDI device.\n\n\n3.3.6 Data Flow\n\nInitialize Scale: The scale intervals are defined and appended to a list.\nGenerate Melody: A loop generates random notes mapped to the scale, storing them in the melody array.\nPlayback: A metronome triggers reading from the array, sending notes to MIDI output.\nVisualization: The melody array is displayed as a graph in the patch.\n\n\n\n3.3.7 Key Objects and Their Roles\n\n\n\n\nObject\nPurpose\n\n\n\n\nrandom 48\nGenerates random note indices\n\n\n+ 60\nShifts notes to a higher MIDI octave\n\n\nmod 12\nMaps notes to scale degrees\n\n\nlist-idx\nRetrieves scale degree from the list\n\n\ntabwrite\nWrites notes to the melody array\n\n\nmetro\nControls playback timing\n\n\ntabread\nReads notes from the melody array\n\n\nmakenote\nCreates MIDI notes with velocity/duration\n\n\nnoteout\nSends MIDI notes to output\n\n\n\n\n\n\n3.3.8 Diagram: Melody Generation and Playback\n\n\n\n\n\nflowchart TD\n    S[Scale Message] --&gt; L[list Append]\n    L --&gt; R[random + Offset]\n    R --&gt; M[Modulo/Indexing]\n    M --&gt; A[Melody Array]\n    A --&gt; P[Playback - metro, tabread]\n    P --&gt; MN[makenote]\n    MN --&gt; NO[noteout]\n\n\n\n\n\n\n\n\n3.3.9 Summary\nThis patch demonstrates how to algorithmically generate a melody using a musical scale, store it, and play it back via MIDI in Pure Data. The modular structure allows for easy experimentation with different scales, lengths, and playback parameters.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#the-euclidean-algorithm-generates-traditional-musical-rhythms",
    "href": "chapters/sequencer.html#the-euclidean-algorithm-generates-traditional-musical-rhythms",
    "title": "3  Sequencer",
    "section": "3.4 The Euclidean Algorithm Generates Traditional Musical Rhythms",
    "text": "3.4 The Euclidean Algorithm Generates Traditional Musical Rhythms\nGodfried Toussaint (2005)\n\n\n\nEucliden image\n\n\nLink to the original article\nThe Euclidean rhythm in music was discovered by Godfried Toussaint in 2004 and is described in a 2005 paper “The Euclidean Algorithm Generates Traditional Musical Rhythms”.(Toussaint 2005) The paper presents a method for generating rhythms that are evenly distributed over a given time span, using the Euclidean algorithm. This method is particularly relevant in the context of world music, where such rhythms are often found. The Euclidean algorithm (as presented in Euclid’s Elements) calculates the greatest common divisor of two given integers. It is shown here that the structure of the Euclidean algorithm can be used to efficiently generate a wide variety of rhythms used as timelines (ostinatos) in sub-Saharan African music in particular, and world music in general. These rhythms, here called Euclidean rhythms, have the property that their onset patterns are distributed as evenly as possible. Euclidean rhythms also find application in nuclear physics accelerators and computer science and are closely related to several families of words and sequences of interest in the combinatorics of words, such as Euclidean strings, with which the rhythms are compared.(Toussaint 2005)\n\n3.4.1 Hypothesis\nSeveral researchers have observed that rhythms in traditional world music tend to exhibit patterns distributed as regularly or evenly as possible. The hypothesis is that the Euclidean algorithm can be used to generate these rhythms. According to the hypothesis, the Euclidean algorithm can be used to generate rhythms that are evenly distributed over a given time span. This is particularly relevant in the context of world music, where such rhythms are often found. The following is a summary of the main points:\n\nPatterns of maximal evenness can be described using the Euclidean algorithm on the greatest common divisor of two integers.\n\n\n\n3.4.2 Patterns of Maximal Evenness\nThe Pattern of Maximal Evenness is a concept used in music theory to create Euclidean rhythms. Euclidean rhythms are rhythmic patterns that evenly distribute beats over a time cycle.\nIn essence, the Pattern of Maximal Evenness seeks to distribute a specific number of beats evenly within a given time span. This is achieved by dividing the time into equal parts and assigning beats to these divisions uniformly.\nExample:\nx = beat\n· = rest\n[×· ×· ×· ×· ] → [1 0 1 0 1 0 1 0] (8,4) = 4 beats evenly distributed over 8 pulses\n[×· ·×· ·×·] → [1 0 0 1 0 0 1 0] (8,3) = 3 beats evenly distributed over 8 pulses\n\n\n3.4.3 Euclidean Algorithm\nOne of the oldest known algorithms, described in Euclid’s Elements (around 300 BCE) in Proposition 2 of Book VII, now known as the Euclidean algorithm, calculates the greatest common divisor of two given integers.\nThe idea is very simple. The smaller number is repeatedly subtracted from the larger until the larger becomes zero or smaller than the smaller one, in which case it becomes the remainder. This remainder is then repeatedly subtracted from the smaller number to obtain a new remainder. This process continues until the remainder is zero.\nTo be more precise, consider the numbers 5 and 8 as an example:\n\nFirst, we divide 8 by 5. This gives a quotient of 1 and a remainder of 3.\nThen, we divide 5 by 3, which gives a quotient of 1 and a remainder of 2.\nNext, we divide 3 by 2, which gives a quotient of 1 and a remainder of 1.\nFinally, we divide 2 by 1, which gives a quotient of 2 and a remainder of 0.\n\nThe idea is to keep dividing the previous divisor by the remainder obtained in each step until the remainder is 0. When we reach a remainder of 0, the previous divisor is the greatest common divisor of the two numbers.\nIn short, the process can be seen as a sequence of equations:\n8 = (1)(5) + 3\n5 = (1)(3) + 2\n3 = (1)(2) + 1\n2 = (1)(2) + 0\nNote: 8 = (1)(5) + 3 means that 8 is divided by 5 once, yielding a quotient of 1 and a remainder of 3.\n \nIn essence, it involves successive divisions to find the greatest common divisor of two positive numbers (GCD from now on).\nThe GCD of two numbers a and b, assuming a &gt; b, is found by first dividing a by b, and obtaining the remainder r.\nThe GCD of a and b is the same as that of b and r. When we divide a by b, we obtain a quotient c and a remainder r such that:\na = c · b + r\nExamples:\nLet’s compute the GCD of 17 and 7.\nSince 17 = 7 · 2 + 3, then GCD(17, 7) is equal to GCD(7, 3). Again, since 7 = 3 · 2 + 1, then GCD(7, 3) is equal to GCD(3, 1). Here, it is clear that the GCD between 3 and 1 is simply 1. Therefore, the GCD between 17 and 7 is also 1.\nGCD(17,7) = 1\n\n17 = 7 · 2 + 3\n\n7 = 3 · 2 + 1\n\n3 = 1 · 3 + 0\n \nAnother example:\nGCD(8,3) = 1\n\n8 = 3 · 2 + 2\n\n3 = 2 · 1 + 1\n\n2 = 1 · 2 + 0\n\n\n3.4.4 How does computing the GCD turn into maximally even distributed patterns?\nRepresent a binary sequence of k ones [1] and n − k zeros [0], where each [0] bit represents a time interval and the ones [1] indicate signal triggers.\nThe problem then reduces to:\nConstruct a binary sequence of n bits with k ones such that the ones are distributed as evenly as possible among the zeros.\nA simple case is when k evenly divides n (with no remainder), in which case we should place ones every n/k bits. For example, if n = 16 and k = 4, then the solution is:\n[1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0]\nThis case corresponds to n and k having a common divisor of k (in this case 4).\nMore generally, if the greatest common divisor between n and k is g, we would expect the solution to decompose into g repetitions of a sequence of n/g bits.\nThis connection with greatest common divisors suggests that we could compute a maximally even rhythm using an algorithm like Euclid’s.\n \n\n3.4.4.1 Example (13, 5)\nLet’s consider a sequence with n = 13 and k = 5.\nSince 13 − 5 = 8, we start with a sequence consisting of 5 ones, followed by 8 zeros, which can be thought of as 13 one-bit sequences:\n[1 1 1 1 1 0 0 0 0 0 0 0 0]\nWe begin moving zeros by placing one zero after each one, creating five 2-bit sequences, with three remaining zeros:\n[10] [10] [10] [10] [10] [0] [0] [0]\n13 = 5 · 2 + 3\nThen we distribute the three remaining zeros similarly, placing a [0] after each [10] sequence:\n[100] [100] [100] [10] [10]\n5 = 3 · 1 + 2\nWe now have three 3-bit sequences, and a remainder of two 2-bit sequences. So we continue the same way, placing one [10] after each [100]:\n[10010] [10010] [100]\n3 = 2 · 1 + 1\nThe process stops when the remainder consists of a single sequence (here, [100]), or we run out of zeros.\nThe final sequence is, therefore, the concatenation of [10010], [10010], and [100]:\n[1 0 0 1 0 1 0 0 1 0 1 0 0]\n2 = 1 · 2 + 0\n \n\n\n3.4.4.2 Example (17, 7)\nSuppose we have 17 pulses and want to evenly distribute 7 beats over them.\n1. We align the number of beats and silences (7 ones and 10 zeros):\n[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n\n\n\n1 1 1 1 1 1 1\n0 0 0 0 0 0 0 0 0 0\n\n\n\n\n\n2. We form 7 groups, corresponding to the division of 17 by 7; we get 7 groups of [1 0] and 3 remaining zeros [000], which means the next step forms 3 groups until only one or zero groups remain.\n[1 0] [1 0] [1 0] [1 0] [1 0] [1 0] [1 0] [0] [0] [0]\n17 = 7 · 2 + 3\n\n\n\n1\n1\n1\n1\n1\n1\n1\n0 0 0\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n \n3. Again, this corresponds to dividing 7 by 3. In our case, we are left with only one group and we are done.\n \n[1 0 0] [1 0 0] [1 0 0] [1 0] [1 0] [1 0] [1 0]\n\n\n\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n[1 0 0 1 0] [1 0 1 0 0] [1 0 0 1 0] [1 0]\n7 = 3 · 2 + 1\n\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n1\n1\n1\n\n\n\n\n\n\n0\n0\n0\n\n\n\n\n\n\n\n \n4. Finally, the rhythm is obtained by reading the grouping column by column, from left to right, step by step.\n[1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0]\n3 = 1 · 3 + 0\n \n\n\n\n3.4.5 Implementation in Pure Data - Euclidean sequencer\nThe following code implements the Euclidean algorithm in Pure Data to generate a Euclidean rhythm. The algorithm is based on the mathematical formula:\n(index * hits ) % steps\n↓\n[&lt; notes]\n\n\n\n\n\nflowchart TB\n    A[index * hits]\n    A --&gt; B[% steps]\n    B --&gt; C[&lt; notes]\n\n\n\n\n\n\nwhere:\nindex = index of the Euclidean series (array)\nhits = number of notes to be played\nsteps = array size\nYoy can check this Euclidean rhythm demo in order to interact and see how the algorithm works.\n\n3.4.5.1 Understanding the Mathematical Formula\nThe formula (index * hits) % steps &lt; hits provides a simple and efficient way to approximate the distribution of pulses (or beats) over a sequence of discrete steps. It works by multiplying the current position (represented by index) by the total number of hits (pulses) desired. This result is then taken modulo the total number of steps to ensure it wraps around properly in a cyclical pattern. Finally, the result of this modulo operation is compared to the number of hits. If the condition is true, we place a beat at that position; otherwise, we place a rest.\nThis method produces a rhythm by relying on modular arithmetic. The result is a mathematically regular distribution of beats that often approximates what we expect from an even rhythmic distribution. However, it does so without any recursive logic or iteration—it simply applies a consistent rule to each step in isolation. This makes the formula extremely efficient: it runs in constant time for any position, and it does not require any memory to store the pattern.\n\n\n3.4.5.2 The Bjorklund Algorithm Explained\nIn contrast, the Bjorklund algorithm is a more sophisticated procedure rooted in the Euclidean algorithm for computing the greatest common divisor (GCD). This algorithm begins with two values: the number of pulses (beats) and the number of rests (steps minus beats). It then recursively groups these elements in a way that maximizes the evenness of their distribution.\nThe method proceeds by repeatedly pairing elements—first grouping pulses with rests, then regrouping the leftovers, and so on—until the sequence cannot be subdivided further. The final pattern emerges from this recursive grouping, and it is typically rotated so that it starts with a pulse. The result is a rhythm that is maximally even, meaning that the beats are spaced as equally as possible given the constraints.\nThis process, while more computationally demanding and conceptually complex, produces the canonical Euclidean rhythms often cited in academic and musical literature.\n\n\n3.4.5.3 Comparing the Formula and the Algorithm\nThe core difference between the two approaches lies in how they arrive at the rhythmic pattern. The mathematical formula provides a direct, position-based method for determining beat placement. It does not take into account the context of previous or future beats—it treats each step in isolation. This is why it is so fast and well-suited for real-time applications, such as live audio processing in Pure Data or other creative coding environments.\nThe Bjorklund algorithm, on the other hand, is concerned with the global structure of the pattern. It ensures that beats are distributed with maximal evenness and follows a well-defined sequence of operations that are both recursive and stateful. This means it needs to store and manipulate arrays of data to arrive at the final rhythm. The computational complexity of this algorithm is higher, and it is not as straightforward to implement, but the results are musically and mathematically robust.\nOne major distinction is that the formula often produces a rotated version of the Bjorklund rhythm. That is, while the number and spacing of beats may be similar, the starting position may differ. The Bjorklund rhythm always starts with a pulse, ensuring that it adheres to a particular musical convention, while the formula does not guarantee this.\nAnother important distinction lies in the distribution logic. The formula uses a fixed mathematical rule to space out the beats, leading to regular but not necessarily optimal placement. The Bjorklund algorithm, however, iteratively rearranges beats and rests to achieve the best possible balance.\n\n\n3.4.5.4 A Concrete Example: (13, 5)\nLet’s consider the case of 13 steps with 5 beats.\nUsing the mathematical formula, we apply (index * 5) % 13 &lt; 5 for each position:\nPattern: 1 0 0 1 0 0 1 0 1 0 0 1 0\nIn contrast, the Bjorklund algorithm produces:\nPattern: 1 0 1 0 0 1 0 1 0 0 1 0 0\nBoth patterns contain five beats. Both distribute them fairly evenly. But the Bjorklund version achieves a more perceptually even spacing and aligns with theoretical expectations. The formula result is essentially a rotated variant.\nIn summary, the formula (index * hits) % steps &lt; hits offers a pragmatic and computationally efficient way to generate rhythm patterns that resemble Euclidean rhythms. It is well-suited for real-time use cases and environments where simplicity and speed are more important than strict accuracy. In contrast, the Bjorklund algorithm provides a mathematically rigorous method for generating rhythms with maximal evenness. It aligns with canonical definitions and is favored in theoretical and compositional contexts.The choice between these methods depends on your priorities: use the formula for lightweight, real-time approximation, and use Bjorklund when you need precision and adherence to the canonical Euclidean model.\n\n\n\n3.4.6 Euclidean Rhythm Generator\nThis section provides a detailed explanation of the Euclidean-basic-Serie.pd patch. This Pure Data patch demonstrates how to generate Euclidean rhythms using a straightforward mathematical formula.\n\n\n\nFig.\n\n\n\n3.4.6.1 Patch Overview\nThe patch is structured into several functional blocks. First, there are controls for setting the number of steps and the number of hits (pulses). Next, the patch calculates the Euclidean pattern using a mathematical formula. Finally, the resulting pattern is output and visualized as a list of pulses and rests.\nTo help visualize the flow of data through the patch, consider the following diagram:\n\n\n\n\n\nflowchart TD\n    S[Steps Slider] --&gt; A[Step Counter]\n    H[Hits Slider] --&gt; B[Hits Value]\n    A --&gt; C[Euclidean Formula: index * hits % steps]\n    B --&gt; C\n    C --&gt; D[Compare &lt; hits]\n    D --&gt; E[Pattern List]\n    E --&gt; F[Output/Display]\n\n\n\n\n\n\n\n\n3.4.6.2 Data Flow\nThe process begins with the user setting the number of steps in the sequence using a horizontal slider labeled “Steps.” This determines the total length of the rhythmic cycle. The user also sets the number of hits, or pulses, using another slider labeled “HITS.” These two values are stored and sent to the rest of the patch, where they are used to calculate the rhythmic pattern.\nOnce the parameters are set, the patch uses a loop, implemented with the until object, to iterate through each step index from 0 up to one less than the total number of steps. For each index, the patch calculates a value using the formula (index * hits) % steps. This formula determines the position of each pulse within the cycle by multiplying the current index by the number of hits and taking the result modulo the total number of steps. The result of this calculation is then compared to the number of hits. If the result is less than the number of hits, a pulse (represented by a 1) is added to the pattern. Otherwise, a rest (represented by a 0) is added. This process is repeated for every step in the sequence, gradually building up the complete Euclidean rhythm as a list of ones and zeros.\nAfter the pattern has been calculated, it is displayed as a list, allowing you to see the sequence of pulses and rests. The patch also includes a reset mechanism, so that the pattern can be recalculated and updated whenever the user changes the number of steps or hits.\n\n\n3.4.6.3 Example: (Steps = 13, Hits = 5)\nTo illustrate how the patch works, consider the case where the number of steps is 13 and the number of hits is 5. The following table shows the calculation for each step:\n\n\n\nIndex\nCalculation\nResult\n&lt; Hits?\nOutput\n\n\n\n\n0\n(0 * 5) % 13 = 0\n0\nYes\n1\n\n\n1\n(1 * 5) % 13 = 5\n5\nYes\n1\n\n\n2\n(2 * 5) % 13 = 10\n10\nNo\n0\n\n\n3\n(3 * 5) % 13 = 2\n2\nYes\n1\n\n\n4\n(4 * 5) % 13 = 7\n7\nNo\n0\n\n\n5\n(5 * 5) % 13 = 12\n12\nNo\n0\n\n\n6\n(6 * 5) % 13 = 4\n4\nYes\n1\n\n\n7\n(7 * 5) % 13 = 9\n9\nNo\n0\n\n\n8\n(8 * 5) % 13 = 1\n1\nYes\n1\n\n\n9\n(9 * 5) % 13 = 6\n6\nNo\n0\n\n\n10\n(10 * 5) % 13 = 11\n11\nNo\n0\n\n\n11\n(11 * 5) % 13 = 3\n3\nYes\n1\n\n\n12\n(12 * 5) % 13 = 8\n8\nNo\n0\n\n\n\nThe resulting pattern is:\n1 1 0 1 0 0 1 0 1 0 0 1 0\nThis sequence distributes five pulses as evenly as possible across thirteen steps.\n\n\n3.4.6.4 Key Objects and Their Roles\nThe following table summarizes the key objects used in the patch and their roles:\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nhsl (slider)\nSets number of steps and hits\n\n\nuntil\nLoops through each step index\n\n\nexpr\nCalculates (index * hits) % steps\n\n\n&lt;\nCompares result to hits, outputs 1 or 0\n\n\nadd2\nAppends each result to the output pattern list\n\n\nset\nResets the output list for new calculations\n\n\nmod\nEnsures index wraps around for correct calculation\n\n\n\n\n\n\n3.4.7 Euclidean Rotation Generator\nThis section explains the Euclidean-Rotation.pd patch. This Pure Data patch builds on the basic Euclidean rhythm generator by introducing a rotation parameter, allowing you to shift the starting point of the rhythm pattern. This feature is useful for exploring different phase relationships and rhythmic variations without changing the underlying distribution of pulses.\n\n\n\nFig.\n\n\n\n3.4.7.1 Patch Overview\nThe patch allows you to set the number of steps, the number of hits (pulses), and the rotation amount. The rotation parameter shifts the pattern cyclically, so the rhythm can start at any point in the sequence. The patch calculates the Euclidean pattern using a formula that incorporates the rotation, and then outputs the resulting pattern for visualization and further use.\nThe following diagram illustrates the data flow for the rhythm and rotation:\n\n\n\n\n\nflowchart TD\n    Steps[Steps Slider] --&gt; Counter[Step Counter]\n    Hits[Hits Slider] --&gt; HitsVal[Hits Value]\n    Rot[Rotation Slider] --&gt; RotVal[Rotation Value]\n    Counter --&gt; Formula[Euclidean Formula: index_plus_rotation_times_hits_mod_steps]\n    RotVal --&gt; Formula\n    HitsVal --&gt; Formula\n    Formula --&gt; Compare[Compare to Hits]\n    Compare --&gt; Pattern[Pattern List]\n    Pattern --&gt; Output[Output/Display]\n\n\n\n\n\n\n\n\n3.4.7.2 Data Flow\nThe process begins with the user setting the number of steps, hits, and rotation using horizontal sliders. The steps slider determines the total length of the rhythmic cycle, the hits slider sets the number of pulses to be distributed, and the rotation slider specifies how many positions to shift the pattern.\nA loop, implemented with the until object and a counter, iterates through each step index. For each index, the patch calculates the value using the formula ((index + rotation) * hits) % steps. This formula determines the position of each pulse within the cycle, taking into account the rotation. The result is then compared to the number of hits: if it is less than the number of hits, a pulse (represented by a 1) is added to the pattern; otherwise, a rest (represented by a 0) is added. This process is repeated for every step, building up the complete rotated Euclidean rhythm.\nThe resulting pattern is displayed as a list, showing the sequence of pulses and rests after rotation. The patch also provides a reset mechanism to clear and recalculate the pattern when parameters change.\n\n\n3.4.7.3 Example: Rotating a Euclidean Rhythm\nSuppose you set 8 steps, 3 hits, and a rotation of 2. The main rhythm without rotation might be:\n1 0 0 1 0 0 1 0\nApplying a rotation of 2 shifts the pattern two steps to the right, resulting in:\n0 1 0 0 1 0 0 1\nThis allows you to experiment with different phase offsets and rhythmic feels.\n\n\n3.4.7.4 Key Objects and Their Roles\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nhsl (slider)\nSets number of steps, hits, and rotation\n\n\nuntil\nLoops through each step index\n\n\ncounter\nIncrements the step index for each iteration\n\n\nexpr\nCalculates the Euclidean formula with rotation\n\n\n&lt;\nCompares the formula result to hits, outputs 1 or 0\n\n\nadd2\nAppends each result to the output pattern list\n\n\nset\nResets the output list for new calculations\n\n\nmod\nEnsures index wraps around for correct calculation\n\n\ntabwrite\nWrites the final pattern to an array for visualization\n\n\nprint\nPrints the final pattern to the console (for debugging)\n\n\n\n\n\n\n3.4.8 Euclidean Accents Generator\nThis section explains the Euclidean-Accents.pd patch. This Pure Data patch extends the basic Euclidean rhythm generator by adding a second, independent Euclidean pattern to control accents. The result is a two-layer sequencer: one layer for the main rhythm (hits) and another for accentuation, allowing for more expressive and dynamic rhythmic patterns.\n\n\n\nFig.\n\n\n\n3.4.8.1 Patch Overview\nThe patch is organized into two parallel sections. The first section generates the main Euclidean rhythm, while the second section generates an accent pattern using the same mathematical approach. Both sections allow for independent control of steps, hits, rotation, and accents. The outputs of both patterns are then combined to produce a final sequence where steps can be silent, regular, or accented.\nThe following diagram illustrates the data flow for both rhythm and accent layers:\n\n\n\n\n\nflowchart TD\n    S[Steps Slider] --&gt; A[Step Counter]\n    H[Hits Slider] --&gt; B[Hits Value]\n    R[Rotation Slider] --&gt; C[Rotation Value]\n    A --&gt; D[Euclidean Formula: index+rotation*hits%steps]\n    B --&gt; D\n    C --&gt; D\n    D --&gt; E[Compare &lt; hits]\n    E --&gt; F[Main Pattern List]\n    ACC[Accents Slider] --&gt; ACCV[Accents Value]\n    ACCROT[Accents Rotation Slider] --&gt; ACCROTVAL[Accents Rotation Value]\n    A2[Step Counter] --&gt; D2[Euclidean Formula: index+acc_rotation*accents %steps]\n    ACCV --&gt; D2\n    ACCROTVAL --&gt; D2\n    D2 --&gt; E2[Compare &lt; accents]\n    E2 --&gt; F2[Accent Pattern List]\n    F & F2 --&gt; G[Combine Patterns]\n    G --&gt; H[Output/Display]\n\n\n\n\n\n\n\n\n3.4.8.2 Data Flow\nThe process begins with the user setting the number of steps, hits, and rotation for the main rhythm using horizontal sliders. These parameters determine the length of the sequence, the number of pulses, and the rotation (offset) of the pattern. The patch uses a loop (until) and a counter to iterate through each step index. For each step, it calculates the value using the formula ((index + rotation) * hits) % steps. This value is compared to the number of hits; if it is less, a pulse (1) is added to the main pattern, otherwise a rest (0).\nIn parallel, the user can set the number of accents and their rotation using additional sliders. The accent pattern is generated in the same way as the main rhythm, but with its own independent parameters. The formula ((index + acc_rotation) * accents) % steps is used to determine the placement of accents. The result is a second pattern of 1s (accent) and 0s (no accent).\nAfter both patterns are generated, they are combined step by step. If both the main pattern and the accent pattern have a 1 at the same step, the output for that step is set to 2 (indicating an accented hit). If only the main pattern has a 1, the output is 1 (regular hit). If neither has a 1, the output is 0 (rest). The final combined pattern is written to an array and displayed, allowing for visualization and further use in sequencing.\n\n\n3.4.8.3 Example: Combining Rhythm and Accents\nSuppose you set 8 steps, 3 hits, and 2 accents. The main rhythm might produce a pattern like:\n1 0 0 1 0 0 1 0\nThe accent pattern, with its own rotation, might produce:\n1 0 1 0 0 1 0 0\nCombining these, the output would be:\n2 0 1 1 0 1 1 0\nHere, “2” indicates an accented hit, “1” a regular hit, and “0” a rest.\n\n\n3.4.8.4 Key Objects and Their Roles\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nhsl (slider)\nSets number of steps, hits, rotation, accents, and accent rotation\n\n\nuntil\nLoops through each step index\n\n\ncounter\nIncrements the step index for each iteration\n\n\nexpr\nCalculates the Euclidean formula for both rhythm and accent patterns\n\n\n&lt;\nCompares the formula result to hits or accents, outputs 1 or 0\n\n\nadd2\nAppends each result to the output pattern list\n\n\nset\nResets the output list for new calculations\n\n\nlist-idx\nRetrieves values from the generated lists for combination\n\n\nexpr if($f1+$f2==2, 2, $f1)\nCombines main and accent patterns into a single output\n\n\ntabwrite\nWrites the final pattern to an array for visualization\n\n\nprint\nPrints the final pattern to the console (for debugging)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#cellular-automata",
    "href": "chapters/sequencer.html#cellular-automata",
    "title": "3  Sequencer",
    "section": "3.5 Cellular Automata",
    "text": "3.5 Cellular Automata\n\n\n\nFig. Cellular Automaton\n\n\nCellular automata are simple machines consisting of cells that update in parallel at discrete time steps. In general, the state of a cell depends on the state of its local neighborhood at the previous time step. The earliest known examples were engineered for specific purposes, such as the two-dimensional cellular automaton constructed by von Neumann in 1951 to model biological self-replication (Brummitt and Rowland 2012)\nA cellular automaton is a mathematical and computational model for a dynamic system that evolves in discrete steps. It is suitable for modeling natural systems that can be described as a massive collection of simple objects interacting locally with each other.\nA cellular automaton is a collection of “colored” cells on a grid of specified shape that evolves through a series of discrete time steps according to a set of rules based on the states of neighboring cells. The rules are then applied iteratively over as many time steps as desired.\nCellular automata come in a variety of forms and types. One of the most fundamental properties of a cellular automaton is the type of grid on which it is computed. The simplest such grid is a one-dimensional line. In two dimensions, square, triangular, and hexagonal grids can be considered.\nOne must also specify the number of colors (or distinct states) k that a cellular automaton can assume. This number is typically an integer, with k=2 (binary, [1] or [0]) being the simplest choice. For a binary automaton, color 0 is commonly referred to as “white” and color 1 as “black”. However, cellular automata with a continuous range of possible values can also be considered.\nIn addition to the grid on which a cellular automaton resides and the colors its cells can assume, one must also specify the neighborhood over which the cells influence each other.\n\n3.5.1 One-Dimensional Cellular Automata (1DCA)\nThe simplest option is \"nearest neighbors\", where only the cells directly adjacent to a given cell can influence it at each time step. Two common neighborhoods in the case of a two-dimensional cellular automaton on a square grid are the so-called Moore neighborhood (a square neighborhood) and the von Neumann neighborhood (a diamond-shaped neighborhood).\nCellular automata are considered as a vector. Each component of the vector is called a cell. Each cell is assumed to take only two states:\n\n[0] (white)\n[1] (black)\n\nThis type of automaton is known as an elementary one-dimensional cellular automaton (1DCA). A dynamic process is performed, starting from an initial configuration C(0) of each of the cells (stage 0), and at each new stage, the state of each cell is calculated based on the state of the neighboring cells and the cell itself in the previous stage.\nThe cellular automata that we study in this section are one-dimensional. A one-dimensional cellular automaton consists of:\n\nan alphabet Σ of size k,\na positive integer d,\na function i from the set of integers to Σ, and\na function f from Σd (d-tuples of elements in Σ) to S.\n\nThe function i is called the initial condition, and the function f is called the rule. We think of the initial condition as an infinite row of discrete cells, each assigned one of k colors. To evolve the cellular automaton, we update all cells in parallel, where each cell updates according to f, a function of d cells in its vicinity on the previous step. I adopt the usual convention of naming a cellular automaton’s rule by the number whose base-k digits consist of the outputs of the rule under the kd possible inputs of d cells, ordered reverse-lexicographically. For example, the two-color rule depending on three cells that maps the eight possible inputs according to the rule 00011110 = 30 in this numbering. Here we have identified 0 = [ ] and 1 = [x].\n\n\n3.5.2 The Case of Rule 30\nRule 30 is a binary one-dimensional cellular automaton introduced by Stephen Wolfram (Wolfram 1983). It considers an infinite one-dimensional array of cellular automaton cells with only two states, with each cell in some initial state.\nAt discrete time intervals, each cell changes state spontaneously based on its current state and the state of its two neighbors.\nWhat it consists of:\n\nEach cell can be in one of two states: alive or dead.\nThe next generation of a cell is determined by the current state of the cell and the state of its two neighboring cells.\nThere are 8 possible configurations of neighboring states (3^2), and for each, Rule 30 defines whether the cell lives or dies in the next generation.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is called Rule 30 because in binary, 000111102 = 30\n\n\nFor Rule 30, the set of rules that governs the automaton’s next state is:\n\n\n\n\n\n\n\n\n\nPattern (decimal)\nPattern (binary)\nNext State(center cell)\nNew state(center cell)\n\n\n\n\n0\n000\nDead\n000\n\n\n1\n001\nAlive\n011\n\n\n2\n010\nDead\n000\n\n\n3\n011\nAlive\n011\n\n\n4\n100\nAlive\n011\n\n\n5\n101\nDead\n000\n\n\n6\n110\nAlive\n011\n\n\n7\n111\nDead\n000\n\n\n\nIn the following diagram, the top row shows the state of the central cell (cell i) and its two neighboring cells at a given stage, and the bottom row shows the state of the central cell in the next stage.\nFor example, in the first case of the figure:\n\nif the state of a cell at a given stage is [1] (black) and\nits two neighbors at that stage are also [1] (black),\nthen the cell’s state in the next stage will be [0] (white).\n\n\n\n\nFig. Rule 30\n\n\nLet’s break down the procedure for determining the next state in the 8 combinations:\nInput configurations: Consider the 8 possible input combinations of the three cells (a central cell and its left and right neighbors). Since each cell can be in one of two possible states (0 or 1), the combinations are 000, 001, 010, 011, 100, 101, 110, and 111.\nBinary representation of the rule: Rule 30 is represented by the number 30 in binary, which is 00011110. This binary representation determines the rules for the next state of the central cell for each of the 8 possible combinations.\nBit correspondence: The 8 bits of the binary representation (00011110) correspond to the 8 input combinations in order. From right to left, the bits represent the next state of the central cell for each input combination.\nFor example, the least significant bit of 00011110 (the rightmost bit) is 0. This means that when the input combination is 000, the next state of the central cell will be 0.\nNext state determination: For each input combination (e.g., 000, 001, 010, 011, 100, 101, 110, 111), the corresponding bit in the binary representation of Rule 30 [00011110] indicates the next state of the central cell.\n\n\n\nInput Configuration\nNext State\n\n\n\n\n000\n0\n\n\n001\n1\n\n\n010\n1\n\n\n011\n1\n\n\n100\n1\n\n\n101\n0\n\n\n110\n0\n\n\n111\n0\n\n\n\nFor example, if the input combination is 000 and the corresponding bit in Rule 30 is 0, then the next state of the central cell will be 0.\nTherefore, the rules are not arbitrary but are determined by the binary representation of Rule 30, which specifies the next state of the central cell for each input combination of its neighbors.\n\n\n\nFig. Rule 30\n\n\n\n\n3.5.3 Rule 30 Implementation in Python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef rule30(cells):\n    \"\"\"Applies Rule 30 to the input cells.\"\"\"\n    new_cells = np.zeros_like(cells)\n    extended_cells = np.concatenate(([cells[-1]], cells, [cells[0]]))  # Apply periodic boundary conditions\n    for i in range(1, len(extended_cells) - 1):\n        neighborhood = extended_cells[i-1:i+2]\n        if np.array_equal(neighborhood, [1, 1, 1]) or np.array_equal(neighborhood, [1, 1, 0]) or \\\n           np.array_equal(neighborhood, [1, 0, 1]) or np.array_equal(neighborhood, [0, 0, 0]):\n            new_cells[i-1] = 0\n        else:\n            new_cells[i-1] = 1\n    return new_cells\n\ndef main():\n    # Initialize the cells\n    cells = np.zeros(100)\n    cells[50] = 1  # Start with one cell in the middle\n\n    # Apply Rule 30 for 100 steps\n    history = [cells]\n    for i in range(100):\n        cells = rule30(cells)\n        history.append(cells)\n\n    # Display the history as an image\n    plt.imshow(history, cmap='binary')\n    plt.show()\n\nif __name__ == \"__main__\":\n    main()\nThis Python script uses the numpy and matplotlib libraries to simulate and visualize the evolution of a cellular automaton using Rule 30.\nThe rule30 function is the core of this script. It takes a one-dimensional array of cells as input, where each cell is either 0 or 1. The function first creates a new array new_cells with the same shape as the input, filled with zeros. It then extends the input array at both ends to apply periodic boundary conditions, meaning that the first cell is considered a neighbor of the last cell and vice versa.\nNext, the function iterates over each cell in the extended array (excluding the added boundary cells). For each cell, it considers the cell and its two neighbors as a neighborhood. If the neighborhood matches one of four specific patterns ([1, 1, 1], [1, 1, 0], [1, 0, 1], or [0, 0, 0]), the corresponding cell in new_cells is set to 0. Otherwise, it is set to 1. This is the implementation of Rule 30. Finally, the function returns the new array of cells.\nThe main function initializes a one-dimensional array of 100 cells, all set to 0 except the middle cell, which is set to 1. It then applies the rule30 function to this array 100 times, storing each resulting array in a list called history. This list is then visualized as a binary image using matplotlib’s imshow function, where each row corresponds to one step in the cellular automaton’s evolution.\nThe script is designed to run as a standalone program. The line if __name__ == \"__main__\": ensures that the main function is called only when the script is run directly, not when imported as a module.\n\n\n3.5.4 Two-Dimensional Cellular Automata (2DCA)\nTwo-dimensional cellular automata are an extension of one-dimensional cellular automata, where cells not only have neighbors to the left and right, but also above and below. This allows modeling of more complex and structurally rich systems, such as two-dimensional phenomena like wave propagation, growth patterns in biology, fire spread, fluid simulation, among others.\nTwo-dimensional cellular automata (2DCA) are computational models that simulate dynamic systems on a two-dimensional grid.\nThey consist of:\n\nCells: Each cell in the grid can have a finite state, such as alive or dead, and can change state according to the automaton’s rules.\nNeighborhood: Each cell has a neighborhood, which is a set of adjacent cells that influence its state. The neighborhood can be rectangular, hexagonal, circular, or of any other shape.\nTransition rule: The transition rule defines how the state of a cell changes based on its current state and the state of the cells in its neighborhood. The rule can be deterministic or probabilistic.\nEvolution: The cellular automaton evolves through iterations. In each iteration, the state of each cell is updated according to the transition rule.\n\n\n\n3.5.5 Case Study: Conway’s Game of Life\n\n\n\nGame of life Source\n\n\nThe most well known cellular automaton is the Game of Life, a two-dimensional cellular automaton which has been invented by John Horton Conway in 1970. It is a simulation that describes the evolution of a population of cells across a two-dimensional grid of squares by three (or four, depending on the wording) simple rules. Every cell has one of the two states ”dead” and ”alive”, and the state changes of the cells depend on the number of living neighbor cells. When it was first published by Martin Gardner in the journal Scientific American in 1970, it was praised to have ”fantastic combinations” (Gardner 1970). Since its invention, thousands of patterns have been found by many people, and there seems to be no end in sight. Even after more than 50 years of history, interesting new patterns are still discovered.\nThe “game” is actually a zero-player game, meaning its evolution is determined by its initial state, requiring no further input from human players. One interacts with the Game of Life by creating an initial configuration and observing how it evolves.\nView Original Article - MATHEMATICAL GAMES - The fantastic combinations of John Conway’s new solitaire game life (Martin Gardner)\n\n\n\n\n\n\nNote\n\n\n\nGOL (Game of Life) and CGOL (Conway’s Game of Life) are commonly used acronyms.\n\n\n\n3.5.5.1 Rules\nThe universe of the Game of Life is an infinite two-dimensional orthogonal grid of square cells, each of which (at any given time) is in one of two possible states, alive (alternatively “on”) or dead (alternatively “off”). At each time step, the following transitions occur:\nThe four rules of Conway’s Game of Life:\n1. Overpopulation: Any live cell with more than three live neighbors dies due to overpopulation.\n2. Underpopulation: Any live cell with fewer than two live neighbors dies due to underpopulation.\n3. Stability: Any live cell with two or three live neighbors survives to the next generation.\n4. Reproduction: Any dead cell with exactly three live neighbors becomes a live cell.\n \nWe can also summarize the rules in a table:\n\n\n\n\n\n\n\n\n\n\nRule\nDescription\nCurrent State\nLive Neighbors\nNext State\n\n\n\n\nOverpopulation\nDeath by overcrowding\nAlive\nMore than 3\nDead\n\n\nUnderpopulation\nDeath by isolation\nAlive\nFewer than 2\nDead\n\n\nStability\nSurvival\nAlive\n2 or 3\nAlive\n\n\nReproduction\nBirth\nDead\n3\nAlive\n\n\n\n \nOr summarized as:\n0 → 3 live neighbors → 1 1 → &lt; 2 or &gt; 3 live neighbors → 0\nWhere 0 represents a dead cell and 1 a live cell.\n \n\n\n\nGame of life rules Source\n\n\n\n\n\nFig. Game of life rules Source\n\n\nThe initial pattern constitutes the system’s ‘seed’. The first generation is created by applying the above rules simultaneously to each cell in the seed; births and deaths occur simultaneously, and the discrete moment in which this occurs is sometimes called a step. (In other words, each generation is a pure function of the previous one.) The rules continue to be applied repeatedly to create more generations.\n\n\n\n3.5.6 Origins\nConway was interested in a problem presented in the 1940s by renowned mathematician John von Neumann, who was trying to find a hypothetical machine that could build copies of itself and succeeded when he found a mathematical model for such a machine with very complicated rules on a rectangular grid. The Game of Life emerged as Conway’s successful attempt to simplify von Neumann’s ideas.\nThe game made its first public appearance in the October 1970 issue of Scientific American, in Martin Gardner’s “Mathematical Games” column, under the title “The fantastic combinations of John Conway’s new solitaire game ‘Life’”.\nSince its publication, Conway’s Game of Life has attracted significant interest due to the surprising ways patterns can evolve.\nLife is an example of emergence and self-organization. It is of interest to physicists, biologists, economists, mathematicians, philosophers, generative scientists, and others, as it shows how complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counterintuitive notion that “design” and “organization” can spontaneously arise in the absence of a designer.\nConway carefully selected the rules, after considerable experimentation, to meet three criteria:\n\nThere should be no initial pattern for which a simple proof exists that the population can grow without limit.\nThere must be initial patterns that appear to grow indefinitely.\nThere should be simple initial patterns that evolve and change for a considerable period before ending in one of the following ways:\n\ndying out completely (due to overcrowding or becoming too sparse); or\nsettling into a stable configuration that remains unchanged thereafter, or entering an oscillating phase in which they repeat a cycle endlessly of two or more periods.\n\n\n\n\n3.5.7 Patterns\nMany different types of patterns occur in the Game of Life, including static patterns (“still lifes”), repeating patterns (“oscillators” – a superset of still lifes), and patterns that move across the board (“spaceships”). Common examples of these three classes are shown below, with live cells in black and dead cells in white.\n\n\n\nGosper glider gun\n\n\nUsing the provided rules, you can investigate the evolution of simple patterns:\n\n\n\n3 cells Source\n\n\n\n\n\n4 cells Source\n\n\nPatterns that evolve for long periods before stabilizing are called Methuselahs, the first of which discovered was the R-pentomino.\n\n\n\nR-pendomino source\n\n\nDiehard is a pattern that eventually disappears, rather than stabilizing, after 130 generations, which is believed to be the maximum for initial patterns with seven or fewer cells.\n\n\n\nDiehard source\n\n\nAcorn takes 5,206 generations to produce 633 cells, including 13 escaping gliders.\n\n\n\nAcorn source\n\n\nConway originally conjectured that no pattern could grow indefinitely; that is, for any initial configuration with a finite number of live cells, the population could not grow beyond some finite upper bound. The Gosper glider gun pattern produces its first glider in the 15th generation, and another every 30 generations thereafter.\n\n\n\nGosper’s glider gun source\n\n\n\n\n\nGosper glider gun\n\n\nFor many years, this pattern was the smallest known. In 2015, a gun called Simkin glider gun was discovered, which emits a glider every 120 generations, and has fewer live cells but is spread across a larger bounding box at its ends.\n\n\n\nSimkin glider gun\n\n\nFor a more detailed overview of gliders and other patterns, you can refer (Kalbande 2022).\n\n\n3.5.8 Python Implementation of Game of Life\nimport time\nimport pygame\nimport numpy as np\n\nCOLOR_BG = (10, 10, 10,)  # Color de fondo\nCOLOR_GRID = (40, 40, 40)  # Color de la cuadrícula\nCOLOR_DIE_NEXT = (170, 170, 170)  # Color de las células que mueren en la siguiente generación\nCOLOR_ALIVE_NEXT = (255, 255, 255)  # Color de las células que siguen vivas en la siguiente generación\n\npygame.init()\npygame.display.set_caption(\"conway's game of life\")  # Título de la ventana del juego\n\n# Función para actualizar la pantalla con las células\ndef update(screen, cells, size, with_progress=False):\n    updated_cells = np.zeros((cells.shape[0], cells.shape[1]))  # Matriz para almacenar las células actualizadas\n\n    for row, col in np.ndindex(cells.shape):\n        alive = np.sum(cells[row-1:row+2, col-1:col+2]) - cells[row, col]  # Cálculo de células vecinas vivas\n        color = COLOR_BG if cells[row, col] == 0 else COLOR_ALIVE_NEXT  # Color de la célula actual\n\n        if cells[row, col] == 1:  # Si la célula actual está viva\n            if alive &lt; 2 or alive &gt; 3:  # Si tiene menos de 2 o más de 3 vecinos vivos, muere\n                if with_progress:\n                    color = COLOR_DIE_NEXT\n            elif 2 &lt;= alive &lt;= 3:  # Si tiene 2 o 3 vecinos vivos, sigue viva\n                updated_cells[row, col] = 1\n                if with_progress:\n                    color = COLOR_ALIVE_NEXT\n        else:  # Si la célula actual está muerta\n            if alive == 3:  # Si tiene exactamente 3 vecinos vivos, revive\n                updated_cells[row, col] = 1\n                if with_progress:\n                    color = COLOR_ALIVE_NEXT\n\n        pygame.draw.rect(screen, color, (col * size, row * size, size - 1, size - 1))  # Dibuja la célula en la pantalla\n\n    return updated_cells  # Devuelve las células actualizadas\n\n# Función principal del programa\ndef main():\n    pygame.init()\n    screen = pygame.display.set_mode((800, 600))  # Crea la ventana del juego\n\n    cells = np.zeros((60, 80))  # Crea una matriz de células muertas\n    screen.fill(COLOR_GRID)  # Rellena la pantalla con el color de la cuadrícula\n    update(screen, cells, 10)  # Actualiza la pantalla con las células\n\n    pygame.display.flip()\n    pygame.display.update()\n\n    running = False  # Variable para controlar si el juego está en ejecución\n\n    while True:\n        for Q in pygame.event.get():\n            if Q.type == pygame.QUIT:  # Si se cierra la ventana, termina el programa\n                pygame.quit()\n                return\n            elif Q.type == pygame.KEYDOWN:\n                if Q.key == pygame.K_SPACE:  # Si se presiona la tecla espacio, se inicia o pausa el juego\n                    running = not running\n                    update(screen, cells, 10)\n                    pygame.display.update()\n            if pygame.mouse.get_pressed()[0]:  # Si se presiona el botón izquierdo del ratón\n                pos = pygame.mouse.get_pos()  # Obtiene la posición del ratón\n                cells[pos[1] // 10, pos[0] // 10] = 1  # Marca la célula correspondiente como viva\n                update(screen, cells, 10)\n                pygame.display.update()\n\n        screen.fill(COLOR_GRID)  # Rellena la pantalla con el color de la cuadrícula\n\n        if running:  # Si el juego está en ejecución\n            cells = update(screen, cells, 10, with_progress=True)  # Actualiza las células con progreso\n            pygame.display.update()\n\n        time.sleep(0.001)  # Espera un breve tiempo para controlar la velocidad del juego\n\nif __name__ == \"__main__\":\n    main()\n\nThis Python script implements Conway’s Game of Life, a cellular automaton devised by British mathematician John Horton Conway. It is a zero-player game, meaning its evolution is determined entirely by its initial state, with no further input required.\nThe script begins by importing the necessary modules: time, pygame for the graphical interface, and numpy to handle the game grid as a 2D matrix. It then defines color constants used for visualizing the game.\nThe pygame.init() function is called to initialize all imported Pygame modules. The window title is set to “Conway’s Game of Life” using pygame.display.set_caption().\nThe update() function updates the game state and redraws the grid. It takes four arguments: screen (the Pygame surface to draw on), cells (the current game state as a 2D numpy array), size (the pixel size of each cell), and with_progress (a boolean indicating whether to display cells that will change in the next generation).\nThe function creates a new 2D matrix updated_cells filled with zeros, matching the shape of cells. It then iterates over each cell, calculates the number of live neighbors, and applies the game rules to determine whether the cell will be alive in the next generation. The function draws each cell on the screen using the appropriate color and returns updated_cells.\nThe main() function initializes Pygame, creates the game window, and initializes the game state as a 2D numpy array of zeros (representing dead cells). It then enters the main loop, which handles Pygame events (such as closing the window or key presses), updates the game state if it is running, and redraws the grid. The game can be started or paused by pressing the spacebar, and cells can be toggled manually by clicking on them.\nFinally, the script calls main() to launch the game. Press the spacebar to begin.\n\n\n3.5.9 Further Reading\n\nThe Game Of Life – Emergence In Generative Art (2020)\nConway’s Game of Life – Life on Computer by Swaraj Kalbande\nWikipedia – Conway’s Game of Life\n\n\n\n3.5.10 Interactive Websites\n\nJohn Conway’s Game of Life – An Introduction to Cellular Automata\nconwaylife.com\nCellular Automata Megathread\n\n\n\n\n\n\nBrown, Andrew. 2023. “Live Coding Patterns and a Toolkit for Pure Data.” Organised Sound 28: 1–12. https://doi.org/10.1017/S1355771823000365.\n\n\nBrummitt, Charles D., and Eric Rowland. 2012. “Boundary Growth in One-Dimensional Cellular Automata.” Complex Systems 21: 85–116. https://doi.org/10.48550/arXiv.1204.2172.\n\n\nEpstein, Paul. 1986. “Pattern Structure and Process in Steve Reich’s Piano Phase.” The Musical Quarterly 72 (4): 494–502.\n\n\nGardner, Martin. 1970. “MATHEMATICAL GAMES - the Fantastic Combinations of John Conway’s New Solitaire Game Life.” Scientific American 223 (4): 120–23. http://www.jstor.org/stable/24927642.\n\n\nKalbande, Swaraj. 2022. “Conway’s Game of Life – Life on Computer.” Medium. https://medium.com/@swarajkalbande123/conways-game-of-life-life-on-computer-b7edfc85d21a.\n\n\nReich, Steve. 2002. “Music as a Gradual Process.” In Writings on Music 1965–2000, 34–36. Oxford; New York: Oxford University Press.\n\n\nToussaint, Godfried. 2005. “The Euclidean Algorithm Generates Traditional Musical Rhythms.” In, 47–56.\n\n\nWolfram, Stephen. 1983. “Statistical Mechanics of Cellular Automata.” Reviews of Modern Physics 55 (3): 601–44. https://doi.org/10.1103/RevModPhys.55.601.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html",
    "href": "chapters/synthesis.html",
    "title": "4  Synthesis",
    "section": "",
    "text": "4.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#oscillatory-movements",
    "href": "chapters/synthesis.html#oscillatory-movements",
    "title": "4  Synthesis",
    "section": "4.2 Oscillatory Movements",
    "text": "4.2 Oscillatory Movements\nOscillatory motion will be analyzed in detail—not only in terms of technical implementation but also through its creative and conceptual applications. Using example patches provided in both Pure Data and VCV Rack, we will explore how oscillation becomes an expressive tool in interactive works and live sound experiments. This approach will form the core of the module’s practical component, where students are encouraged to adapt the exercises to their own interests: whether by replicating existing works, developing new compositions, or analyzing the results of comparative experiments between different oscillators and their spectral behaviors.\n\n4.2.1 Between Technique and Aesthetics\nThis chapter begins with the idea of delving deeper into oscillatory movements from both a technical and aesthetic perspective. Building upon tools introduced in previous chapters we will work with oscillators and explore their many applications. Drawing on the text The Poetics of Signal Processing by Jonathan Sterne and Tara Rodgers, we will approach signal processing not merely as a set of mathematical operations, but as a form of artistic expression. In this context, signal manipulation becomes more than just a means to produce sound—it becomes a way to articulate broader cultural ideas about sound in the digital age.\nWe will explore what could be described as the “rawness” of analog oscillators—that tangible, organic quality inherent in their operation. We will examine how this rawness both contrasts with and complements the digital techniques we’ve developed so far. The use of oscillators in their various forms will serve as a starting point for a series of hands-on exercises, where theory and practice are deeply intertwined.\nThrough this combination of theory, practice, and critical reflection on signal processing, the module aims to open up new sonic possibilities. The references provided are intended not only to contextualize student work, but also to inspire expanded thinking about how these principles might inform personal projects.\nThis practical and flexible framework is reflected in the module’s initial activity on oscillatory movement. Students will have the freedom to choose their working environment and the context in which they wish to develop their experiments. Oscillator implementation can be approached from multiple angles: waveform manipulation, waveshaper design, or even the creation of interactive sound pieces. Each student will be invited to present a short description of their process, along with their code or patch, in order to foster dialogue between theory and practice.\nIn this way, the module not only deepens our understanding of the technical aspects of signal processing, but also invites critical reflection on the role of these technologies in shaping meaning and sonic aesthetics. Special emphasis is placed on the oscillator’s capacity to act as a driver of creativity and expression.\n\n\n4.2.2 Poetics of Signal Processing\nOne of the most important aspects of this chapter is the idea of “poetics” in signal processing. This concept is not just about the technical aspects of sound manipulation, but also about the cultural and artistic implications of these processes. The term “poetics” suggests a deeper engagement with the materiality of sound and its representation in various contexts. I’m going to talk a bit about my reflections and observations on the text Poetics of Signal Processing (Sterne and Rodgers 2011). First, I want to mention the authors, who I find very interesting. The first is Jonathan Sterne, a professor and director of the Culture and Technology program at McGill University in Canada. His work focuses on the cultural dimension of communication technologies, and he specializes in the history and theory of sound in the modern Western world. Then we have another author, a sound artist and musician named Tara Rodgers. She’s an electronic music composer, programmer, and historian of electronic music. She holds a PhD in Communication and has worked in the field of Women’s Studies. She created the platform Pink Noises, which is worth checking out—especially because it gathers a series of interviews with improvisers, composers, and instrument builders. There’s a cross-section of gender, sexuality, feminism, music, sound studies, theater, performance, and performative arts in general.\nLet’s get into some comments on specific sections of the article. The first is “The Sonic Turn,” which describes the emergence of a new culture of listening beginning in the second half of the 20th century. This is discussed through the work of authors like Cox and Kahn. There’s also a growing interest in oral history and anthropology among social scientists, and the emergence of sound art within the art world during the 20th century.Another key point is the growing interest in listening itself, and in the creative possibilities enabled by recording, reproduction, and other forms of sound transmission. This leads us to ask: where do today’s sound technologies come from? What are we going to address in this course?\nWe can start by discussing the “audible past,” or what the article refers to as the “auditory past.” The text mentions that between 1900 and 1925, sound becomes an object of thought and practice. Before this period, sound was thought of in more idealized terms—mainly through the lens of voice and music, along with all the structures they imply. During this period, there were significant socioeconomic and cultural shifts—capitalism, rationalism, science, and colonialism—that influenced ideas and practices related to sound and listening. These changes were not only cultural but social as well. In this “audible past,” even the most basic mechanical elements of sound reproduction technologies were shaped by how they had been used up until then. In this way, sound technologies are tied to habits—they sometimes enable new habits, like new ways of listening, or sometimes they solidify and reinforce existing ones.\nSo let’s reflect on the “poetics” of signal processing. At first, that might sound surprising—poetics? But we can start with a very basic unit: the signal. Signals have a certain materiality. Sound has materiality—it occupies space in a transmission, recording, or playback channel. It exists in a medium and can be manipulated in various ways. There’s a clear distinction between analog electrical signals, electronic signals, and digital ones. Each implies different content and meanings. So, signal processing occurs in the medium of sound transmission, but in a technologized era—that is, the present. This involves manipulating sound in what we might call a “translucent state.” Here, the transducer becomes central: we’ll talk a lot in the course about this idea of converting one form of energy—measurable by a certain magnitude—into another. This concerns almost everything in sound or image that reaches our senses through electronic media. This also exists in the domains of the musician, the playback device, the listener, and the interstices between them.\nRegarding the poetics of signal processing as signal [music plays briefly], the article refers mostly to the figurative dimensions of the process itself. These are the ways that processing is represented in the discourse of audio technology, particularly from a technical or engineering perspective. Signal processing carries cultural meanings—it’s not an isolated technical fact. It has cultural significance. Two metaphorical frameworks commonly used in everyday language among users and creators are discussed in the article: cooking and travel. These are two metaphors I find fascinating.\nLet’s begin with cooking—specifically, “the raw and the cooked.” This draws from the work of anthropologist Claude (Lévi-Strauss 1964), who analyzed the raw, the cooked, and the rotten. The axis of raw/cooked belongs to culture, while fresh/rotten relates more to nature. This is a very important reference: cooking is a cultural operation. Fire—the act of cooking—is the basis of a social order, of stability.\nIn this sense, when we talk about the raw and the cooked in relation to sound, rawness doesn’t mean purity. It’s a relative condition—it refers to the availability of audio for further processing. This is very useful when thinking in opposition to the Hi-Fi culture. We might even think of a scale of rawness, or various degrees of it, which relate to how sound can be manipulated and used.\nWithin this raw/cooked metaphor, terms like slicing (cutting into slices) and dicing (cutting into cubes) appear—these are actual signal processing terms and very fitting metaphors.\nThinking further with this metaphor: raw audio might be seen as passive, something that must be “cooked” through technological processes. This reveals the technologized nature of music technologies, where composition becomes a kind of masculine performance of technological mastery. And I want to stress this point again: composition is often framed as a male-dominated act of technical skill.\nPaul Théberge (Théberge 1997) analyzes musicians as consumers within the sound tech industry. In one chapter of his book, he studies advertising in music tech magazines, showing how the marketing of music technologies has been directed predominantly at men. Fortunately, this is changing slowly. In the raw/cooked metaphor, the idea of sound as a material to be processed and preserved for future use emerges in the late 19th century—just like technologies developed to preserve and can food. It’s a very strong metaphor: processed food and processed sound were both invented to extend and control organic life through technological preservation.\nNow let’s move to the other metaphor—travel. Signal processing can be thought of as a journey, which I find very exciting. We can connect this to topology—a mathematical field that studies spatial relationships. The term topos refers to place. In this sense, we can think of electronics as the arrangement and interaction of various components. A synthesizer circuit, or an oscillator, can be conceived as a space in itself—a map. Early texts in electroacoustics from the late 19th and early 20th centuries started to describe sound and electricity as fluid media. They used water metaphors—talking about waves, oscillations, flow, and current. This “processing as travel” metaphor involves the idea of particles moving through space, with the destination originally being the human ear. Today, that destination could be a transducer—or a computer.\nEven the inner ear was once conceptualized as a terrain made up of interconnected parts through which vibrations would travel. These metaphors matter: sea voyages during the historical periods mentioned in the article also symbolized scientific exploration and the conquest of the unknown. One example I love is that in the 1800s, Lord Kelvin created what could be considered the first synthesizer—but it didn’t produce sound. It was a mechanical device designed to predict tides. I’ll share some images to illustrate this. It essentially summed simple waves into one more complex waveform.\nSo, to conclude this idea of processing as travel, the text also reflects on how maritime metaphors privilege a particular kind of subject—a white, Western male as the ideal “navigator” of synthetic sound waves. This is a clearly colonial and masculinist rhetoric. Generating and controlling electronic sounds becomes associated with a kind of pleasure aligned with capitalism, and also with danger—of disobedient or unruly sounds.\nSwitching to a less metaphorical aspect, the text discusses Helmholtz’s On the Sensations of Tone (Helmholtz 1954). It laid the epistemological foundations for synthesis techniques. Helmholtz argued that any sound could be broken down into volume, pitch, and timbre. For him, sound was a material with clearly defined properties. These properties could be analyzed and then mimicked using synthesis techniques. However, other researchers, like Jessica Roland, explored different approaches. She compared sound to things like rain and wind. Her approach emphasized experience, memory, and the use of synthesis as a kind of onomatopoeia—imitation of natural phenomena. For Roland, unpredictability and chaos are at the heart of synthesis.\nIn conclusion, one of the key points of the article is that metaphors in audio-technical discourse—supposedly neutral or instrumental—are actually shaped by the cultural positions of specific subjects living in specific societies. They are deeply entangled with issues of gender, race, class, and culture. The language of technical culture is highly metaphorical and filled with implicit assumptions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#sound-sources",
    "href": "chapters/synthesis.html#sound-sources",
    "title": "4  Synthesis",
    "section": "4.3 Sound Sources",
    "text": "4.3 Sound Sources\nOscillators are a fundamental component of sound synthesis and play a crucial role in the creation of electronic music. They generate periodic waveforms, which can be manipulated to produce a wide range of sounds. In this section, we will explore the different types of oscillators, their characteristics, and how they can be used creatively in sound design.\n\n4.3.1 Oscillators\nSound sources in synthesizers are largely based on mathematics. There are two fundamental types: waveforms and random signals. Waveforms are typically described as simple geometric shapes—sawtooth, square, pulse, sine, and triangle being the most common. These shapes are mathematically straightforward and electronically feasible to generate. On the other hand, random waveforms produce noise, a constantly shifting mixture of all frequencies.\n\n\n\nFig. Sine Oscillator & White Noise Generator\n\n\nOscillators are one of the core building blocks of synthesizers, often implemented as function generators. A function generator produces a waveform that may be continuous or triggered and can take arbitrary shapes. In a basic analog subtractive synthesizer, an oscillator usually outputs a few continuous waveforms, with frequency controlled by voltage. Since these sources typically output a continuous signal, modifiers must be applied to shape timbre or envelope the sound.\n\n\n4.3.2 Sine & Cosine\nA “pure tone” consists of a single frequency and is produced by a sine wave oscillator, which can be implemented using either the sine or cosine function. These functions take an angle value, or “phase,” as input. Below, we see the angle “alpha,” the sine’s amplitude value, and the cosine’s output denoted as x.\n\n\n\nFig.\n\n\nIn the resulting graph, amplitude is on the vertical axis and angle on the horizontal axis. The cosine output traces the same function as the sine but starts at 1, meaning it has a different initial phase. Sine and cosine are essentially the same waveform offset by 90 degrees of phase.\n\n\n\nFig.\n\n\nPure Data’s native [sin] and [cos] objects take angle values in radians (0 to 2π). However, the audio object [cos~] uses a linear range from 0 to 1 to represent a full cycle. The ELSE library provides the [pi] object, which outputs the constant π. This can be stored in a [value] object and accessed within [expr]. To convert a linear 0–1 range into radians, multiply it by 2 * pi. Then, [cos] and [sin] yield amplitude values accordingly.\n\n\n\n\nFig.\n\n\n\n\n\n4.3.3 Phasor\nIn the following example, we implement a sine oscillator using the [sin~] object and the native [phasor~] object.\n\n\n\nFig.\n\n\nTwo graphs illustrate this: in the top one, the horizontal axis is time, and the vertical axis shows a steadily increasing phase, forming a linear ramp. In the bottom graph, this ramp is transformed into a sine waveform, with amplitude on the vertical axis.\nThe [phasor~] object outputs a linear ramp from 0 to just under 1, representing a complete cycle. It is ideal for driving objects like [cos~] and [sin~], which expect a 0–1 input representing phase progression.\nThe input to [phasor~] is frequency, expressed in cycles per second (hertz). This defines how many full 0–1 cycles occur per second.\nNote that [phasor~] never actually reaches 1—it wraps around to 0. Due to its cyclic nature, 1 is functionally equivalent to 0, just like 360° equals 0° in circular geometry.\nThe output of [phasor~] can be described as a “running phase.” It defines the angular increment applied to the phase at every audio sample.\n\n\n\n\nFig.\n\n\n\n\n\n4.3.4 Oscillator\nIn the analog domain, oscillators are commonly referred to as VCOs (Voltage Controlled Oscillators). VCOs allow frequency or pitch to be controlled via voltage. Some VCOs also feature voltage control inputs for modulation (typically FM) and for altering the waveform shape—usually the pulse width of square waves, although some VCOs allow shaping other waveforms as well.\nMany VCOs include an additional input for synchronization with another VCO’s signal. Phase sync forces the VCO to reset its phase in sync with the incoming signal, limiting operation to harmonics of the input frequency. This results in a harsh, buzzy tone. Softer sync techniques can yield timbral variations rather than locking to an exact frequency.\nA typical VCO offers controls for coarse and fine tuning, waveform selection (often sine, triangle, square, sawtooth, and pulse), pulse width modulation (PWM), and output level. Some VCOs also provide multiple simultaneous waveform outputs and sub-octave outputs one or two octaves below the main signal. Pulse width modulation (PWM) allows dynamic alteration of the pulse waveform shape.\nTo summarize, an oscillator is typically defined by:\n\nWaveform function (sine, sawtooth, square, triangle)\nFrequency (Hz)\nInitial phase (degrees)\nPeak amplitude (optional)\n\nHow do we control these parameters in our model?\n\n\n\nFig.\n\n\n\n\n4.3.5 VCO\nIn this example, [phasor~] and [cos~] form an oscillator. The [cos~] object outputs amplitude values from -1 to 1, yielding a maximum amplitude of 1 without additional gain control.\n\n\n\nFig.\n\n\nThe waveform produced is a cosine. While [phasor~] sets the frequency, it can also define the initial phase. Since sine and cosine are essentially phase-shifted versions of the same function, we can easily produce sine waves as well. However, the initial phase does not affect the perceived pitch of a pure tone.\nTry this patch with different phase offsets. Note that [phasor~] also accepts negative frequencies, reversing the phase direction.\nConnecting [phasor~] to [cos~] replicates the functionality of the [osc~] object.\n\n\n\nFig.\n\n\n\n\n4.3.6 Waveforms\nThe sine wave is the simplest oscillator, generating a pure tone. Other basic and musically useful waveforms include triangle, sawtooth, and square.\nThe sine wave is a smooth, rounded waveform based on the sine function. It contains only one harmonic—the fundamental—which makes it less suitable for subtractive synthesis as it lacks overtones to filter.\nA triangle wave consists of two linear slopes. It contains small amounts of odd harmonics, providing just enough spectral content for filtering.\nA square wave contains only odd harmonics and produces a hollow, synthetic sound. A sawtooth wave contains both odd and even harmonics and sounds bright. Some pulse waves may contain even more harmonic content than basic sawtooth waves. Variants like “super-saw” replace linear slopes with exponential ones and alternate teeth with gaps, producing an even richer harmonic spectrum.\n\n\n4.3.7 Waveshapers\nThis section focuses on creating oscillators in Pure Data (Pd) using the [phasor~] object. The only true oscillator in Pd Vanilla is [osc~], a sine wave oscillator. Even standard waveforms must be built manually.\nAs mentioned earlier, [osc~] is essentially [phasor~] connected to [cos~]. The [phasor~] object outputs a 0–1 ramp, functionally similar to a sawtooth wave with half the amplitude and an offset. [cos~] multiplies this ramp by 2π and computes its cosine. The result is a sine wave oscillator.\n\n\n\nFig.\n\n\nAnother way to construct this oscillator is by using [expr~] and [value]. Here we use the [pi] abstraction to calculate π (or approximate it by sending 1 to [atan] and multiplying the result by 4). We then multiply by 2 and store it in a [value] object. [value] acts like a global variable: any object using the same name accesses the same value. [expr~] can then use this to calculate the cosine, just like [cos~]. While this approach may be more CPU-intensive, it helps deepen understanding of oscillator construction in Pd.\n\n\n\nFig.\n\n\n\n4.3.7.1 Sawtooth Oscillator\nSince [phasor~] already produces a ramp, generating a sawtooth wave is straightforward. Simply multiply [phasor~] by 2 to get the correct amplitude, then subtract 1 to shift the range to -1 to 1.\n\n\n\nFig.\n\n\n\n\n4.3.7.2 Square Wave Oscillator\nTo create a square wave, you can use [expr~] (included in Pd Vanilla), but [&gt;~] is faster and more CPU-efficient. A square wave toggles between -1 and 1. The [&gt;~] object compares its left input signal to a threshold (right input or argument). It outputs 1 if the input is greater than the threshold, and 0 otherwise. Using 0.5 as the threshold with a [phasor~] input yields a square wave: 0 for half the cycle, 1 for the other half.\n\n\n\nFig.\n\n\n\n\n4.3.7.3 Triangle Wave Oscillator\nAmong standard waveforms, the triangle wave is the most complex to construct. Starting with [phasor~], which is an upward ramp from 0 to 1, we create an inverted version by multiplying it by -1 and then adding 1. This gives us a descending ramp from 1 to 0.\nNow we have both ascending and descending ramps. Sending both to [min~] (which outputs the smaller of two values) gives us a triangle waveform spanning 0 to 0.5. [min~] effectively splices the ascending and descending ramps to form a symmetric triangle wave.\n\n\n\nFig.\n\n\n\n\n\n4.3.8 Frequency\nFrequency is presented here in terms of angular velocity! One common unit of measurement is the hertz (Hz), which equals “cycles per second.” Frequency also determines a period of oscillation, which is simply the inverse of frequency. For example, a frequency of 100 Hz corresponds to a period of 0.01 seconds (or 10 milliseconds):\nPeriod = 1 / Frequency\nPeriod = 1 / 100 Hz = 0.01 s = 10 ms\n\n\n\nFig.\n\n\nWe can convert between Hz and milliseconds using this same relationship. Another way to express period is in number of samples, which requires the sampling rate to perform the conversion:\n\n\n\nFig.\n\n\nAngular velocity units require both an angle and a time unit. One cycle per second defines a full cycle (360 degrees) as the angular unit, and seconds as the time unit. Other units are also possible. For example, the angle may be expressed in radians and the time unit as a single sample, yielding a unit of “radians per sample.”\nTo convert Hz to radians per sample, multiply by 2π and divide by the sampling rate. See below for this conversion and the [hz2rad] and [rad2hz] objects from the ELSE library that handle it.\n\n\n4.3.9 Phase\nThe term phase can be used in various contexts, often making it ambiguous and potentially confusing. A useful strategy is to adopt more specific terminology instead of simply referring to “phase” in isolation. On its own, “phase” refers to a stage within a cycle—much like the four phases of the moon. Sound waveforms are cyclical, and we can speak of a positive or negative phase, as shown below. However, this original meaning is rarely used in music theory. Here, we focus on other more relevant applications and interpretations of phase (see right and below).\n\n\n\nFig.\n\n\nInitial phase refers to the point in the cycle where the oscillation begins.\nInstantaneous phase: In music theory, “phase” often refers to the instantaneous phase—a specific point in time, not a stage in a sequence. It’s helpful to adopt the term instantaneous explicitly to denote a single position within a given cycle.\nSince instantaneous phase refers to a single position within a cycle, it can also be represented as an angle. This leads to a synonymous relationship between phase and angle, though it’s important to stress that both denote a position within the cycle.\n\n\n\nFig.\n\n\nTwo oscillators operating at the same frequency can be in phase or out of phase. Being in phase means they are synchronized—there is no phase difference. Being out of phase indicates a lack of synchronization, i.e., a phase difference. This difference can take many forms, but two specific cases are of particular interest: quadrature phase and phase opposition.\nQuadrature phase is the phase difference between sine and cosine waves, which equals a quarter of a cycle (90 degrees).\nPhase opposition is the maximum possible phase difference—half a cycle or 180 degrees.\n\n\n4.3.10 Polarity\nAs we’ve seen, phase opposition leads to signal cancellation—but only under certain waveform conditions! This occurs with sine waves, for instance, but not with all waveforms or signals. Note how, in the figure to the right, inverting the sign of every amplitude value in a waveform results in cancellation when added to the original signal.\nInverting polarity means changing the sign, or multiplying by -1. For sine waves, this results in the same effect as a 180-degree phase opposition. However, a true phase inversion is different, as it involves a time or phase shift.\nDespite this distinction, the term phase inversion is often misused when it really refers to a polarity inversion—which is neither a time shift nor a phase shift.\nYes, this can be very confusing and requires careful attention. That’s why this tutorial prefers the term polarity inversion, although many audio devices refer to a 180-degree phase shift when they are, in fact, performing a polarity inversion.\nBoth phase and polarity inversion produce the same result for sine waves due to their symmetrical waveforms, where the second half of the cycle mirrors the first with opposite sign. Other waveforms with this property include triangle and square waves (with a 0.5 pulse width).\n\n\n\nFig.\n\n\nSawtooth waves, however, do not share this symmetry. Therefore, phase opposition is not equivalent to polarity inversion in this case. The only way to achieve full cancellation of a sawtooth wave is through polarity inversion. Refer to the graphs below: only the original sawtooth combined with its polarity-inverted version results in complete cancellation.\nThe native [phasor~] and [osc~] objects include a right inlet that accepts control data to reset the phase. Whenever the inlet receives a number from 0 to 1, the waveform resets to that initial phase position. Note that this is unrelated to phase modulation techniques discussed earlier.\n\n\n\nFig.\n\n\nThe [osc~] object does not support phase modulation; we implemented it using [phasor~] in the previous examples. Hence, using a [phasor~] together with a [cos~] enables both phase modulation and oscillator resetting.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#additivity",
    "href": "chapters/synthesis.html#additivity",
    "title": "4  Synthesis",
    "section": "4.4 Additivity",
    "text": "4.4 Additivity",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#amplitude-ring-modulation",
    "href": "chapters/synthesis.html#amplitude-ring-modulation",
    "title": "4  Synthesis",
    "section": "4.5 Amplitude & Ring Modulation",
    "text": "4.5 Amplitude & Ring Modulation\nAmplitude modulation (AM) and ring modulation (RM) are two techniques that manipulate the amplitude of a signal using another signal. While they share similarities, they produce distinct results and are used in different contexts. In this section, we will explore the differences between AM and RM, their applications, and how they can be implemented in sound synthesis.\n\n4.5.1 Amplitude Modulation\nWe can modulate the amplitude of any signal—referred to as the carrier—by multiplying it with an oscillating signal, called the modulator. The modulator is typically another oscillator, and its frequency determines the modulation frequency.\n\n\n\nAmplitude Modulation\n\n\nIn the example provided, both the carrier and modulator are sine wave oscillators. This is what we call “classic” amplitude modulation (AM), where the modulator signal includes a DC offset, making it unipolar, ranging only from 0 to 1. This unipolarity ensures that the carrier’s amplitude is scaled without ever becoming negative.\n\n\n4.5.2 Ring Modulation\nRing modulation is a particular form of amplitude modulation where both the carrier and modulator signals are bipolar, meaning they oscillate between -1 and 1 without any DC offset. In this configuration, there’s no functional distinction between carrier and modulator—both behave symmetrically.\n\n\n\nRing Modulation\n\n\nNonetheless, in practical terms, the carrier is usually an audio signal such as a musical instrument, while the modulator remains a simple oscillator. A key technical detail is that when the modulator signal is negative, it inverts the polarity of the carrier signal—producing a unique and often metallic timbre.\n\n\n4.5.3 DC Offset\nAmplitude modulation schemes can involve various DC offset settings—not just limited to AM or RM. In our example, preset configurations for AM and RM are available, but one can also manually adjust peak levels and DC offset using sliders.\nObserve how, in the frequency spectrum of classic AM, we see two sidebands—above and below the carrier frequency—each at half the amplitude of the original carrier. These sidebands are spaced apart by the modulation frequency.\n\n\n\nFig.\n\n\nIn contrast, ring modulation removes the original carrier frequency entirely from the spectrum, leaving only the sidebands, which typically carry more energy than in AM.\nBy adjusting amplitude and DC offset with sliders, we can morph continuously between AM and RM modes, gaining nuanced control over the presence of the original frequency component and the energy distribution in the sidebands.\n\n\n4.5.4 Audio Samples & Modulation\nIn this example, an audio sample replaces the oscillator as the carrier signal. This demonstrates how amplitude modulation can function as an audio effect processor rather than just a synthesis technique. In fact, much of what we traditionally associate with synthesis techniques is often more accurately described as audio processing.\nConversely, many effects processors—such as filters—are integral to sound synthesis. The boundary between synthesis and processing is thus fluid and contextual.\n\n\n\nFig.\n\n\nTry both the classic AM and RM examples. In both cases, sidebands are generated for each sine wave component within the carrier signal. AM retains the carrier’s original sine components, which coexist and interact with the generated sidebands. For this reason, AM is commonly used for tremolo effects (which we’ll examine later). On the other hand, RM removes the original sine components entirely, yielding a more sonically distinctive result.\n\n\n4.5.5 Other Waveforms\nUsing more complex waveforms for the modulator signal leads to the creation of additional partials within any amplitude modulation patch—including ring modulation. Sine waves are typically favored as modulators since they offer clean and controlled results, especially when applying AM as an audio effect.\n\n\n\nFig.\n\n\nHowever, in synthesis contexts, more intricate and harmonically rich methods—such as frequency and phase modulation—offer more efficient and versatile approaches for generating complex timbres. We’ll explore these in the following sections.\n\n\n4.5.6 Tremolo\nTremolo is essentially amplitude modulation using a low-frequency modulator. The key addition is a depth parameter, ranging from 0 to 1, which determines the modulation intensity. At 0, no modulation occurs (dry signal), while a depth of 1 results in full tremolo, where the carrier’s amplitude is modulated across its full range.\n\n\n\nFig.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#frequency-modulation",
    "href": "chapters/synthesis.html#frequency-modulation",
    "title": "4  Synthesis",
    "section": "4.6 Frequency Modulation",
    "text": "4.6 Frequency Modulation\nIn general terms, to modulate a signal means to alter it in some way. In the context of this course, however, we refer specifically to using a modulating signal to control a parameter—such as amplitude, as previously discussed. We now turn to the basic structure of frequency modulation (FM), where an oscillator acts as the modulator.\nThe signal being modulated is called the carrier; in the case of frequency modulation, it is also referred to as the carrier frequency. In contrast, we have the modulating frequency, which corresponds to the frequency of the modulating oscillator. The depth of frequency variation is determined by the amplitude of the modulator and is commonly referred to as the modulation index. The modulation process itself is straightforward: we add the modulating signal to the frequency input of the carrier oscillator. See the example on the right\n\n\n\nFig.\n\n\nBy default, we have a carrier frequency of 400 Hz, a low modulation frequency of 1 Hz, and a modulation index of 100. This means the modulating signal oscillates between -100 and +100 Hz, causing the carrier frequency to vary between 300 and 500 Hz. Note that when the modulation frequency is low, the result is a vibrato-like effect.\n\n4.6.1 FM Simple\nWe apply the same structure as before. Besides a vibrato example, this section includes fully developed FM examples. As with amplitude modulation, FM produces sidebands spaced by intervals equal to the modulation frequency. However, FM can generate many more sidebands, potentially resulting in a much richer spectrum.\n\n\n\nFig.\n\n\nThe higher the modulation index, the greater the number of resulting partials—enabling the creation of dense and complex waveforms. When the carrier and modulator frequencies share a simple harmonic ratio, the resulting waveform is harmonic. Otherwise, it tends to be inharmonic. Click on the message boxes to hear the preset examples.\n\n\n4.6.2 Other Waveforms\nIn this patch, we experiment with different oscillator combinations. Waveforms are arranged according to spectral complexity—from simple sine waves to rich sawtooth waves (the only waveform here containing both even and odd harmonics). The more complex the waveform used, the more intricate the FM result becomes.\n\n\n\nFig.\n\n\nOn the modulator side, the waveforms are idealized and band-unlimited—perfect in theory. However, the frequency-modulated oscillator has a limited bandwidth, which imposes practical constraints on the resulting spectrum.\n\n\n4.6.3 Exponential Frequency\nWe can also use exponential pitch values (such as MIDI note numbers) instead of linear frequency input. The key difference here is that frequency deviation—the modulation index—is now expressed in semitones, not Hertz. This shift affects the entire modulation behavior.\n\n\n\nFig.\n\n\nAs a result, the output waveform becomes asymmetrical and significantly different in character. The main change in the patch is the use of [mtof~] to convert MIDI pitch to frequency in Hz.\n\n\n4.6.4 Ratio\nIt is common practice to define the modulating frequency as a ratio of the carrier frequency. This allows us to work with a single frequency input while maintaining a consistent sonic character across different pitches.\n\n\n\nFig.\n\n\nThis approach preserves the relationship between the carrier and modulator frequencies, which is crucial since this ratio determines how the additional spectral components—partials—are distributed. Harmonic ratios (such as 0.5 or 2) yield harmonic results, while non-integer ratios lead to inharmonic spectra.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#references",
    "href": "chapters/synthesis.html#references",
    "title": "4  Synthesis",
    "section": "References",
    "text": "References\n\n\n\n\nHelmholtz, Hermann. 1954. On the Sensations of Tone as a Physiological Basis for the Theory of Music. 2nd ed. New York: Dover Publications.\n\n\nLévi-Strauss, Claude. 1964. Le Cru Et Le Cuit. Vol. 1. Mythologiques. Paris: Plon.\n\n\nSterne, Jonathan, and Tara Rodgers. 2011. “Poetics of Signal Processing.” Differences 22 (2-3): 31–53. https://doi.org/10.1215/10407391-1428834.\n\n\nThéberge, Paul. 1997. Any Sound You Can Imagine: Making Music/Consuming Technology. Hanover & London: Wesleyan University Press.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html",
    "href": "chapters/sonification.html",
    "title": "5  Sonification",
    "section": "",
    "text": "5.1 Introduction\nIn this chapter, we will explore the concept of sonification, its applications, and how it can be implemented using Pure Data (Pd). We will also discuss the importance of understanding data types and classifications in the context of sonification.\nImagine hearing the changes in global temperature over the past thousand years. What does a brainwave sound like? How can sound be used to enhance a pilot’s performance in the cockpit? These intriguing questions, among many others, fall within the realm of auditory display and sonification.\nResearchers in Auditory Display explore how the human auditory system can serve as a primary interface channel for communicating and conveying information. The goal of auditory display is to foster a deeper understanding or appreciation of the patterns and structures embedded in data beyond what is visible on the screen.\nAuditory display encompasses all aspects of human-computer interaction systems, including the hardware setup (speakers or headphones), modes of interaction with the display system, and any technical solutions for data collection, processing, and computation needed to generate sound in response to data.\nIn contrast, sonification is a core technique within auditory display: the process of rendering sound from data and interactions. Unlike voice interfaces or artistic soundscapes, auditory displays have gained increasing attention in recent years and are becoming a standard method alongside visualization for presenting data across diverse contexts.\nThe international research effort to understand every aspect of auditory display began with the founding of the International Community for Auditory Display (ICAD) in 1992. It is fascinating to observe how sonification and auditory display techniques have evolved in the relatively short time since their formal definition, with development accelerating steadily since 2011.\nAuditory display and sonification are now employed across a wide range of fields. Applications include chaos theory, biomedicine, interfaces for visually impaired users, data mining, seismology, desktop and mobile computing interaction, among many others.\nEqually diverse is the set of research disciplines required for successful sonification: physics, acoustics, psychoacoustics, perceptual research, sound engineering, and computer science form the core technical foundations. However, psychology, musicology, cognitive science, linguistics, pedagogy, social sciences, and philosophy are also essential for a comprehensive, multifaceted understanding of the description, technical implementation, usage, training, comprehension, acceptance, evaluation, and ergonomics of auditory displays and sonification in particular.\nIt is clear that in such an interdisciplinary field, a narrow focus on any single discipline risks “seeing the trees but missing the forest.” As with all interdisciplinary research efforts, auditory display and sonification face significant challenges, ranging from differing theoretical orientations across fields to even the very vocabulary used to describe our work.\nInterdisciplinary dialogue is crucial to advancing auditory display and sonification. However, the field must overcome the challenge of developing and employing a shared language that integrates many divergent disciplinary ways of speaking, thinking, and approaching problems. On the other hand, this very challenge often unlocks great creative potential and new ideas, as these varied perspectives can spark innovation and fresh insights.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#data-types",
    "href": "chapters/sonification.html#data-types",
    "title": "5  Sonification",
    "section": "5.2 Data Types",
    "text": "5.2 Data Types\nIn the realm of digital data, understanding the nature and classification of data types is essential for their effective processing, storage, and analysis. The table above presents a detailed taxonomy of data types broadly categorized into Static and Stream / Realtime data, further subdivided into various subtypes. This section will unpack these classifications, elaborating on their characteristics, common formats, and sources.\n\n5.2.1 Static Data\nStatic data refers to data that is stored and remains unchanged until explicitly modified. It is typically collected, stored, and then analyzed or processed offline or asynchronously. This type of data is fundamental in many computational tasks, including machine learning, data mining, and archival storage.\n\n5.2.1.1 Structured Data\nStructured data is organized in a predefined manner, typically conforming to a schema or model that allows easy querying and manipulation.\nExamples:\n\nDatasets (CSV, XLS): Tabular data with rows and columns, where each column has a defined datatype (e.g., numeric, string, datetime).\nFields: The basic elements of structured data that have specific data types, such as integers, floating-point numbers, text strings, or timestamps.\nClasses: Labels used in supervised learning, often binary (two classes) or multiclass (multiple categories).\nImages: Digital images can be structured if stored with metadata or standardized file formats. Examples include compressed formats like JPG and uncompressed formats like BMP.\nMIDI Files: Musical Instrument Digital Interface files encode structured note and control information.\nAudio: Raw waveforms or extracted descriptors (e.g., Mel-frequency cepstral coefficients - MFCCs, Fourier transforms) represent audio signals in structured formats.\nAudio Formats: Common audio file formats such as MP3 (compressed), FLAC (lossless compressed), and WAV (uncompressed).\n\nSources: Data portals (e.g., Kaggle, UCI Machine Learning Repository), social media platforms like Instagram, Reddit, Flickr (for images), and audio repositories.\n\n\n5.2.1.2 Semi-structured Data\nSemi-structured data does not conform to a rigid schema like structured data but still contains tags or markers to separate semantic elements.\nExamples:\n\nMarkup Languages: HTML, XML, JSON, and YAML files are examples of semi-structured data because they contain hierarchical tags and attributes describing the data, but the content may be variable.\n\nSources: Web content, APIs that deliver data in JSON or XML formats.\n\n\n5.2.1.3 Unstructured Data\nUnstructured data lacks a predefined data model or schema. It is the most common form of data generated and can be more challenging to analyze without preprocessing.\nExamples:\n\nTexts: Documents, emails, articles, or social media posts are typical unstructured data examples.\n\nSources: Document collections, text corpora, email archives.\n\n\n\n5.2.2 Stream / Realtime Data\nStream or realtime data refers to continuous data generated in real-time, often requiring immediate or near-immediate processing. These data types are crucial in applications like live monitoring, interactive systems, and real-time analytics.\nExamples:\n\nAudio Streams: Continuous audio feeds such as online radio broadcasts.\nVideo Streams: Live video feeds from CCTV cameras or autonomous vehicles.\nSensor Data: Real-time telemetry or environmental data from IoT devices, Arduino boards, or embedded systems.\nLive MIDI: Streaming musical performance data, used in live concerts or interactive installations.\nOSC (Open Sound Control): A protocol used for networking sound synthesizers, computers, and other multimedia devices, enabling real-time control.\n\nSources: Online radio platforms, surveillance systems, smart vehicles, embedded IoT devices, live music performance setups.\n\n\n5.2.3 Data Classification\nThe classification of data into static and stream/realtime reflects fundamentally different operational paradigms in digital systems. Static data often allows batch processing, archival, and complex analysis without stringent timing constraints. In contrast, stream data demands continuous, low-latency processing to support live feedback, monitoring, or interaction.\nThe structured / semi-structured / unstructured distinction highlights the complexity of dealing with data formats:\n\nStructured data is well-suited for traditional databases and straightforward analysis.\nSemi-structured data requires flexible parsers and understanding of nested or tagged data.\nUnstructured data often necessitates advanced techniques such as natural language processing or computer vision for meaningful extraction.\n\nUnderstanding these data types and their sources is critical when designing systems for data ingestion, storage, processing, and analysis — especially in fields such as machine learning, multimedia processing, and IoT applications.\n\n\n\n\n\n\n\n\n\nType\nSubtype\nExamples\nSources\n\n\n\n\nStatic\nStructured\nSemi-structured Unstructured\n\nDatasets (CSV, XLS)\nFields: numeric, string, datetime\nClasses: binary, multiclass\nImages (compressed: JPG, uncompressed: BMP)\nMIDI files\nAudio: raw, descriptors, Fourier\nAudio formats: MP3, FLAC, WAV\nMarkup languages: HTML, XML, JSON, YAML\nTexts\n\n\nData portals\nInstagram, Reddit, Flickr\nAudio repositories\nWeb, APIs\nDocument collections\n\n\n\nStream / Realtime\n—\n\nAudio streams (e.g. online radio)\nVideo streams (e.g. CCTV, autonomous vehicles)\nSensor data (e.g. Arduino, real-time telemetry)\nLive MIDI\nOSC (Open Sound Control)\n\n\nOnline radio platforms\nSurveillance systems, smart vehicles\nIoT devices, embedded systems\nLive performance setups\nInteractive art and media systems\n\n\n\n\n\n5.2.3.1 References\nGantz, J., & Reinsel, D. (2012). The Digital Universe in 2020: Big Data, Bigger Digital Shadows, and Biggest Growth in the Far East. IDC iView.\nMarr, B. (2016). Big Data in Practice: How 45 Successful Companies Used Big Data Analytics to Deliver Extraordinary Results. Wiley.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#sonification-as-a-creative-framework",
    "href": "chapters/sonification.html#sonification-as-a-creative-framework",
    "title": "5  Sonification",
    "section": "5.3 Sonification as a Creative Framework",
    "text": "5.3 Sonification as a Creative Framework\nSonification, traditionally defined as the systematic, objective, and reproducible translation of data into sound, finds itself at a dynamic intersection between scientific inquiry and creative expression. While often framed within the methodological rigor of data representation, its deeper potential lies equally in its artistic embodiment—an expressive convergence of logic, perception, and auditory imagination. This duality is vital when considering the use of software in the development of creative code practices for sonification. In its essence, sonification provides a means to explore and interpret data temporally. It transforms static information into dynamic sonic experiences, revealing patterns not just to the analytical mind but to the aesthetic ear. As Gresham-Lancaster (Gresham-Lancaster 2012) argues, the field must extend beyond scientific formalism to embrace a broader spectrum of cultural and musical meaning.\nA foundational concept introduced by Gresham-Lancaster is the distinction between first-order and second-order sonification. First-order sonification typically involves straightforward mapping of data points to sound parameters—e.g., a numerical value controlling oscillator frequency or filter cutoff. This direct translation preserves the integrity of the data but may lack emotional resonance or contextual clarity for listeners unfamiliar with the dataset or mappings. Second-order sonification introduces a higher level of abstraction. Here, data is not merely converted to sound but is used to control compositional structures such as rhythm, harmony, timbre, and formal development. This could manifest through software designs that incorporate statistical analysis of datasets to determine musical motifs or drive stochastic processes that evolve over time. By embedding data within culturally familiar or stylistically resonant frameworks—be it ambient textures, rhythmic patterns, or harmonic progressions allows the sonification to become more legible and emotionally impactful.\nA key insight is that listeners inevitably interpret sound within a cultural and stylistic frame. Whether intentional or not, sonification software maps data to sonic outputs will be perceived through the lens of genre conventions—ambient, noise, glitch, minimalism, etc. This reinforces the necessity for designers of sonification systems to consciously engage with aesthetic decisions. Rather than viewing these as distractions from scientific rigor, they should be recognized as essential design variables that determine how well the sonification communicates. For instance, layering the sonified stream within an evolving drone texture or embedding it in rhythmic pulses tied to diurnal cycles may enhance the intelligibility of subtle shifts. These choices, while extramusical, profoundly affect the user’s ability to detect and interpret meaningful changes.\nThe expressive capacity of sonification technics invites an analogy with sculpture, as cited in Xenakis’ conception of raw mathematical outputs as “virtual stones” to be artistically shaped. The creative coder similarly refines the raw outputs of data-driven patches, sculpting auditory forms through iterative experimentation, parameter tuning, and aesthetic discernment. This process reveals sonification not as a deterministic output of data, but as a collaboration between the structure of information and the intuition of the artist-programmer. Moreover, by incorporating techniques such as timbral mapping, adaptive filtering, and data-driven granular synthesis, it becomes a laboratory for crafting sonic experiences that honor both the source data and the listener’s perceptual world. This is particularly effective when the goal is to embed sonification within broader artistic or performative contexts—installations, interactive systems, or generative concerts—where expressivity and audience engagement are critical.\nThe evolution of sonification within the creative coding domain demands a reconceptualization of what constitutes success in sonification. Beyond reproducibility, the true measure lies in perceptual clarity, aesthetic engagement, and communicative power. As Gresham-Lancaster asserts, the inclusion of musical form, stylistic awareness, and cultural embedding are not luxuries, but necessities for sonification to become a widely adopted and meaningful practice Pure Data, with its flexibility, open-ended structure, and visual coding paradigm, provides an ideal environment to develop and test both first-order and second-order sonification systems. By embracing both scientific precision and artistic insight, Pd-based sonification becomes a model for interdisciplinary practice—where the auditory exploration of data is not just informative but transformative.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#data-humanism-a-visual-manifesto-of-giorgia-lupi",
    "href": "chapters/sonification.html#data-humanism-a-visual-manifesto-of-giorgia-lupi",
    "title": "5  Sonification",
    "section": "5.4 Data Humanism: A Visual Manifesto of Giorgia Lupi",
    "text": "5.4 Data Humanism: A Visual Manifesto of Giorgia Lupi\n\n\n\nHumanism of Data\n\n\nData is now recognized as one of the foundational pillars of our economy, and the idea that the world is exponentially enriched with data every day has long ceased to be news.\nBig Data is no longer a distant dystopian future; it is a commodity and an intrinsic, iconic feature of our present—alongside dollars, concrete, automobiles, and Helvetica. The ways we relate to data are evolving faster than we realize, and our minds and bodies are naturally adapting to this new hybrid reality built from physical and informational structures. Visual design, with its unique power to reach deep into our subconscious instantly—bypassing language—and its inherent ability to convey vast amounts of structured and unstructured information across cultures, will play an even more central role in this quiet yet inevitable revolution.\nPioneers of data visualization like William Playfair, John Snow, Florence Nightingale, and Charles Joseph Minard were the first to harness and codify this potential in the 18th and 19th centuries. Modern advocates such as Edward Tufte, Ben Shneiderman, Jeffrey Heer, and Alberto Cairo have been instrumental in the field’s renaissance over the past twenty years, supporting the transition of these principles into the world of Big Data.\nThanks to this renewed interest, an initial wave of data visualization swept across the web, reaching a wider audience beyond the academic circles where it had previously been confined. Unfortunately, this wave was often ridden superficially—used as a linguistic shortcut to cope with the overwhelming nature of Big Data.\n“Cool” infographics promised a key to mastering this untamable complexity. When they inevitably failed to deliver on this overly optimistic expectation, we were left with gigabytes of illegible 3D pie charts and cheap, translucent user interfaces cluttered with widgets that even Tony Stark or John Anderton from Minority Report would struggle to understand.\nIn reality, visual design is often applied to data merely as a cosmetic gloss over serious and complicated problems—an attempt to make them appear simpler than they truly are. What made cheap marketing infographics so popular is perhaps their greatest contradiction: the false claim that a few pictograms and large numbers inherently have the power to “simplify complexity.” The phenomena that govern our world are, by definition, complex, multifaceted, and often difficult to grasp. So why would anyone want to dumb them down when making critical decisions or delivering important messages?\nYet, not all is bleak in this sudden craze for data visualization. We are becoming increasingly aware that there remains a considerable gap between the real potential hidden within vast datasets and the superficial images we typically use to represent them. More importantly, we now recognize that the first wave succeeded in familiarizing a broader audience with new visual languages and tools.\nHaving moved past what we might call the peak of infographics, we are left with a general audience equipped with some of the necessary skills to welcome a second wave of more meaningful, thoughtful visualization.\nWe are ready to question the impersonality of a purely technical approach to data and begin designing ways to connect numbers with what they truly represent: knowledge, behaviors, and people.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#references-1",
    "href": "chapters/sonification.html#references-1",
    "title": "5  Sonification",
    "section": "References",
    "text": "References\n\n\n\n\nGresham-Lancaster, Scot. 2012. “Relationships of Sonification to Music and Sound Art.” AI and Society 27 (2): 207–12.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/space.html",
    "href": "chapters/space.html",
    "title": "6  Space",
    "section": "",
    "text": "6.1 The Music Sound Space\nIn the realm of sound, the concept of space is often overlooked. Yet, it plays a crucial role in shaping our auditory experience. The spatial dimension of sound is not merely an acoustic phenomenon; it is a fundamental aspect of how we perceive sound. This chapter explores the multifaceted nature of sound space, its historical evolution, and its implications for contemporary electroacoustic music.\nSound always unfolds within a specific time and space. From the very moment sound is generated in music, space is implicitly present, enabling the possibility of organizing, reconstructing, and shaping that space to influence musical form. Thus, the use of space in music responds to concerns that go far beyond mere acoustic considerations.\nConceiving space as a structural element in the construction of sonic discourse requires us to address a complex notion—one that touches on multiple dimensions of composition, performance, and perception. This idea rests on the argument that space is a composite musical element, one that can be integrated into a compositional structure and assume a significant role within the formal hierarchy of sound discourse.\nSpatiality in music represents a compositional variable with a long historical lineage. The treatment of sound space on stage can be traced back to classical Greek theatre, where actors and chorus members used masks to amplify vocal resonance and enhance vocal directionality (Knudsen 1932). However, while compositional techniques addressing spatial sound have been developing for centuries, it is not until the early 20th century that a systematic use of spatial dimensions becomes central to musical structure. Landmark works such as Universe Symphony (1911–51) by Charles Ives, Déserts (1954) and Poème électronique (1958) by Edgard Varèse, Gruppen (1955–57) by Karlheinz Stockhausen, and Persephassa (1969) by Iannis Xenakis approach spatiality as an independent and structural dimension. In these works, space is interrelated with other sonic parameters such as timbre, dynamics, duration, and pitch.\nParticularly noteworthy is that these pieces do not merely establish sonic space as a new musical dimension—they also develop theoretical frameworks for the spatialization of sound. These theories consider physical, poetic, pictorial, and perceptual aspects of spatial sound. As a result, in recent decades the areas of research and application related to artistic-musical knowledge have expanded significantly, fostering interdisciplinary inquiry into sound space and its parameters.\nBeyond its central role in music, in recent years spatial sound has also become a subject of study across a variety of disciplines. One example of this is found in research into sound spatialization techniques in both real and virtual acoustic environments. These techniques have largely evolved based on principles of psychoacoustics, cognitive modeling, and technological advancements, leading to the development of specialized hardware and software for spatial sound processing. In the scientific realm, theses and research articles have explored, in great detail, the cues necessary to create an acoustic image of a virtual source and how these cues must be simulated—whether by electronic circuitry or computational algorithms.\nHowever, it must be noted that musical techniques for managing sound space have not yet fully integrated findings from perceptual science. As we will explore later, many artistic approaches rely on only a subset of the cues involved in auditory spatial perception, overlooking others that are essential for producing a convincing acoustic image.\nIn general terms, the spatial dimension of a musical composition can function as the primary expressive and communicative attribute for the listener. For instance, from the very first moment, the acoustic characteristics of a venue—real or virtual—directly affect how we perceive the sonic discourse. This dimension of musical space is further shaped by the placement of sound sources, whether instrumentalists or loudspeakers. A historical precedent for this can be found in the Renaissance period through the use of divided choirs in the Basilica of San Marco in Venice. This stylistic practice, known as antiphonal music, represents a clear antecedent of the deep connection between musical construction and architectural space (Stockhausen 1959). The style of Venetian composers was strongly influenced by the acoustics and architectural features of San Marco, involving spatially separated choirs or instrumental groups performing in alternation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Space</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#the-music-sound-space",
    "href": "chapters/space.html#the-music-sound-space",
    "title": "6  Space",
    "section": "",
    "text": "Figure 1: Floor plan of the Basilica of San Marco in Venice (Italy).\n\n\n\n6.1.1 Spatial Distribution and the Evolution of Listening Spaces\nDuring the European Classical and Romantic periods, the spatial distribution of musicians generally adhered to the French ideal inspired by 19th-century military band traditions. In this widely adopted setup, space was typically divided into two main zones: musicians arranged on a frontal stage and the audience seated facing them. This concert model—linear, frontal, and fixed—reflected a formalized, hierarchical approach to the musical experience.\nBy the 20th century, this front-facing paradigm began to be reexamined and reimagined. Composers and sound artists started to question the limitations of conventional concert hall layouts. A notable example is found in 1962, when German composer Karlheinz Stockhausen proposed a radical redesign of the concert space to accommodate the evolving needs of contemporary composition. His vision included a circular venue without a fixed podium, flexible seating arrangements, adaptable ceiling and wall surfaces for mounting loudspeakers and microphones, suspended balconies for musicians, and configurable acoustic properties. These directives were not merely architectural; they were compositional in nature, meant to transform the listening experience by embedding spatiality into the core of musical structure.\nThis reconceptualization of space enabled a new way of thinking about the sensations and experiences that could be elicited in the listener through the manipulation of spatial audio. The concert hall was no longer a passive container of sound, but an active participant in its articulation.\nIn parallel developments, particularly in the field of electroacoustic music during the mid-20th century, spatial concerns were often shaped by the available technology of the time. Spatiality lacked a clearly defined theoretical vocabulary and was not yet integrated systematically with other familiar musical parameters. Nonetheless, from its inception, electroacoustic music leveraged technology to expand musical boundaries—either through electronic signal processing or by interacting with traditional instruments.\nHowever, the notion of space in electroacoustic music differs significantly from that of instrumental music. In this context, spatiality is broad, multidimensional, and inherently difficult to define. It encompasses not only the individual sonic identity of each source—typically reproduced through loudspeakers—but also the relationships between those sources and the acoustic space in which they are heard. Critically, the audible experience of space unfolds through the temporal evolution of the sonic discourse itself.\nLoudspeakers in electroacoustic music afford a uniquely flexible means of organizing space as a structural element. The spatial potential enabled by electroacoustic technologies has been, and continues to be, highly significant. With a single device (see Fig. 2), it is possible to transport the listener into a wide range of virtual environments, expanding the scope of musical space beyond the physical limitations of the performance venue. This opens the door to sonic representations of distance, movement, and directionality—factors that can be fully integrated into the musical structure and treated as compositional dimensions in their own right.\n\n\n\nFigure 2: Multichannel electroacoustic concert hall.\n\n\n\n\n\n6.1.2 Spatial Recontextualization and Auditory Expectation\nOne of the defining characteristics of electroacoustic music is its capacity to recontextualize sound. Through virtual spatialization, a sound can acquire new meaning depending on how it functions within diverse contexts. For example, two sounds that would never naturally coexist—such as a thunderstorm and a mechanical engine in the same acoustic scene—can be juxtaposed to create a landscape that defies ecological realism. This deliberate disjunction can trigger a complex interaction between what is heard and the listener’s prior knowledge of those sound sources, drawn from personal experience.\nOur ability to interpret spatial cues in sound is deeply shaped by the patterns of interpersonal communication we engage in, the lived experience of urban or rural environments, and the architectural features of our surroundings. These formative influences are so embedded in our perceptual systems that we are often unaware of their role in shaping how we understand sensory information. Spatial cues are constantly processed in our everyday auditory experience and play a vital role in shaping our listening behaviors.\nIn the context of electroacoustic music, environmental cues may not only suggest associations with a physical location but also inform more abstract properties of the spatial discourse articulated by the piece. Jean-Claude Risset (Risett 1969) noted a tension in using environmental recordings in electroacoustic composition: the recognizable identity of “natural” sounds resists transformation without diminishing their spatial content. Nevertheless, identifiable sounds remain central to electroacoustic works, particularly those aligned with the tradition of musique concrète.\nUnder normal listening conditions, perception of a singular sound source is shaped by what Pierre Schaeffer (Schaeffer 2003) defined as the “sound object”—a sonic phenomenon perceived as a coherent whole, grasped through a form of reduced listening that focuses on the sound itself, independent of its origin or meaning. Our auditory system has evolved to identify and locate such sound objects in space by mapping them according to the physical attributes of their sources. A barking sound is understood as coming from a dog; a voice is mapped to a human speaker. As a result, the spatial interpretation of sound is closely linked to the listener’s expectations, shaped not only by the inherent acoustic characteristics of the signal but also by the listener’s accumulated learning and experience.\nUnlike incidental listening in everyday life, attentive listening in electroacoustic music generally occurs from a fixed position. Apart from minor head movements, the spatial information available to the listener depends entirely on the acoustic cues encoded in the sound. While spatial hearing has been extensively researched in the field of psychoacoustics, its compositional potential as a carrier of musical form remains underexplored. In particular, the dimension of distance has received little attention compared to azimuth (horizontal angle) and elevation (vertical angle). These two spatial dimensions have been rigorously studied and well-documented, forming the basis of many standard models of spatial perception.\nBy contrast, the auditory perception of distance remains one of the most enigmatic topics in both music and psychophysics. It involves a complex array of cues—many of which are still not fully understood or integrated into compositional practice. As such, distance remains an open and promising field of inquiry for creative and scientific exploration alike (Abregú, Calcagno, and Vergara 2012).\n\n\n\nFigure 3: Schematic representation of the azimuth, elevation, and distance axes.\n\n\n\n\n\n6.1.3 The Expanded Spatial Palette of Electroacoustic Music\nUnlike instrumental music, which relies on the physicality of pre-existing sound sources, electroacoustic music is not constrained by such limitations. This fundamental distinction allows composers not only to design entirely new sounds tailored to their spatial intentions but also to explore acoustic environments that defy conventional physical logic. While a performance of acoustic music offers visual and sonic cues grounded in the material world—a stage, a hall, visible instruments—electroacoustic music invites a more abstract engagement, where the listener navigates a virtual sound world.\nIn acoustic settings, physical space provides the listener with consistent spatial information. In contrast, electroacoustic environments—particularly those utilizing multichannel reproduction—radically extend the spatial canvas. These environments challenge and expand our auditory expectations, offering a broader spectrum of spatial strategies shaped by the interaction between sound diffusion technologies and the perceptual mechanisms of the listener.\nYet, this very potential also brings complications. The ephemeral and immaterial nature of loudspeaker-based sound makes it difficult to generate a spatial image as vivid or intuitive as that encountered in the physical world. Despite technological precision, virtual spatial environments often lack the tactile immediacy of real acoustic spaces.\nResearch in the field of computer music typically falls into two major domains: the study of spatial hearing and cognition, and the development of sound reproduction technologies. One of the advantages in electroacoustic contexts is the ability to isolate and manipulate spatial cues independently—a task nearly impossible with traditional acoustic instruments. This level of control opens up possibilities for treating space as a fully sculptable compositional parameter. However, practical results vary greatly, and many perceptual questions remain unresolved. It is within this intersection—where psychophysics, spatialization technologies, and compositional imagination meet—that the most fertile ground for innovation lies.\nA classic example is John Chowning’s Turenas (1972), which employed quadraphonic speaker arrangements and a mathematical model to simulate virtual spatial motion. Chowning implemented a distance-dependent intensity multiplier according to the inverse square law, applied independently to each channel. The outcome was a virtual “phantom source” whose movement through space was dynamically shaped and perceptually engaging. Interestingly, Chowning discovered that simple, well-structured paths—such as basic geometric curves—produced the most convincing spatial gestures. In On Sonic Art (Wishart 1996), one finds multiple visual representations of two-dimensional spatial trajectories; however, these diagrams often fail to translate into equally perceptible auditory experiences. For instance, while Lissajous curves are visually complex and elegant, their perceptual distinctiveness can be minimal. Clarity, it turns out, is key to audible spatial expression.\nSince the mid-20th century, various spatialization systems have emerged to simulate acoustic spaces through loudspeaker arrays. Among the most notable are Intensity Panning, Ambisonics, and Dolby 5.1. Although a detailed technical comparison of these systems lies beyond the scope of this section (see (Basso, Di Liscia, and Pampin 2009) for an extensive overview), it is worth noting that electroacoustic space only becomes meaningful if its structural characteristics—such as reverberation—can be clearly perceived. In orchestral music, spatial features are often inherent to the instrument layout and performance context. With fixed instrument positions, the spatial configuration is relatively stable. In virtual space, however, reverberation must be explicitly created and shaped to define the acoustic character of the environment in which sonic events unfold.\nUnlike the relatively fixed reverberation properties of physical rooms, virtual spaces offer remarkable plasticity. Composers can craft distinct acoustic identities for different sections of a piece, allowing each space to function as a compositional agent in its own right. This flexibility implies that in electroacoustic music, space must often be invented—not merely inherited from the sounding objects, as is typically the case with acoustic instruments.\nIt becomes clear, then, that spatial design can be a powerful structural element in music. Sound space carries poetic, aesthetic, and expressive dimensions. At the same time, convincing spatial construction often depends on our embodied experience of real-world acoustics. This dual grounding—in perceptual realism and creative abstraction—opens new theoretical and practical avenues for artists. The potential to reconceptualize spatiality through an expanded compositional lens can empower creators to engage more fully with the multidimensionality of sonic art. In doing so, it enables the development of richer analytical and taxonomical criteria that extend beyond sound itself, embracing broader cultural, perceptual, and technological contexts.\n\n\n\n\nAbregú, E. L., E. R. Calcagno, and R. O. Vergara. 2012. “La Distancia Como Factor Estructural de La Música.” Revista Argentina de Musicología, no. 12-13: 379–400.\n\n\nBasso, Gustavo, Oscar Pablo Di Liscia, and Juan Pampin, eds. 2009. Música y Espacio: Ciencia, Tecnología y Estética. Buenos Aires: Editorial de la Universidad Nacional de Quilmes.\n\n\nKnudsen, V. O. 1932. Architectural Acoustics. J. Wiley y Sons.\n\n\nRisett, J. C. 1969. “An Introductory Catalogue of Computer Synthesized Sounds.” Nueva Jersey: Bell Telephone Laboratories.\n\n\nSchaeffer, Pierre. 2003. Tratado de Los Objetos Musicales. Translated by Araceli Cabezón de Diego. Alianza Editorial.\n\n\nStockhausen, Karlheinz. 1959. “Musik Im Raum.” Die Reihe, no. 5.\n\n\nWishart, Trevor. 1996. On Sonic Art. Amsterdam: Harwood Academic Publishers.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Space</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html",
    "href": "chapters/conclusion.html",
    "title": "8  Conclusion",
    "section": "",
    "text": "8.1 Key Takeaways\nIn this chapter, we summarize the key points discussed throughout the book. We reflect on the main topics introduced in the first chapter and the detailed discussions in the second chapter.\nThank you for reading! We hope this book has provided valuable insights and knowledge.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html#key-takeaways",
    "href": "chapters/conclusion.html#key-takeaways",
    "title": "8  Conclusion",
    "section": "",
    "text": "Highlight the main objectives of the book.\nDiscuss the significance of the topics covered.\nEncourage further exploration and learning in the subject area.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]