[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Code that Sounds",
    "section": "",
    "text": "Preface\nThis book was born out of the classroom—but it does not stay there. It is the result of years of teaching creative coding and interactive media at public universities, engaging with students from a wide range of disciplines, backgrounds, and interests. Over time, a common thread emerged: the desire to bridge artistic expression and technical skill, to write code not just as a means to an end, but as a space of exploration, experimentation, and play.\nIn these pages, you’ll find the distilled insights, exercises, and creative strategies that have shaped countless workshops and academic programs. The goal has always been twofold: to equip readers with the tools to build interactive digital systems, and to nurture a mindset where sound, movement, code, and structure can be explored as artistic materials. Whether it’s a generative soundscape, a data-driven artwork, or a custom tool built from scratch, the projects in this book are designed to foster technical growth while encouraging individual expression.\nThis is not a manual in the traditional sense, nor is it a fixed curriculum. Instead, this book invites you to engage with Pure Data on your own terms, navigating its chapters in whatever order best suits your curiosity. The modular structure is intentional: it supports creative detours, sudden insights, and unexpected connections between ideas. You are encouraged to experiment, remix, and stretch the boundaries of the examples presented. Treat the book as both a guide and a sandbox—one where art, sound, code, and interactivity come alive through your engagement.\nWhether you’re an artist exploring new tools, a programmer seeking creative outlets, or a student diving into the world of interactive media, I hope this book helps you discover how code can become a language for imagination.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-am-i",
    "href": "index.html#who-am-i",
    "title": "Code that Sounds",
    "section": "Who am I?",
    "text": "Who am I?\nMy name is Ezequiel Abregú, and I am a sound artist, composer, multi-instrumentalist, and researcher originally from Buenos Aires, Argentina. My artistic practice encompasses sound recordings, audio installations, performances, sound sculptures, sound design, and compositions for chamber music, choreography, and theater. I am particularly interested in the interplay between music, performance, sound art, live electronics, auditory and visual perception, interactive media, and the application of technology in art.\n\n\n\n\nDr. Ezequiel Abregú\n\n\n\nI hold a Ph.D. focusing on the relationship between visual and auditory perception in sound art, and a degree in Composition with Electroacoustic Media from the National University of Quilmes (UNQ). My academic career includes teaching positions at several institutions: I am a professor at the University of Quilmes (Computing Applied to Music area), the National University of Arts (Multimedia Arts area), and the University of Tres de Febrero (Electronic Arts area).\nMy passion for programming and digital audio applications has led me to explore various programming languages and tools over the past two decades, including C, C++, Python, and Pure Data. I am an advocate of the open-source philosophy, regularly working with Linux and sharing my projects in publicly accessible repositories. My technical expertise extends to hardware development using microcontrollers and single-board computers, enabling me to adopt a hands-on approach in both my artistic and research endeavors.\n\n\n\n\n\n\nMore information about my work can be found on my personal website ezequielabregu.net",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "Code that Sounds",
    "section": "What is this book about?",
    "text": "What is this book about?\nThe goal of this book is to support undergraduate and postgraduate students in exploring the intersection of creativity and technology alongside peers from diverse backgrounds. Building on years of teaching experience at several public universities1, this work encourages the integration of a creative mindset with programming skills to design original tools, algorithms, and artworks. Through this synthesis, the book invites students to engage with sound, interactivity, and control protocols in both technical and expressive dimensions.\nRather than offering a fixed, linear progression, the structure of this book is deliberately open and modular. Readers are encouraged to navigate the content according to their interests, needs, or curiosity. This flexibility supports an experimental approach to learning, where exploration and play are not only welcome but essential.\nBy approaching programming as a creative practice, this book invites you to think, make, and reflect through code. Whether you’re building an interactive sound installation, prototyping a digital instrument, or simply experimenting with new ideas, the goal is to empower you with the tools and concepts to express yourself in the digital domain—and to enjoy the process along the way.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-code-that-sounds",
    "href": "index.html#why-code-that-sounds",
    "title": "Code that Sounds",
    "section": "Why Code That Sounds?",
    "text": "Why Code That Sounds?\nThe title may appear straightforward at first, but upon deeper reflection, it reveals a rich conceptual landscape that frames the entire approach of this book. More than a literal description of code that produces audio, it invites us to consider the interplay between computation, perception, and artistic expression. In this book, we explore how code becomes a medium for crafting sonic experiences—how it can be shaped, composed, and performed to produce not only functional systems, but also resonant and meaningful works of art.\nAt the most immediate level, the phrase refers to the act of writing code that generates sound. Whether synthesizing a sine wave, constructing a step sequencer, or routing audio signals in real time, we are engaging with code as a tool for sonic creation. Tools like Pure Data make this direct connection between programming and audio particularly tangible: the act of connecting objects, defining behaviors, and mapping parameters becomes inseparable from the process of producing audible outcomes. In this sense, Code That Sounds expresses the essential relationship between the logical structure of algorithms and the materiality of sound.\nYet the phrase also invites a more poetic interpretation. “Sounds,” in English, can refer not only to audio but also to the way something resonates intellectually or emotionally. Code, then, is not merely a set of instructions—it is a language with its own voice, rhythm, and character. A piece of code can “sound right” in the same way that a musical phrase does. This expressive quality of code becomes especially relevant in the context of creative practice. As we will see throughout the book, the aesthetic choices embedded in code—its structure, logic, timing, and responsiveness—are as crucial as the sounds it ultimately produces. In this light, the act of programming becomes a compositional gesture, where syntax and semantics coalesce into a form of audible thinking.\nThis perspective positions code as a bridge between traditionally separate domains: between the analytical and the intuitive, the procedural and the performative, the technical and the poetic. Sound has long been associated with music, art, and embodiment; code, with abstraction and logic. But when we write Code That Sounds, we dissolve these boundaries. We begin to treat code not as a rigid mechanism, but as a generative medium—capable of producing form, emotion, and experience. The phrase thus embodies the interdisciplinary ethos that guides this book, where creative coding is not merely about mastering a tool, but about exploring how different ways of knowing and making converge through sound.\nThere is also a sense of agency embedded in the phrase. This is not just code that happens to produce sound—it is code that is designed to sound. The use of the active verb implies intention, purpose, and dynamism. It reminds us that the systems we build are not passive containers for sound, but active participants in shaping it. In interactive contexts especially, where the code responds to movement, sensors, or user input, the sounding becomes an emergent property of a living system. Here, the code is not simply a means to an end—it is the co-author of a sonic event.\nCode That Sounds gestures toward a methodology rooted in exploration and discovery. It encourages us to treat code as something to be listened to, not just debugged. What does a patch “sound like” when put into motion? What happens when you tweak a parameter or rewire a signal path? This book invites you to approach creative coding as a space of experimentation—where understanding emerges through making, and insight through listening. We embrace this attitude by providing examples, sketches, and exercises that are meant not to be followed blindly, but to be remixed, transformed, and extended.\nCode That Sounds captures the spirit of this book: it is about making sound with code, but also about listening to the voice of code itself. It is about finding resonance between systems and sensations, and about using programming as a medium for creative exploration. As you work through the following chapters, we encourage you to tune your ears not only to the sounds that emerge, but to the ideas, questions, and possibilities that those sounds carry with them. This is not just about learning how to code—this is about learning how to listen to what code can become.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#from-problem-to-algorithm",
    "href": "index.html#from-problem-to-algorithm",
    "title": "Code that Sounds",
    "section": "From Problem to Algorithm",
    "text": "From Problem to Algorithm\nOne of the central ideas guiding this book is that programming begins not with a tool, but with a question. Before we concern ourselves with implementation, with lines of code or patching wires in Pure Data, we start by identifying a problem—something that needs to be understood, shaped, or transformed. This problem might be technical (how to generate a rhythm evenly distributed over time?), perceptual (how to spatialize a sound to suggest motion?), or poetic (how to evoke a sense of disorientation through modulation?). Each chapter begins with such a prompt, inviting us to investigate the mechanics and metaphors behind the systems we aim to build.\nFrom this point, the process becomes analytical and compositional. We break down the problem into its smallest components, identify the parameters and constraints at play, and explore the structures—logical, mathematical, sonic—that underpin it. This leads us to design an algorithm: a set of steps, a conceptual model, or a generative rule that connects the problem to a potential solution. The algorithm is not just a procedure—it is a form of reasoning, a translation of intuition into system.\nOnly once this conceptual groundwork has been laid do we turn to implementation. The choice of environment—Pure Data, in our case—becomes relevant, but always in service of the larger question. It is not simply learning how to use a tool; you are using the tool to articulate a thought. This means thinking through code, not just writing it. It means seeing your patch not as an end, but as a hypothesis, a sketch, an evolving structure open to revision.\nThroughout the book, I invite you to cultivate this mindset. When studying an example patch, don’t ask only what it does—ask how it solves a problem. What are the assumptions embedded in its logic? Could the same result be achieved differently? What happens if you modify the rules, invert the process, or apply the algorithm in another context?\nBy working in this way, programming becomes more than an exercise in execution. It becomes a space of inquiry and invention—an extension of your thinking, your listening, your compositional practice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Code that Sounds",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book is for anyone drawn to the idea that code can become an expressive, sonic, and artistic material. It is designed for students, artists, educators, and curious minds—regardless of their technical background—who wish to explore programming as a medium for crafting sound, interactivity, and performance. Whether you’re new to creative coding or already familiar with digital media, this book will guide you through approaches that treat code as more than just a set of instructions. Here, programming becomes a form of listening, composing, and experimenting. Pure Data serves as our primary environment, but our focus lies in cultivating a creative and critical practice with sound and code at its core.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-you-going-to-learn",
    "href": "index.html#what-are-you-going-to-learn",
    "title": "Code that Sounds",
    "section": "What are you going to learn?",
    "text": "What are you going to learn?\nThroughout this book, you will learn how to develop and shape sonic experiences using code. You will build interactive systems in Pure Data that respond, transform, and sound—systems that move beyond the screen and into space, time, and perception. Along the way, you will engage with fundamental techniques of sound synthesis, algorithmic thinking, and generative design, all within a context that values exploration, composition, and artistic intent. Beyond tools and syntax, you will learn to listen to your code, to let it surprise you, and to use it as a means for creative discovery and auditory expression.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-you-not-going-to-learn",
    "href": "index.html#what-are-you-not-going-to-learn",
    "title": "Code that Sounds",
    "section": "What are you not going to learn?",
    "text": "What are you not going to learn?\nThis book does not attempt to provide exhaustive coverage of Pure Data or its many technical details. It is not a manual or reference guide. Rather than aiming for completeness, we prioritize depth over breadth: we focus on specific concepts, practices, and case studies that support an artistic and experimental approach. You won’t find here every object or external library documented, nor step-by-step instructions for GUI design or audio engineering. Instead, you will find a flexible, hands-on framework to begin crafting systems that sound, and from there, make them your own. This book works best in dialogue with other resources—especially the official Pure Data documentation—and invites you to learn by making, remixing, and listening.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#a-work-in-progress",
    "href": "index.html#a-work-in-progress",
    "title": "Code that Sounds",
    "section": "A Work in Progress",
    "text": "A Work in Progress\nBefore we delve deeper into the subject, let it be clear from the outset:\n\n\nThis book is, by design, a work in progress.\n\n\nThe intention is not to present a definitive compendium or a closed collection of recipes. Instead, what I offer here is a growing body of materials, case studies, and conceptual tools that evolve in parallel with the creative and technical challenges faced by artists and researchers working at the intersection of code, sound, and interactivity. The choice of Pure Data as the primary platform reflects both its openness and its suitability for rapid prototyping and conceptual clarity. But beyond that, this book aims to serve as a framework for thinking, not just for coding.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#backend-first-leaving-the-gui-for-later",
    "href": "index.html#backend-first-leaving-the-gui-for-later",
    "title": "Code that Sounds",
    "section": "Backend First: Leaving the GUI for Later",
    "text": "Backend First: Leaving the GUI for Later\nAnother important methodological decision in this book is our emphasis on the backend. We begin by focusing on signal flow, algorithmic thinking, and control structures—those components that shape the inner workings of a patch. In many cases, the graphical interface (GUI) can be a distraction from the deeper mechanics at play.\nBy concentrating first on backend logic, we build a strong foundation that can later support more refined user interactions. GUI design will certainly be addressed, but in future chapters, when we are equipped with a clearer understanding of how our systems behave and how we want them to evolve. In other words, we treat the visual layer as a representation of logic, not a replacement for it.\nThis book is an invitation to approach creative coding not as a set of shortcuts or pre-made solutions, but as a process of inquiry. We will get our hands dirty, make mistakes, and revise along the way. In doing so, we learn not just how to build systems, but how to think with and through them. That is the deeper promise of creative code.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Code that Sounds",
    "section": "Contributing",
    "text": "Contributing\nIf you would like to contribute to this book, please feel free to fork the repository and submit a pull request. I welcome any suggestions, corrections, or improvements to the content. You can also report issues or request features by opening an issue in the repository. You can find the source code for this book on GitHub.\n\n\n\n\n\n\nNote\n\n\n\nDo not write me emails with questions about PD.\nPlease, if you have questions about PD, post them on the Pure Data forum, the Pd mailing lists, or the Facebook PD Group.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Code that Sounds",
    "section": "Contact",
    "text": "Contact\nIf you have any questions, comments, or feedback about this book, please feel free to reach out to me at eabregu.dev@gmail.com. I would love to hear from you!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Code that Sounds",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to express my gratitude to the following individuals and organizations for their support and contributions to this book:\n\nPure Data for providing a powerful and flexible platform for creative coding.\nThe Pd community for their invaluable resources, tutorials, and support.\nThe open-source community for their dedication to sharing knowledge and tools for creative coding.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Code that Sounds",
    "section": "License",
    "text": "License\nThis book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. You are free to share and adapt the material, provided you give appropriate credit, do not use it for commercial purposes, and distribute your contributions under the same license.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Code that Sounds",
    "section": "",
    "text": "Multimedia Arts UNA, Electronic Arts UNTREF, Bachelor of Music and Technology UNQ, Master’s Degree in Sound Art UNQ, Doctorate in Arts UNA↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Pure Data?\nTraditionally, the process of creating software applications has relied heavily on text-based programming languages. Developers would write code manually in text files and execute them later to observe the outcome. While this method is efficient for those trained in programming, it often presents a steep learning curve for artists, musicians, and other creatives without a technical background. The abstract nature of text-based coding can act as a barrier to entry, especially for those more accustomed to working in visual or tactile media.\nPure Data offers a radically different approach by introducing a graphical programming environment tailored to creative exploration. Instead of writing lines of code, users work with visual representations of functions—called objects—that are placed and connected on a canvas. This paradigm allows users to construct interactive programs, known as “patches,” by literally drawing connections between elements. Each object can receive messages that modify its behavior in real time, creating a highly responsive and accessible system for building audio, visual, or multimedia tools.\nThe design of Pure Data echoes the modular synthesis systems of the 20th century, where sound was shaped by routing audio through a network of physical devices linked with patch cables. This historical reference not only grounds Pure Data in a legacy of experimental electronic music but also makes it intuitive for users familiar with analog workflows. By visually “patching” connections, users can simulate and extend these traditional processes in a digital context.\nTo fully engage with the creative potential of Pure Data, one can develop or modify algorithms for digital audio using imaginative coding strategies. For instance, building a real-time audio granulator that reinterprets live microphone input can offer both technical challenge and artistic reward. Through this lens, programming becomes a form of creative expression, blending logical structures with aesthetic decisions. By experimenting with signal flow, modulation techniques, or interactive sensors, users cultivate a mindset that is both inventive and analytical—unlocking new possibilities in digital sound design and performance.\nPure Data (or Pd) is a real-time graphical programming environment for audio, video, and graphical processing. Pure Data is commonly used for live music performance, VeeJaying, sound effects, composition, audio analysis, interfacing with sensors, using cameras, controlling robots or even interacting with websites. Because all of these various media are handled as digital data within the program, many fascinating opportunities for cross-synthesis between them exist. Sound can be used to manipulate video, which could then be streamed over the internet to another computer which might analyze that video and use it to control a motor-driven installation.\nProgramming with Pure Data is a unique interaction that is much closer to the experience of manipulating things in the physical world. The most basic unit of functionality is a box, and the program is formed by connecting these boxes together into diagrams that both represent the flow of data while actually performing the operations mapped out in the diagram. The program itself is always running, there is no separation between writing the program and running the program, and each action takes effect the moment it is completed.\nThe community of users and programmers around Pure Data have created additional functions (called “externals” or “external libraries”) which are used for a wide variety of other purposes, such as video processing, the playback and streaming of MP3s or Quicktime video, the manipulation and display of 3-dimensional objects and the modeling of virtual physical objects. There is a wide range of external libraries available which give Pure Data additional features. Just about any kind of programming is feasible using Pure Data as long as there are externals libraries which provide the most basic units of functionality required.\nThe core of Pure Data written and maintained by Miller S. Puckette and includes the work of many developers, making the whole package very much a community effort.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#why-pure-data",
    "href": "chapters/introduction.html#why-pure-data",
    "title": "1  Introduction",
    "section": "1.2 Why Pure Data?",
    "text": "1.2 Why Pure Data?\nPure Data (Pd) is a powerful, open-source environment for creative coding, offering a uniquely visual approach to programming that is especially suited for artists, musicians, and interactive media designers. Its intuitive interface and modular structure make it a flexible and accessible tool for developing real-time audio and visual projects—from live performances to experimental installations.\nOne of Pure Data’s standout features is its graphical programming interface, which replaces traditional lines of code with visual objects and patch cords. This allows users to construct complex behaviors by connecting elements on screen, making it easier to prototype and refine creative ideas. Real-time processing capabilities mean that audio and visual data can be generated, modified, and responded to instantly—ideal for performances, generative art, or interactive systems that react to sensors or user input.\nBeyond its creative potential, Pure Data offers practical advantages: it is cross-platform, running on Windows, macOS, and Linux, and integrates easily with tools like Arduino, Raspberry Pi, and Max/MSP, enabling hybrid systems that combine digital and physical components. A rich ecosystem of external libraries supports advanced functions like synthesis, visualization, and computer vision. Its open-source nature encourages exploration and collaboration, with a supportive community, extensive documentation, and countless tutorials available. Because it’s free and widely used in education, Pd is not only an effective tool for artistic expression but also a valuable learning resource for developing a strong foundation in programming and multimedia design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#what-is-creative-coding",
    "href": "chapters/introduction.html#what-is-creative-coding",
    "title": "1  Introduction",
    "section": "1.3 What is Creative Coding?",
    "text": "1.3 What is Creative Coding?\nCreative coding is the practice of using programming as a tool for artistic expression. It transforms code from a purely functional medium into a creative one, enabling the development of visual art, music, interactive experiences, and experimental media. This approach encourages artists, designers, and technologists to explore beyond the limits of traditional art forms, embracing code as a flexible and dynamic means of invention and communication.\nThe applications of creative coding are diverse, ranging from generative visuals and algorithmic design to responsive installations and live audiovisual performances. Creators often use languages and environments specifically designed to support creative work, such as Processing, OpenFrameworks, Max/MSP, and Pure Data. These platforms make it easier to manipulate data, control media in real time, and experiment with unconventional interfaces and outputs.\nImportantly, creative coding transcends the boundaries of specific disciplines. It can intersect with visual art, music, dance, theater, architecture, and even narrative writing. What unites these practices is the use of code as an expressive tool—one that invites innovation, play, and conceptual exploration. Closely tied to the values of the open-source movement, creative coding thrives in a culture of sharing, where artists and developers freely exchange code, tutorials, and ideas. This collaborative ecosystem fosters continuous learning and reinvention, empowering creators to expand what is possible through digital technology.\nFor an in-depth exploration of creative coding, consider checking out these resources:\n\nAwesome Creative coding - A curated list of resources, libraries, and tools for creative coding.\nCreative Code Berlin - A collection of creative coding resources, tutorials, and projects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#getting-started",
    "href": "chapters/introduction.html#getting-started",
    "title": "1  Introduction",
    "section": "1.4 Getting Started",
    "text": "1.4 Getting Started\nThis section will guide you through the installation of Pure Data Vanilla, the official and minimal distribution of Pure Data, on Windows, macOS, and Linux systems. The installation process is straightforward and will have you up and running in no time.\nTo get the most out of this book, you should have a basic understanding of programming and some familiarity with the Pure Data language.\n\n1.4.1 Tools, Manuals, and Resources\nIf you are new to Pure Data or programming, there are several free online resources that can help you get started. Pure Data may initially appear intimidating—its blank, minimalistic interface offers no hints or prompts, and users are often left wondering where to begin. What objects are available? How do they function? This curated list of resources is intended to guide newcomers and experienced users alike by offering clear pathways into the Pd ecosystem.\nPure Data FLOSS Manual – A beginner-friendly and comprehensive guide. This manual offers a full introduction, covering everything from installation and configuration to more advanced programming techniques. A recommended first stop for new users.\nOfficial Pure Data Documentation – Authored by Miller Puckette, the creator of Pure Data, this manual is essential for serious exploration. While it can be technical and terse, it serves as the definitive reference, especially when third-party materials are lacking in detail. The documentation can also be accessed from within Pd via Help → HTML Manual.\nTheory and Techniques of Electronic Music – This canonical text by Miller Puckette accompanies Pd’s example patches (found via Help → Browser → Audio Examples). Although it has a steep learning curve, it is invaluable for those wishing to deepen their understanding of digital signal processing in the Pd environment.\nPure Data Tutorials Browser (Help → Browser → Manuals) – Within Pd, under the “Manuals” section of the browser, you’ll find a range of beginner-focused tutorials, including sections on audio, visuals, and networking. These are ideal for building a foundational understanding.\nObject Help Patches – Right-clicking on any object in a patch and selecting “Help” will open a contextual help patch explaining how the object works, often with embedded examples. Right-clicking on the background of a patch will show a list of available objects—an excellent way to explore what’s possible in Pd.\npuredata.org – The official website for Pd. It offers current news, documentation, FAQs, and additional links and downloads. It’s also a great place to find updates and learn about the community.\nYouTube and Vimeo Tutorials – A large number of video tutorials can be found on platforms like YouTube and Vimeo. These visual resources are invaluable for those who prefer step-by-step demonstrations.\nPatchstorage – A community-driven platform for sharing and discovering Pure Data patches. From beginner-level exercises to full multimedia installations, Patchstorage is an inspiring and practical resource for real-world Pd applications.\nPd Workshop Files – Patches and resources by Max Neupert and Frank Barknecht. These materials include motion detection examples (Pd/Gem), some annotated in German.\nAndy Farnell – Designing Sound – The author of Designing Sound (ISBN 0-262-01441-6) provides patches and tutorials focused on procedural audio and sound design in Pd.\nLoadbang – Johannes Kreidler – An online tutorial and book (ISBN 978-3-936000-57-3) offering practical guidance through creative coding projects in Pd.\nKunstuniversität Graz / Pure Data – An extensive collection of tutorials and resources for learning Pure Data, including tutorials and example patches.\nMultimedia Programming with Pure Data (B. Chung) – You will learn how to author various digital media, such as images, animations, audio, and videos. From simple to sophisticated interaction techniques, you will learn to apply these techniques in your practical multimedia projects with Pure Data.\nPure Data: Electronic Music and Sound Design - Theory and Practice - Volume 1 - by Francesco Bianchi, Alessandro Cipriani and Maurizio Giri, the book is an overview of the theory and practice of Pure Data, with a glossary of terms and suggested tests that allow students to evaluate their progress.\nEkran.org Gem Tutorials – Additional tutorials on Gem for those interested in audiovisual composition and performance.\n\n\n\n1.4.2 Giving Back to the Pd Community\nPure Data is not only free and open-source—it is also supported by a global community of contributors. If you’ve found value in Pd, consider giving back:\n\nShare your knowledge and patches with others\nHelp newcomers by answering questions and offering guidance\nContribute abstractions, externals, or even core Pd code if you have programming skills\nPromote the use of Pd in your community or academic setting\n\nSupporting the Pure Data community ensures that this powerful creative tool continues to evolve and remain accessible to artists, educators, and researchers around the world.\n\n\n1.4.3 Recommended Pure Data distributions\nThis book is based on Pure Data Vanilla distribution, which is the most widely used version. You can download it from the PD official website. This version is maintained by the original author, Miller Puckette1, and is the most stable supported version of Pure Data. It includes all the core features and libraries needed for most projects, making it a great starting point for beginners and experienced users alike. Pure Data Vanilla is a free, open-source and available for Windows, macOS, and Linux operating systems.\nIn addition to Pure Data Vanilla, there are several other distributions and versions of Pure Data that you may find useful:\n\nPurr Data – A fork of Pd-l2ork that focuses on usability and accessibility, with a more polished interface and additional features. Purr Data serves the same purpose, but offers a new and much improved graphical user interface and includes many 3rd party plug-ins. Like Pd, it runs on Linux, macOS and Windows, and is open-source throughout.\nPlugdata – plugdata is a plugin wrapper designed for Miller Puckette’s Pure Data (Pd), featuring an enhanced graphical user interface (GUI) created using JUCE, headed by Timothy Schoen. this project is still a work in progress, and may still have some bugs. By default, plugdata comes with the cyclone and ELSE collections of externals and abstractions.\nPD Ceammc - a general purpose Pd distribution and library that is used for performance, sound-design and education purposes in Centre of electroacoustic music of Moscow Conservatory (CEAM).\n\n\n\n1.4.4 Recommended libraries and externals\nThe following libraries are recommended to be used with Pure Data. They are not included in the Pure Data Vanilla distribution, but they can be easily installed and used with it:\n\n\n\n\n\n\n\nLibrary\nDescription & Usefulness\n\n\n\n\ncyclone\nA large collection of Max/MSP-compatible objects. Useful for porting Max patches and advanced DSP tasks.\n\n\nELSE\nExtensive library for audio, control, math, and UI\n\n\niemlib\nCollection of general purpose objects and filters for Pure Data\n\n\nlist-abs\nTools for advanced list manipulation and data processing\n\n\nzexy\nEssential utilities for math, signal, and control operations. Adds many missing core features.\n\n\nGem\nGraphics Environment for Multimedia. Enables real-time visuals and video.\n\n\nceammc\nlibrary used for work and education purposes in Centre of electroacoustic music of Moscow Conservatory (CEAMMC) and ZIL-electro studio\n\n\ncomport\nSerial port communication. Useful for connecting Pd to Arduino, sensors, and hardware\n\n\nmrpeach\nNetworking and OSC support. Essential for interactive and networked Pd projects\n\n\nfreeverb~\nHigh-quality reverb effect. Simple way to add spatialization to audio patches\n\n\njmmmp\nCollection of GUI and utility objects. Enhances user interface design in Pd patches\n\n\nmapping\nTools for mapping data (e.g., sensors to sound). Useful for interactive installations and performances\n\n\n\nThese libraries provide additional objects and features that can enhance your projects and make it easier to work with sound synthesis, data visualization, and interactive installations.\n\n\n1.4.5 Installing Pure Data Vanilla\nThe following is a step-by-step installation of Pure Data Vanilla, the official and minimal distribution of Pure Data, on Windows, macOS, and Linux systems.\n\n\n1.4.6 Installing on Windows\n\n1.4.6.1 Download the Installer\n\nVisit the official Pure Data website: https://puredata.info/downloads/pure-data\nScroll down to the Windows section.\nClick the latest version link (e.g., pd-0.55-2.windows-installer.exe).\n\n\n\n1.4.6.2 Run the Installer\n\nLocate the downloaded .exe file in your Downloads folder.\nDouble-click to run the installer.\nFollow the installation wizard:\n\nChoose the installation location (default is fine).\nAllow the program to create a Start Menu shortcut.\n\n\n\n\n1.4.6.3 Verify Installation\n\nAfter installation, open Pure Data from the Start Menu or desktop shortcut.\nThe main Pd window should appear with a blank patch ready.\n\n\n\n\n\n1.4.7 Installing on macOS\n\n1.4.7.1 Download the Disk Image\n\nVisit: https://msp.ucsd.edu/software.html\nScroll to the macOS section.\nDownload the .dmg file (e.g., Pd-0.54-1.dmg).\n\n\n\n1.4.7.2 Install the Application\n\nOpen the downloaded .dmg file.\nDrag the Pure Data icon into the Applications folder.\n\n\n\n1.4.7.3 Open Pure Data\n\nOpen your Applications folder.\nRight-click (or Ctrl + click) the Pure Data icon and select Open.\n\nThe first time, macOS may warn that the app is from an unidentified developer.\nConfirm to proceed.\n\n\n\n\n1.4.7.4 Optional: Enable Audio Permissions\n\nIf prompted, allow Pure Data to access the microphone.\nOpen System Preferences &gt; Security & Privacy &gt; Microphone and ensure Pure Data is enabled.\n\n\n\n\n\n1.4.8 Installing on Linux\nPure Data is available in the package repositories of most major Linux distributions. Below are instructions for popular systems.\n\n1.4.8.1 Debian/Ubuntu-based Systems\nsudo apt update\nsudo apt install puredata\n\n\n1.4.8.2 Fedora\nsudo dnf install puredata\n\n\n1.4.8.3 Arch Linux\nsudo pacman -S puredata\n\n\n1.4.8.4 Verify Installation\n\nOpen a terminal and type pd.\nThe Pure Data GUI should launch, displaying a blank patch.\nIf you encounter issues, check your package manager or consult the Pure Data community for troubleshooting.\n\n\n\n\n1.4.9 Installing Externals\nPure Data’s functionality can be extended through the use of externals—additional libraries that provide new objects and features. These externals are created by the Pd community and can add everything from new audio effects to advanced data processing tools. Installing externals is straightforward thanks to Pure Data’s built-in package manager.\n\n1.4.9.1 Step 1: Open Pure Data\n\nLaunch Pure Data (Pd) on your computer.\nMake sure you are using the Vanilla version for best compatibility with the package manager.\n\n\n\n1.4.9.2 Step 2: Access the “Find Externals” Tool\n\nIn the Pure Data menu, go to Help → Find Externals…\nThis opens the Deken package manager, which allows you to search for and install externals directly from within Pd.\n\n\n\n1.4.9.3 Step 3: Search for an External\n\nIn the search bar, type the name of the external or library you want to install (for example, zexy, cyclone, or iemlib).\nPress Enter or click the Search button.\nA list of matching externals will appear, showing available versions for your platform.\n\n\n\n1.4.9.4 Step 4: Install the External\n\nClick on the desired external in the list.\nChoose the version that matches your operating system and Pd version.\nClick Install.\nThe external will be downloaded and placed in your Pd externals folder (typically ~/Documents/Pd/externals on macOS and Linux, or C:\\Users\\&lt;YourName&gt;\\Documents\\Pd\\externals on Windows).\n\n\n\n1.4.9.5 Step 5: Add the External to Your Pd Path (if needed)\n\nMost externals are automatically available after installation.\nIf Pd cannot find the external, you may need to add its folder to Pd’s search path:\n\nGo to Preferences → Path…\nClick New and add the path to the external’s folder (for example, ~/Documents/Pd/externals/cyclone).\nClick OK and restart Pd.\n\n\n\n\n1.4.9.6 Step 6: Use the External in Your Patch\n\nIn your patch, create an object with the name of the external’s library, followed by the object you want to use.\nFor example, to use the counter object from the cyclone library:\n[cyclone/counter]\nSome libraries require you to load them with a special object, such as [declare -lib cyclone] at the top of your patch.\n\n\n\n1.4.9.7 Step 7: Verify Installation\n\nIf the object appears without errors (no red box), the external is installed correctly.\nIf you see errors, double-check the installation path and that you are using the correct object/library name.\n\n\n\n1.4.9.8 Manual Installation (Advanced)\nIf you need to install an external manually:\n\nDownload the external from https://deken.puredata.info/ or the developer’s website.\nExtract the files to your Pd externals folder.\nAdd the folder to Pd’s path as described above.\n\n\nTip: Always check the documentation for each external, as installation steps or requirements may vary.\n\nFor more details, see the official Pure Data documentation on externals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Miller Puckette is a computer music researcher and the creator of Pure Data. He has been involved in the development of various software tools for music and multimedia, including Max/MSP and Pure Data. His work has had a significant impact on the field of computer music and interactive media. More information about Miller Puckette can be found on his website.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html",
    "href": "chapters/playrec.html",
    "title": "2  Rec & Play",
    "section": "",
    "text": "2.1 I’m Sitting in a Room – Alvin Lucier\nIn this chapter, we will explore the relationship between recording and playback. We will look at how these two processes can be used to create new sounds and compositions. We will also discuss the technical aspects of recording and playback, including the equipment and techniques used in the process. We will also consider the artistic implications of recording and playback, including how these processes can be used to create new forms of expression and communication.\nI’m Sitting in a Room (Alvin Lucier) consists of a 15-minute and 23-second sound recording. The piece opens with Lucier’s voice as he declares he is sitting in a room different from ours. His voice trembles slightly as he delivers the text, describing what will unfold over the next 15 minutes:\nAs listeners, we know what is going to happen, but we don’t know how it will happen (Hasse 2012). We listen, following Lucier’s recorded voice. Then, Lucier plays the recording into the room and re-records it. This time, we begin to hear more of the room’s acoustic characteristics. He continues to play back and re-record his voice—over and over—until his speech becomes softened, almost dissolved, into the sonic reflections of the room in which the piece was recorded and re-recorded.\nOne effective way to study a piece is to replicate its technical aspects and the devices involved. The aim of this activity is to recreate the technical setup of I’m Sitting in a Room, focusing on the processes of recording, playback, and automation.\nThere are many ways to implement this technical system. Just to mention two environments we’re currently working with: it’s relatively straightforward in Pure Data.\nIn terms of hardware, besides a computer, you’ll need a speaker (mono audio output), a microphone (mono audio input), and a semi-reverberant room.\nThe system should include at least two manual (non-automated) controls:\nChoose a room whose acoustic or musical qualities you’d like to evoke. Connect the microphone to the input of tape recorder #1. From the output of tape recorder #2, connect to an amplifier and speaker. Use the following text, or any other text of any length:\nThe following steps outline the process:\nAll the recorded generations, presented in chronological order, create a tape composition whose duration is determined by the length of the original statement and the number of generations produced. Make versions in which a single statement is recycled through different rooms. Create versions using one or more speakers in different languages and spaces. Try versions where, for each generation, the microphone is moved to different parts of the room(s). You may also develop versions that can be performed in real time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#im-sitting-in-a-room-alvin-lucier",
    "href": "chapters/playrec.html#im-sitting-in-a-room-alvin-lucier",
    "title": "2  Rec & Play",
    "section": "",
    "text": "“…I am recording the sound of my speaking voice and I am going to play it back into the room again and again…”\n\n\n\n\nI am sitting in a room design\n\n\n\n\n\n\n\n\nStart recording\nStop recording\n\n\n\nI am sitting in a room different from the one you are in now. I am recording the sound of my speaking voice, and I am going to play it back into the room again and again until the resonant frequencies of the room reinforce themselves so that any semblance of my speech, with perhaps the exception of rhythm, is destroyed. What you will hear, then, are the natural resonant frequencies of the room articulated by speech. I regard this activity not so much as a demonstration of a physical fact, but more as a way to smooth out any irregularities my speech might have.\n\n\n\nRecord your voice through the microphone into tape recorder #1.\nRewind the tape, transfer it to tape recorder #2, and play it back into the room through the speaker. - Record a second generation of the statement via the microphone back into tape recorder #1.\nRewind this second generation to the beginning and splice it to the end of the original statement in tape recorder #2.\nPlayback only the second generation into the room and record a third generation into tape recorder #1.\nContinue this process through multiple generations.\n\n\n\n2.1.1 Pure Data implementation of I’m sitting in a room\n \n\n\n\nPure Data implementation of I am sitting in a room\n\n\nIn this section, we will break down the Pure Data patch I-am-sitting-in-a-room.pd step by step. This patch is inspired by Alvin Lucier’s iconic piece and demonstrates how to recursively record and play back sound to reveal the resonant frequencies of a room.\n\n2.1.1.1 Conceptual Overview\nThe patch enables you to:\n\nRecord your voice or any sound in a room.\nPlay back the recording into the room and re-record it.\nRepeat this process, so the room’s resonances become more pronounced with each generation.\n\n\n\n\n2.1.2 System Diagram\nThe following diagram illustrates the recursive nature of the recording and playback process, similar to how Lucier’s piece unfolds. Each generation of tape recorders captures the previous one, creating a feedback loop that emphasizes the room’s resonances.\n\n\n\n\n\nflowchart TB\n  %% Top level source\n  Input[Microphone]\n\n  subgraph PLAYBACK RECORDERS\n    direction LR\n    subgraph RecorderA [DEVICE A]\n      RecordA[Record A.wav]\n      PlayA[Play A.wav]\n      RecordA -.-&gt; PlayA\n    end\n    \n    subgraph RecorderB [DEVICE B]\n      RecordB[Record B.wav]\n      PlayB[Play B.wav]\n      RecordB -.-&gt;PlayB\n    end\n  end\n  \n  %% Output at the bottom  \n  Output((Speaker))\n  \n  %% Clear connections showing signal flow\n  start([1-start]) --&gt; RecordA\n  stop([2-stop]) --&gt; RecordA\n  stop([2-stop]) --&gt; RecordB\n  stop([2-stop]) --&gt; PlayA\n\n  Input ==&gt; RecordA\n  Input ==&gt; RecordB\n\n  PlayA ==&gt; Output\n  PlayB ==&gt; Output\n  \n  %% Feedback path showing recursion\n  PlayB -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordA\n  PlayA -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordB\n  Output ==&gt; room[\"ROOM'S&lt;br&gt;REFLECTIONS\"]:::roomStyle ==&gt; Input\n\n  classDef roomStyle fill:#f5f5f5,stroke:#333,stroke-dasharray:5 5 \n\n\n\n\n\n\n \nThe following sequence diagram illustrates the process of recording and playback in the patch. It shows how the audio input is captured, recorded, played back, and re-recorded in a loop, emphasizing the room’s resonances.\n\n\n\n\n\n\nsequenceDiagram\n    participant Start as Start\n    participant Stop as Stop\n    participant Input as Mic\n    participant RecA as Rec A.wav\n    participant PlayA as Play A.wav\n    participant RecB as Rec B.wav\n    participant PlayB as Play B.wav\n    participant Output as Speaker\n    participant Room as Room\n\n    %% Generation 1\n    activate Input\n    Start--&gt;&gt;RecA: Start button\n    activate RecA\n    Input-&gt;&gt;RecA: Mic input\n\n\n    Stop--xRecA: Stop Rec A  \n    deactivate RecA   \n    Stop--&gt;&gt;RecB: Start Rec B\n    activate RecB \n    Input-&gt;&gt;RecB: Mic input          \n    Stop--&gt;&gt;PlayA: Start Play A\n    activate PlayA\n\n    PlayA-&gt;&gt;Output: Playback A.wav\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n    PlayA--xRecB: Trigger Stop Rec B\n    deactivate RecB\n    deactivate PlayA\n\n    PlayA--&gt;&gt;RecA: Trigger Start Rec B\n    activate RecA\n    Input-&gt;&gt;RecA: Mic input \n    PlayA--&gt;&gt;PlayB: Trigger Start Play A\n    activate PlayB\n\n    %% Generation 2\n    PlayB-&gt;&gt;Output: Playback B.wav\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n\n    %% Generation 3 (process repeats)\n    PlayB--xRecA: Trigger Stop Rec A\n    deactivate RecA\n    deactivate PlayB\n    PlayB--&gt;&gt;RecB: Trigger Start Rec B\n    activate RecB \n    Input-&gt;&gt;RecB: Mic input\n    PlayB--&gt;&gt;PlayA: Trigger Start Play A\n    activate PlayA\n    PlayA-&gt;&gt;Output: Playback\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n\n    Note over Input,Room: The process continues, alternating between devices and reinforcing the room's resonances.\n    deactivate Input\n    deactivate PlayA\n    deactivate RecB\n\n\n\n\n\n\n\n\n\n2.1.3 Patch Overview\nThe Pure Data implementation of I’m Sitting in a Room creates a digital recreation of Alvin Lucier’s seminal work through a carefully orchestrated system of recording, playback, and re-recording processes. This patch demonstrates how computational tools can faithfully reproduce the acoustic phenomena that drive Lucier’s piece, where the gradual accumulation of room resonances transforms speech into pure acoustic space. The implementation leverages Pure Data’s audio file handling capabilities, signal routing architecture, and user interface elements to create an interactive environment that allows users to experience the recursive transformation process in real-time.\nThe system architecture revolves around a dual-recording mechanism that alternates between two separate audio files, mimicking the original tape recorder setup used in Lucier’s piece. This alternating approach ensures that each generation of the recording process builds upon the previous one, creating the cumulative acoustic effect that defines the work. The patch employs a signal routing system that uses Pure Data’s send and receive objects to distribute audio signals to multiple destinations simultaneously, enabling the concurrent processes of playback monitoring and re-recording that are essential to the piece’s realization.\nThe interface design prioritizes clarity and ease of use, providing distinct control buttons for each phase of the recording process. The modular approach to signal routing ensures that the same microphone input can serve multiple functions within the system, while the throw and catch objects create a centralized mixing point that consolidates all audio streams for output. This design philosophy reflects the conceptual elegance of Lucier’s original work, where simple processes yield complex and unpredictable results through iteration and accumulation.\nThe patch incorporates real-time monitoring capabilities that allow users to hear the progressive transformation of their audio material as it passes through successive recording generations. This immediate feedback loop is crucial for understanding the acoustic phenomena at work and provides the experiential component that makes Lucier’s piece so compelling as both an artistic statement and a demonstration of physical principles. The system maintains the temporal structure and rhythmic elements of the original speech while gradually dissolving its semantic content, creating a sonic archaeology of the recording space itself.\n\n\n2.1.4 Key Objects Table\n\n\n\nObject\nPurpose\n\n\n\n\nadc~\nAudio input from microphone\n\n\nwritesf~\nRecord audio to a file\n\n\nreadsf~\nPlay audio from a file\n\n\nthrow~/catch~\nMix and route audio signals\n\n\ndac~\nAudio output to speakers\n\n\nbng\nButton for triggering actions\n\n\nmsg\nSend commands to objects (open, start, stop)\n\n\n\n\n\n2.1.5 How to Use the Patch\n\nStart Recording:\nClick the “Start Recording” button and speak or make a sound.\nStop Recording:\nClick the “Stop Recording” button to finish.\nPlayback and Re-record:\nUse the playback button to play your recording into the room and simultaneously re-record it.\nRepeat:\nRepeat the playback and re-recording process as many times as you like to hear the room’s resonances emerge.\n\nThis patch is a practical and creative way to explore acoustic phenomena and the transformation of sound through recursive recording, echoing the spirit of Lucier’s original work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#i-am-sitting-in-a-freeverb-patch-walkthrough",
    "href": "chapters/playrec.html#i-am-sitting-in-a-freeverb-patch-walkthrough",
    "title": "2  Rec & Play",
    "section": "2.2 I am sitting in a [freeverb~] — Patch Walkthrough",
    "text": "2.2 I am sitting in a [freeverb~] — Patch Walkthrough\nThe patch I-am-sitting-in-a-room-freeverb.pd extends the original I am sitting in a room concept by introducing a virtual acoustic environment using reverb and allowing for pre-recorded audio input instead of just live microphone input. This adaptation provides more creative flexibility and consistent results compared to using a physical room.\n\n\n\nPure Data implementation of I am sitting in a room with freeverb\n\n\nThis patch is designed to simulate the recursive recording process of Lucier’s piece while allowing for more control over the sound environment. It uses the freeverb~ object to create a virtual room effect, enabling you to manipulate the acoustic characteristics of the sound without relying on a physical space.\n\n2.2.1 Conceptual Overview\nThe patch enables you to: - Use a pre-recorded audio file OR live input as your source material - Add artificial reverb to simulate room characteristics - Follow the same recursive recording process as the original - Achieve more controlled, repeatable results\n\n\n2.2.2 System Diagram\n\n\n\n\n\nflowchart TB\n  %% Top level source\n  AudioFile[Audio File]\n  \n  %% Horizontal arrangement of recorders A and B\n  subgraph PLAYBACK RECORDERS\n    direction LR\n    subgraph RecorderA [DEVICE A]\n      RecordA[writesf~ A.wav]\n      PlayA[readsf~ A.wav]\n      RecordA ==&gt; PlayA\n    end\n    \n    subgraph RecorderB [DEVICE B]\n      RecordB[writesf~ B.wav]\n      PlayB[readsf~ B.wav]\n      RecordB ==&gt; PlayB\n    end\n  end\n  \n  subgraph VIRTUAL ROOM\n  %% Reverb block in the middle\n    VirtualRoom(((freeverb~&lt;br&gt;Virtual Room)))\n  end  \n  \n  %% Output at the bottom  \n  Output[dac~]\n  \n  %% Clear connections showing signal flow\n  AudioFile --&gt; RecordA\n  AudioFile --&gt; Output\n  \n  PlayA ==&gt; VirtualRoom\n  PlayB ==&gt; VirtualRoom\n  \n  VirtualRoom ==&gt; RecordB\n  VirtualRoom ==&gt; RecordA\n  VirtualRoom --&gt; Output\n  \n  %% Feedback path showing recursion\n  PlayB -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordA\n  PlayA -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordB\n\n\n\n\n\n\nThis diagram illustrates the recursive nature of the recording and playback process, similar to how Lucier’s piece unfolds. Each generation of tape recorders captures the previous one, creating a feedback loop that emphasizes the room’s resonances.\n\n\n2.2.3 Unique Features of the [freeverb~] Version\n\n2.2.3.1 1. Audio File Input\nUnlike the original patch that only uses microphone input, this version can:\n\nPlay a pre-recorded audio file (speech.wav) as the initial source\nProcess this file through reverb before recording\nTrigger playback with a button (labeled “2. Playback audio”)\n\n\n\n\n\n\nflowchart LR\n    bng([bng]) --&gt; msg[\"msg open speech.wav, 1\"]\n    msg --&gt; readsf[\"readsf~\"]\n    readsf --&gt; freeverb[\"freeverb~\"]\n    freeverb --&gt; send[\"s~ audio.input\"]\n\n\n\n\n\n\n\n\n2.2.3.2 2. Virtual Acoustic Environment\nInstead of relying on a physical room’s acoustics, this patch uses freeverb~ objects to create a simulated acoustic space:\n\nEvery audio signal (input and playback) passes through a freeverb~ object\nThis creates consistent reverberation that can be adjusted and controlled\nMultiple freeverb~ objects process different stages of the audio for cumulative effects\n\n\n\n2.2.3.3 3. Enhanced Control Flow\nThe patch includes numbered controls for better workflow:\n\n\n\n\n\n\n\n\nButton\nLabel\nFunction\n\n\n\n\n1\nStart recording\nBegins recording the input source to record-A.wav\n\n\n2\nPlayback audio\nPlays the speech.wav file through reverb into the system\n\n\n3\nStop recording\nStops the current recording process\n\n\n\n\n\n2.2.3.4 4. Multiple Send/Receive Paths\nThe patch uses additional send/receive pairs to manage signal routing:\n\ns~/r~ audio.input - Routes input signals to the recording object\ns~/r~ player.A - Routes playback from file A to recorder B\ns~/r~ player.B - Routes playback from file B back to recorder A\nthrow~/catch~ audio - Collects all signals to be sent to the output\n\n\n\n\n2.2.4 Step-by-Step Explanation\nThe operation of this patch begins with the establishment of initial audio source options, where users can choose between pre-recorded file input or live input sources. For pre-recorded file input, clicking the “2. Playback audio” button triggers the playback of speech.wav through a readsf~ object, which then processes the audio through freeverb~ before sending it to the s~ audio.input routing system. This approach provides consistent source material that can be used repeatedly for experimentation and comparison across different processing generations. While live input capability is not explicitly shown in this version, the architecture could easily accommodate an adc~ object that would capture real-time microphone input and route it through the same s~ audio.input pathway.\nThe recording of the first generation commences when the user clicks the “1. Start recording” button, which initiates the capture process to record-A.wav. The signal from s~ audio.input is captured by the corresponding r~ audio.input object and sent directly to a writesf~ object that handles the actual file writing operations. This resulting file contains the source material with initial reverb processing applied, establishing the foundation for subsequent generations. The recording process maintains the temporal structure and amplitude characteristics of the original material while introducing the first layer of artificial room acoustics that will be progressively enhanced through the recursive process.\nThe playback and re-recording phase begins automatically after stopping the initial recording with the “3. Stop recording” button. The patch executes a carefully orchestrated sequence where it opens and plays record-A.wav through another readsf~ object, processes this playback through an additional freeverb~ object for cumulative reverb effects, and then routes the processed signal along two parallel paths. The first path sends the audio to throw~ audio for real-time monitoring, allowing users to hear the progressive transformation as it occurs. The second path directs the same signal to s~ player.A, where it is received and simultaneously recorded to record-B.wav, creating the next generation of the recursive process.\nThe recursive process continues as each newly created recording becomes the source for the subsequent generation. When record-B.wav is created, it can be played back through its own readsf~ object, processed through yet another freeverb~ instance, and routed to both the monitoring system via throw~ audio and to s~ player.B for capture back to record-A.wav. This alternating cycle can continue indefinitely, with each generation accumulating more of the virtual room’s characteristics as defined by the freeverb~ parameters. The cumulative effect gradually transforms the original source material into a sonic representation of the artificial acoustic space, with the original speech or audio content becoming increasingly obscured while the rhythmic and temporal elements remain as the structural foundation.\nThe output stage serves as the central collection and distribution point for all audio signals in the system. All audio signals that have been sent to throw~ audio are collected by the corresponding catch~ audio object, which functions as a mixing bus that combines signals from whichever recording generation is currently active. This consolidated audio signal is then sent to an out~ object that handles the final output routing to speakers or headphones, ensuring that users can monitor the entire process in real-time and experience the gradual transformation as it unfolds through successive generations.\n\n\n2.2.5 Key Objects Table\n\n\n\n\n\n\n\nObject\nPurpose in This Patch\n\n\n\n\nfreeverb~\nCreates virtual room acoustics by adding reverberation\n\n\nreadsf~\nPlays audio files (source material or previous generations)\n\n\nwritesf~\nRecords audio to file (for each generation)\n\n\ns~/r~\nRoutes audio between different parts of the patch\n\n\nthrow~/catch~\nMixes and outputs all audio signals\n\n\ndel\nAdds small delays to ensure proper sequencing of operations\n\n\nbng\nProvides buttons for user interaction\n\n\nout~\nSends audio to speakers/headphones\n\n\n\n\n\n2.2.6 Creative Applications\n\nExperiment with reverb settings: Try different room sizes, damping, and wet/dry mix\nUse different source materials: Test various speech samples, musical phrases, or sound effects\nCreate hybrid processes: Record the first generation in a real room, then switch to the virtual environment\nBuild compositional sequences: Layer different generations to create evolving textures\nCompare real vs. virtual: Process the same source through both the original and freeverb patches to contrast physical and virtual acoustics\n\nThis virtual approach offers a controlled laboratory for exploring the core concepts of Lucier’s piece, making it accessible even without an ideal acoustic space, while also opening new creative possibilities that wouldn’t be possible in a physical implementation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#simple-audio-player",
    "href": "chapters/playrec.html#simple-audio-player",
    "title": "2  Rec & Play",
    "section": "2.3 Simple Audio Player",
    "text": "2.3 Simple Audio Player\nThe simple audio player represents a streamlined approach to audio file playback that emphasizes ease of use while providing essential control over playback parameters. This Pure Data implementation demonstrates how to create an intuitive audio player interface that combines file loading, visual feedback, and precise playback control in a single cohesive system. The patch showcases fundamental concepts including automatic file loading, visual waveform display, start/stop position control, playback speed manipulation, and smooth parameter interpolation.\n\n\n\nFig. Pure Data implementation of simple audio player\n\n\n\n2.3.1 Patch Overview\nThe simple audio player patch implements a user-friendly playback system that automatically loads an audio file into a visual array and provides comprehensive control over playback parameters through an intuitive slider-based interface. The system operates by calculating playback positions based on normalized start and stop points, which are then converted to sample-accurate indices for precise audio retrieval. The architecture emphasizes visual feedback through multiple display elements that show both the raw sample data and the calculated playback parameters in real-time.\nThe design philosophy centers around immediate usability, where a single “play!” button triggers playback of a predefined segment with user-controlled speed and timing parameters. The visual interface provides dual representation of the audio content through both a large waveform display that shows the entire audio file and smaller numeric displays that provide precise feedback on calculated values. The playback engine employs interpolated table reading to ensure high-quality audio reproduction across all speed settings, while the parameter calculation system converts user-friendly interface values into the sample-accurate indices required for proper audio playback.\nThe patch incorporates automatic initialization through a loadbang mechanism that ensures the audio file is loaded immediately when the patch opens, eliminating the need for manual file loading procedures. The control system provides independent manipulation of start position, stop position, playback speed, and interpolation timing, with all parameters working together to create a flexible and responsive playback environment suitable for both educational exploration and practical audio playback applications.\n\n\n\n\n\nflowchart TD\n    A[Loadbang] --&gt; B[File Loading]\n    B --&gt; C[Soundfiler]\n    C --&gt; D[Array Storage]\n    D --&gt; E[Sample Count]\n    F[Start Slider] --&gt; G[Position Calculation]\n    H[Stop Slider] --&gt; G\n    I[Speed Slider] --&gt; J[Duration Calculation]\n    E --&gt; K[Sample Index Conversion]\n    G --&gt; K\n    J --&gt; L[Interpolation Parameters]\n    K --&gt; L\n    M[Play Button] --&gt; N[Line~ Object]\n    L --&gt; N\n    N --&gt; O[Tabread4~]\n    D --&gt; O\n    O --&gt; P[Audio Output]\n\n\n\n\n\n\n\n\n2.3.2 Data Flow\nThe audio player system begins its operation with an automatic initialization sequence triggered by the loadbang object at patch startup. This initialization sends a message to the soundfiler object instructing it to load the file “speech.wav” into the “audio” array with automatic resizing enabled. The soundfiler responds by reading the entire audio file into the array and returning the total number of samples, which becomes the fundamental reference value for all subsequent position and timing calculations. This sample count is immediately displayed in a numeric box and distributed throughout the system to serve as the basis for converting normalized slider positions into actual sample indices.\nThe position control system operates through two horizontal sliders that represent the start and stop positions as normalized values between 0 and 1. These normalized positions are processed through expression objects that multiply them by the total sample count, converting the user-friendly slider positions into precise sample indices that correspond to actual locations within the audio file. The start position calculation uses the expression int($f1*$f2) where the first input is the normalized slider value and the second is the total sample count, ensuring that the resulting index points to an exact sample location within the array.\nThe timing and speed control mechanism calculates the duration and interpolation parameters necessary for smooth playback transitions. The duration calculation subtracts the start sample index from the stop sample index using the expression $f2 - $f1, providing the total number of samples that will be played during the current playback segment. This duration value is then converted from samples to milliseconds using the expression abs(int(($f1 / 44100) * 1000)), which divides by the standard sample rate and multiplies by 1000 to produce a time value suitable for the interpolation system.\nThe speed control system modifies the calculated duration by multiplying it with the speed slider value through the expression int($f1*$f2). This calculation allows users to control playback speed, where values less than 1 produce slower playback and values greater than 1 produce faster playback. The resulting modified duration becomes the interpolation time parameter that determines how quickly the playback position moves from the start index to the stop index.\nWhen the “play!” button is triggered, all calculated parameters are combined into a message that drives the line~ object. The pack f f f object consolidates the start position, stop position, and interpolation time into a single list, which is then formatted by a message object into the proper syntax for the line~ object. This formatted message instructs the line~ to smoothly interpolate from the start sample index to the stop sample index over the calculated time duration, creating a continuously changing index value that scans through the selected portion of the audio file.\nThe final audio generation stage employs the tabread4~ object to retrieve audio samples from the array using the continuously changing index values provided by the line~ object. The fourth-order interpolation ensures smooth audio reproduction even when the playback speed produces non-integer index values or when rapid parameter changes occur. The interpolated audio output is then routed to the output~ object for final amplification and speaker routing, completing the signal path from stored audio data to audible output.\n\n\n2.3.3 Key Objects and Their Roles\nThe following table summarizes the essential objects used in the simple audio player implementation:\n\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nloadbang\nAutomatically initiates file loading when patch opens\n\n\nsoundfiler\nLoads audio files into arrays and reports sample count\n\n\narray audio\nStores audio waveform data with visual display\n\n\ntabread4~\nPerforms interpolated audio reading from the array\n\n\nline~\nGenerates smooth ramp signals for position scanning\n\n\npack f f f\nCombines start, stop, and time parameters\n\n\nexpr\nPerforms mathematical calculations for position and timing\n\n\nhsl\nHorizontal sliders for start, stop, and speed control\n\n\nnbx\nNumeric boxes for parameter display and precise input\n\n\nbng\nPlay button for triggering playback\n\n\nt f f\nTrigger objects for distributing values to multiple destinations\n\n\noutput~\nProvides stereo audio output with level control\n\n\nf\nFloat storage for maintaining current sample count\n\n\n\n\nThe patch demonstrates several key Pure Data programming concepts, including automatic patch initialization through loadbang, visual array management for waveform display, mathematical expression evaluation for parameter conversion, and smooth audio-rate interpolation for high-quality playback. The modular design separates user interface elements from audio processing components, making the system easy to understand and modify.\nThis implementation serves as an excellent foundation for understanding basic audio playback principles and can be easily extended with additional features such as looping, multiple file support, or external control integration. The visual feedback provided by the waveform display and numeric readouts makes it particularly suitable for educational applications where understanding the relationship between interface controls and audio parameters is important.\n\n\n2.3.4 Creative Applications\n\nExtreme time stretching: Push the speed control to very low values (0.1 or lower) to create drone-like textures from short audio snippets\nRhythmic patterns: Use rapid speed changes and phase jumps to generate percussive sequences from sustained tones\nReverse archaeology: Load full songs and use reverse playback to discover hidden melodic content and create backwards reveals\nGranular-style effects: Combine fast phase position changes with short loop lengths to simulate granular synthesis behaviors\nLive performance tools: Map MIDI controllers to speed and phase parameters for real-time manipulation during performances\nSound design layers: Layer multiple instances with different source materials and speed relationships to create complex evolving textures\nPitch-shifting alternative: Use speed control as a creative pitch-shifting tool, embracing the tempo changes as part of the aesthetic\nMicro-sampling: Load very short sounds (vocal consonants, instrument attacks) and stretch them to reveal internal acoustic details\nCross-fading narratives: Use position control to jump between different parts of spoken word recordings, creating non-linear storytelling\nRhythmic displacement: Sync multiple samplers to the same clock but with different phase offsets to create polyrhythmic patterns",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#audio-sampler-with-phasor",
    "href": "chapters/playrec.html#audio-sampler-with-phasor",
    "title": "2  Rec & Play",
    "section": "2.4 Audio Sampler with [phasor~]",
    "text": "2.4 Audio Sampler with [phasor~]\nThe audio sampler represents a fundamental tool in digital audio processing and musical performance, enabling the capture, manipulation, and playback of recorded audio material. This Pure Data implementation demonstrates how to create a comprehensive sampling system that provides real-time control over playback speed, direction, position, and interpolation. The patch showcases essential concepts including array-based audio storage, variable-speed playback, phase control, and temporal interpolation for smooth parameter transitions.\n\n\n\nFig.\n\n\n\n2.4.1 Patch Overview\nThe audio sampler patch implements a sophisticated playback engine that reads audio data from a table array and provides extensive real-time control over the playback parameters. The system operates by using a phasor oscillator to generate index values that scan through the stored audio samples, with multiplication factors applied to control playback speed and direction. The architecture supports both forward and reverse playback, variable speed control ranging from extreme slow motion to high-speed acceleration, and precise positioning within the audio material.\nThe design follows a modular approach where audio loading, parameter control, and playback generation are handled by distinct subsystems that communicate through Pure Data’s send and receive mechanism. The audio storage system utilizes a resizable array that automatically adjusts to accommodate different audio file lengths, while the playback control provides independent manipulation of speed, direction, interpolation time, and starting position. The output stage employs interpolated table reading to ensure smooth audio reproduction even during rapid parameter changes or extreme speed variations.\nThe interface provides immediate visual feedback through horizontal sliders for speed control, interpolation time adjustment, and phase positioning, along with numeric displays that show the current playback parameters and array position. This design enables both precise manual control and potential automation through external control sources, making it suitable for live performance, sound design, and experimental audio manipulation applications.\n\n\n\n\n\nflowchart TD\n    A[Audio File Loading] --&gt; B[Soundfiler Object]\n    B --&gt; C[Array Resize]\n    C --&gt; D[Sample Count Storage]\n    E[Speed Control] --&gt; F[Direction Multiplier]\n    F --&gt; G[Interpolation System]\n    G --&gt; H[Phasor Generation]\n    I[Phase Position] --&gt; H\n    J[Sample Rate] --&gt; K[Rate Calculation]\n    D --&gt; K\n    K --&gt; L[Base Frequency]\n    L --&gt; H\n    H --&gt; M[Table Reading]\n    M --&gt; N[Audio Output]\n\n\n\n\n\n\n\n\n2.4.2 Data Flow\nThe sampling system begins its operation with the audio loading process, initiated by a message that triggers the soundfiler object to read an audio file into the designated array. The soundfiler automatically resizes the array to match the audio file length and returns the total number of samples, which is stored and distributed to other components through the send/receive system. This sample count becomes crucial for calculating appropriate playback rates and ensuring the phasor oscillator operates within the correct frequency range.\nThe speed and direction control mechanism processes user input from the horizontal slider, which provides values ranging from -5 to +5, representing both playback speed and direction. Negative values produce reverse playback, while positive values generate forward playback, with the magnitude determining the speed multiplier. This control value is processed through an interpolation system that uses the line object to create smooth transitions between different speed settings, preventing audible artifacts that might occur from abrupt parameter changes.\nThe core playback engine centers around a phasor~ oscillator that generates a continuously increasing ramp signal from 0 to 1. The frequency of this phasor is calculated by dividing the current sample rate by the total number of samples in the array, then multiplied by the speed control factor. This calculation ensures that a speed setting of 1 produces normal playback rate, while other values create proportional speed changes. The phasor output is then scaled to match the array size, creating index values that span the entire audio content.\nThe phase position control provides an additional layer of playback manipulation by allowing users to offset the starting position within the audio material. This parameter is multiplied with the scaled phasor output, enabling jumps to specific locations within the audio file or creating loop-like behaviors when combined with appropriate speed settings. The position control operates independently of the speed system, allowing for complex playback patterns and non-linear audio exploration.\nThe final playback stage employs the tabread4~ object, which performs fourth-order interpolated reading from the audio array. This interpolation method ensures smooth audio reproduction even when the playback speed produces non-integer index values or when rapid parameter changes occur. The interpolated output maintains audio quality across the full range of speed and position manipulations, providing professional-grade sample playback capabilities.\nReal-time monitoring and feedback systems track the current playback position and convert it to visual representations through the horizontal slider display and numeric readouts. The unsig~ objects convert audio-rate signals to control-rate values suitable for interface updates, while the positioning system provides continuous feedback about the current location within the audio material.\n\n\n2.4.3 Key Objects and Their Roles\nThe following table summarizes the essential objects used in the audio sampler implementation:\n\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nsoundfiler\nLoads audio files into arrays and reports sample count\n\n\ntabread4~\nPerforms interpolated audio reading from the array\n\n\nphasor~\nGenerates scanning ramp signal for array indexing\n\n\narray sampler\nStores audio data with automatic resizing capability\n\n\nexpr $f2 / $f1\nCalculates base playback rate from sample count and sample rate\n\n\nline\nProvides smooth interpolation between parameter changes\n\n\npack f f\nCombines value and interpolation time for line object\n\n\nsamplerate~\nReports current system sample rate\n\n\nunsig~\nConverts audio-rate signals to control-rate values\n\n\nhsl\nHorizontal sliders for speed, interpolation, and phase control\n\n\ns / r\nSend and receive objects for parameter distribution\n\n\nloadbang\nInitializes system parameters at patch startup\n\n\n*~\nMultiplies audio signals for speed and direction control\n\n\noutput~\nProvides stereo audio output with level control\n\n\nnbx\nNumeric box for precise parameter display and input\n\n\n\n\nThe patch demonstrates several advanced Pure Data programming techniques, including dynamic array management for variable-length audio files, mathematical rate calculation for accurate speed control, and multi-rate signal processing that combines audio-rate playback with control-rate parameter management. The use of interpolated table reading ensures high audio quality, while the modular design facilitates easy modification and extension of the sampling capabilities.\nThis implementation serves as a foundation for more complex sampling applications, including granular synthesis, multi-sample instruments, and advanced audio manipulation tools. The real-time control capabilities make it suitable for live performance contexts, while the precise parameter control enables detailed studio work and sound design applications.\n\n\n2.4.4 Creative Applications\n\nExperiment with extreme time stretching: Push the speed control to very low values (0.1 or lower) to create drone-like textures from short audio snippets\nCreate rhythmic patterns: Use rapid speed changes and phase jumps to generate percussive sequences from sustained tones\nReverse archaeology: Load full songs and use reverse playback to discover hidden melodic content and create backwards reveals\nGranular-style effects: Combine fast phase position changes with short loop lengths to simulate granular synthesis behaviors\nLive performance tools: Map MIDI controllers to speed and phase parameters for real-time manipulation during performances\nSound design layers: Layer multiple instances with different source materials and speed relationships to create complex evolving textures\nPitch-shifting alternative: Use speed control as a creative pitch-shifting tool, embracing the tempo changes as part of the aesthetic\nMicro-sampling: Load very short sounds (vocal consonants, instrument attacks) and stretch them to reveal internal acoustic details\nCross-fading narratives: Use position control to jump between different parts of spoken word recordings, creating non-linear storytelling\nRhythmic displacement: Sync multiple samplers to the same clock but with different phase offsets to create polyrhythmic patterns",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#references",
    "href": "chapters/playrec.html#references",
    "title": "2  Rec & Play",
    "section": "References",
    "text": "References\n\n\n\n\nHasse, Sarah. 2012. “I Am Sitting in a Room.” Body, Space & Technology 11. https://doi.org/10.16995/bst.71.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html",
    "href": "chapters/sequencer.html",
    "title": "3  Sequencer",
    "section": "",
    "text": "3.1 Arrays and Sequencers\nIn this chapter, we will explore the concept of sequencers and how they can be implemented in Pure Data.\nSequencer is a tool for organizing and controlling the playback of events in a temporally ordered sequence. This sequence consists of a series of discrete steps, where each step represents a regular time interval and can contain information about event activation or deactivation.\nIn addition to controlling musical events such as notes, chords, and percussions, a step sequencer can also manage a variety of other events. This includes parameter changes in virtual instruments or audio/video synthesizers, real-time effect automation, lighting control in live performances or multimedia installations, triggering of samples to create patterns and effect sequences, as well as firing MIDI control events to operate external hardware or software.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#arrays-and-sequencers",
    "href": "chapters/sequencer.html#arrays-and-sequencers",
    "title": "3  Sequencer",
    "section": "",
    "text": "Step\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nValue\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nIndex\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nEach cell represents a step in the sequencer.\n1 to 8 indicate the steps.\nEmpty cells represent steps without events or notes.\nYou can fill each cell with notes or events to represent the desired sequence.\n\n\n3.1.1 Formulation (pseudo code)\nData Structures:\n- Define a data structure to represent each step of the sequence (e.g., array, list).\n\nVariables:\n- Define an array to store the MIDI note values for each step.\n- Initialize the sequencer parameters:\n  - CurrentStep = 0\n  - Tempo\n  - NumberOfSteps = 8\n\nAlgorithm:\n1. Initialization:\n   a. Set CurrentStep to 0.\n   b. Ask the user to input the Tempo.\n   c. Set NumberOfSteps to 8.\n\n2. Loop:\n   a. While true:\n      i. Calculate the time duration for each step based on the Tempo.\n      ii. Play the MIDI note corresponding to the CurrentStep.\n      iii. Increment CurrentStep.\n      iv. If CurrentStep exceeds NumberOfSteps, reset it to 0.\n      v. Wait the calculated duration before moving to the next step.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#implementation-of-the-8-step-sequencer",
    "href": "chapters/sequencer.html#implementation-of-the-8-step-sequencer",
    "title": "3  Sequencer",
    "section": "3.2 Implementation of the 8-Step Sequencer",
    "text": "3.2 Implementation of the 8-Step Sequencer\n\n\n\nFig.\n\n\nThis implementation demonstrates how to create a basic 8-step MIDI sequencer that cycles through a predefined sequence of notes at a controllable tempo. The patch showcases essential concepts including timing control, array-based data storage, step counting with modulo arithmetic, and MIDI output generation.\n\n3.2.1 Patch Overview\nThe 8-step sequencer patch implements a classic step-based sequencing paradigm where musical events are organized into discrete time slots. The system operates by maintaining an internal counter that advances through eight sequential positions, with each position corresponding to a MIDI note value stored in an array. The sequencer provides real-time tempo control through a BPM slider interface and uses Pure Data’s built-in MIDI capabilities to output note events that can be routed to software synthesizers or external MIDI devices.\nThe patch architecture follows a linear signal flow pattern that begins with user control inputs and culminates in MIDI note output. The timing engine uses Pure Data’s metro object as the primary clock source, generating regular pulses that drive the step advancement mechanism. Each pulse triggers a counter increment operation, which is processed through modulo arithmetic to ensure the sequence loops seamlessly after reaching the eighth step. The current step index is then used to retrieve the corresponding MIDI note value from a pre-populated array, which is subsequently formatted into proper MIDI messages and transmitted through the system’s MIDI output infrastructure.\nThe visual interface provides immediate feedback on the sequencer’s operation through numeric displays that show both the current step position and the MIDI note value being played. The tempo control utilizes a mathematical conversion from beats per minute to milliseconds, allowing users to adjust the playback speed in musically meaningful units while maintaining precise timing accuracy.\n\n\n\n\n\nflowchart TD\n    A[Start/Stop Toggle] --&gt; B[Metro Object]\n    C[BPM Slider] --&gt; D[BPM to ms Conversion]\n    D --&gt; B\n    B --&gt; E[Step Counter]\n    E --&gt; F[+1 Increment]\n    F --&gt; G[Modulo 8]\n    G --&gt; E\n    G --&gt; H[Step Index Display]\n    H --&gt; I[Array Lookup]\n    I --&gt; J[MIDI Note Display]\n    J --&gt; K[Makenote Object]\n    K --&gt; L[MIDI Output]\n    M[Loadbang] --&gt; N[Array Initialization]\n    N --&gt; I\n\n\n\n\n\n\n\n\n3.2.2 Data Flow\nThe sequencer’s operation begins when the user activates the start/stop toggle, which enables the metro object to begin generating timing pulses. The tempo of these pulses is determined by the BPM slider value, which is converted from beats per minute to milliseconds using the expression 60000/$f1. This conversion ensures that the metro object receives timing intervals in the appropriate units for accurate tempo control.\nEach timing pulse from the metro object triggers the step counter mechanism, implemented using a combination of f (float storage), +1 (increment), and mod 8 (modulo) objects. The float object stores the current step index, which is incremented by one each time a pulse is received. The incremented value is then processed through the modulo operation to ensure it wraps back to zero after reaching seven, creating the characteristic eight-step cycle.\nThe current step index serves dual purposes within the patch. First, it provides visual feedback to the user through a numeric atom display, allowing real-time monitoring of the sequencer’s position. Second, and more importantly, it functions as an index for retrieving MIDI note values from the midi_note array using the tabread object.\nThe MIDI note array is initialized at patch startup through a loadbang object that sends a message containing the sequence 60 62 64 65 67 69 71 72, representing a C major scale starting from middle C. This initialization ensures that the sequencer has meaningful musical content available immediately upon loading the patch.\nOnce a MIDI note value is retrieved from the array, it is displayed numerically and then processed through the makenote object, which creates properly formatted MIDI note-on and note-off messages. The makenote object is configured with a fixed velocity of 100 and a duration of 250 milliseconds, providing consistent note characteristics across all steps. The resulting MIDI messages are then transmitted through the noteout object to MIDI channel 1, where they can be received by synthesizers or recording software.\n\n\n3.2.3 Key Objects and Their Roles\nThe following table summarizes the essential objects used in the step sequencer implementation and their specific functions within the patch:\n\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\ntgl\nStart/stop toggle switch for sequencer control\n\n\nmetro\nTiming engine that generates regular pulses\n\n\nfloatatom\nBPM input and step/note value display\n\n\nexpr 60000/$f1\nConverts BPM to milliseconds for metro timing\n\n\nf\nStores current step index value\n\n\n+ 1\nIncrements step counter on each pulse\n\n\nmod 8\nWraps step counter back to 0 after reaching 7\n\n\ntabread midi_note\nReads MIDI note values from the array\n\n\nmakenote\nCreates MIDI note-on/off messages with velocity and duration\n\n\nunpack f f\nSeparates MIDI note and velocity values\n\n\nnoteout\nTransmits MIDI messages to specified channel\n\n\nloadbang\nInitializes array with note sequence at patch startup\n\n\narray midi_note\nStores the sequence of MIDI note values\n\n\n\n\nThe patch demonstrates several fundamental programming concepts, including the use of timing objects for rhythmic control, array manipulation for data storage and retrieval, mathematical operations for counter management, and MIDI message formatting for external communication. The modular design allows for easy modification of the note sequence, tempo range, and MIDI output parameters, making it a versatile foundation for more complex sequencing applications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#random-step-sequencer-implementation",
    "href": "chapters/sequencer.html#random-step-sequencer-implementation",
    "title": "3  Sequencer",
    "section": "3.3 Random Step Sequencer Implementation",
    "text": "3.3 Random Step Sequencer Implementation\n\n\n\nFig.\n\n\nThe random step sequencer represents an evolution of the basic sequential approach to rhythm generation. Instead of following a predictable linear progression through stored note values, this implementation introduces stochastic elements that create unpredictable yet musically coherent patterns. This patch demonstrates how randomization can be integrated into sequencing systems to generate non-repetitive musical content while maintaining the fundamental structure of step-based sequencing.\n\n3.3.1 Patch Overview\nThe random step sequencer patch builds upon the foundational concepts of the basic 8-step sequencer while introducing a crucial modification: the step advancement mechanism is replaced with a random selection process. Rather than incrementing through array indices sequentially, the system generates random index values that point to different positions within the stored note sequence. This approach maintains the same timing control, array-based storage, and MIDI output capabilities as the sequential version, but produces entirely different musical results due to the non-linear access pattern.\nThe architecture preserves the linear signal flow from user controls to MIDI output, but introduces a random number generator at the critical junction where step advancement occurs. The timing engine continues to use Pure Data’s metro object as the primary clock source, but each timing pulse now triggers a random selection rather than a predictable increment. This modification transforms the sequencer from a deterministic system into a probabilistic one, where the same stored sequence can produce infinite variations in playback order.\nThe interface maintains the same visual feedback mechanisms as the sequential version, displaying both the randomly selected step index and the corresponding MIDI note value. The tempo control system remains unchanged, allowing users to adjust playback speed while experiencing the unpredictable note selection patterns that emerge from the randomization process.\n\n\n\n\n\nflowchart TD\n    A[Start/Stop Toggle] --&gt; B[Metro Object]\n    C[BPM Slider] --&gt; D[BPM to ms Conversion]\n    D --&gt; B\n    B --&gt; E[Random Generator]\n    E --&gt; F[Random Index 0-7]\n    F --&gt; G[Step Index Display]\n    G --&gt; H[Array Lookup]\n    H --&gt; I[MIDI Note Display]\n    I --&gt; J[Makenote Object]\n    J --&gt; K[MIDI Output]\n    L[Loadbang] --&gt; M[Array Initialization]\n    M --&gt; H\n\n\n\n\n\n\n\n\n3.3.2 Data Flow\nThe operational sequence begins identically to the sequential version, with the user activating the start/stop toggle to enable metro object timing generation. The BPM slider continues to control tempo through the same mathematical conversion formula 60000/$f1, ensuring consistent timing accuracy regardless of the randomization occurring downstream.\nThe fundamental difference emerges at the step generation stage. Instead of using a counter that increments predictably, each timing pulse from the metro object triggers a random number generator implemented with the expr random(0, 8) object. This expression generates random integers between 0 and 7 (inclusive), providing equal probability access to any position within the 8-element note array. The random selection occurs independently for each timing pulse, meaning the same index can be selected multiple times in succession, or the system might skip certain indices entirely during any given playback session.\nThe randomly generated index serves the same dual purpose as in the sequential version: providing immediate visual feedback through a numeric display and functioning as the lookup key for array access. The tabread midi_note object retrieves the MIDI note value corresponding to the randomly selected index, creating an unpredictable sequence of pitches from the stored scale pattern.\nThe subsequent processing stages remain unchanged from the sequential implementation. The retrieved MIDI note value is displayed numerically, then processed through the makenote object with the same fixed velocity (100) and duration (250 milliseconds) parameters. The resulting MIDI messages are transmitted through the noteout object to MIDI channel 1, maintaining full compatibility with external synthesizers and recording systems.\nThe array initialization process follows the identical pattern established in the sequential version, using a loadbang object to populate the midi_note array with the C major scale sequence 60 62 64 65 67 69 71 72 at patch startup. This ensures immediate availability of musical content regardless of the randomization method employed.\n\n\n3.3.3 Key Objects and Their Roles\nThe following table summarizes the essential objects used in the random step sequencer implementation, highlighting the specific changes from the sequential version:\n\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\ntgl\nStart/stop toggle switch for sequencer control\n\n\nmetro\nTiming engine that generates regular pulses\n\n\nfloatatom\nBPM input and step/note value display\n\n\nexpr 60000/$f1\nConverts BPM to milliseconds for metro timing\n\n\nexpr random(0, 8)\nGenerates random indices from 0 to 7\n\n\nbng\nVisual pulse indicator triggered by metro\n\n\ntabread midi_note\nReads MIDI note values from the array\n\n\nmakenote\nCreates MIDI note-on/off messages with velocity and duration\n\n\nunpack f f\nSeparates MIDI note and velocity values\n\n\nnoteout\nTransmits MIDI messages to specified channel\n\n\nloadbang\nInitializes array with note sequence at patch startup\n\n\narray midi_note\nStores the sequence of MIDI note values\n\n\n\n\nThe most significant architectural change is the replacement of the sequential counter mechanism (f, + 1, mod 8) with the random generator (expr random(0, 8)). This substitution fundamentally alters the system’s behavior while maintaining interface compatibility and output format consistency. The addition of a bng (bang) object provides visual confirmation of each timing pulse, which becomes particularly useful when monitoring the random selection process.\nThis implementation demonstrates how simple modifications to algorithmic components can produce dramatically different musical results. The random step sequencer maintains all the practical advantages of the sequential version—including real-time tempo control, visual feedback, and MIDI compatibility—while introducing the creative possibilities inherent in stochastic processes. The resulting system serves as an excellent foundation for exploring probability-based composition techniques and can be extended with weighted random selection, controlled randomness, or hybrid deterministic-stochastic approaches.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#piano-phase-steve-reich",
    "href": "chapters/sequencer.html#piano-phase-steve-reich",
    "title": "3  Sequencer",
    "section": "3.4 Piano Phase (Steve Reich)",
    "text": "3.4 Piano Phase (Steve Reich)\n\n\n\nFig.\n\n\nView Piano Phase Score (S. Reich)\nDesign patterns are abstractions that capture idiomatic tendencies of practices and processes. These abstractions are embedded in live coding languages as functions and syntax, allowing for the reuse of these practices in new projects. (Brown 2023)\nPiano Phase is an example of “music as a gradual process,” as Reich stated in his essay from 1968.(Reich 2002). In it, Reich described his interest in using processes to generate music, particularly noting how the process is perceived by the listener. (Processes are deterministic: a description of the process can describe an entire whole composition.[5] In other words, once the basic pattern and the phase process have been defined, the music consists itself.)\nReich called the unexpected ways change occurred via the process “by-products”, formed by the superimposition of patterns. The superimpositions form sub-melodies, often spontaneously due to echo, resonance, dynamics, and tempo, and the general perception of the listener.(Epstein 1986)\nPiano Phase led to several breakthroughs that would mark Reich’s future compositions. The first is the discovery of using simple but flexible harmonic material, which produces remarkable musical results when phasing occurs. The use of 12-note or 12-division patterns in Piano Phase proved to be successful, and Reich would re-use it in Clapping Music and Music for 18 Musicians. Another novelty is the appearance of rhythmic ambiguity during phasing of a basic pattern. The rhythmic perception during phasing can vary considerably, from being very simple (in-phase), to complex and intricate.\nThe first section of Piano Phase has been the section studied most by musicologists. A property of the first section of phase cycle is that it is symmetric, which results in identical patterns half-way through the phase cycle.(Epstein 1986)\nPiano Phase can be conceived as an algorithm. Starting with two pianos playing the same sequence of notes at the same speed, one of the pianists begins to gradually accelerate their tempo.\nWhen the separation between the notes played by both pianos reaches a fraction of a note’s duration, the phase is shifted and the cycle repeats. This process continues endlessly, creating a hypnotic effect of rhythmic phasing that evolves over time as an infinite sequence of iterations.\n\n\n\nPhase Difference\n\n\n\n3.4.1 Algorithm Formulation (pseudo code)\n1. Initialize two pianists with the same note sequence.\n2. Set an initial tempo for both pianists.\n3. Repeat until the desired phase is reached:\n     a. Gradually increase the second pianist's tempo.\n     b. Compare note positions between both pianists.\n     c. When the separation between the notes reaches a fraction of a note’s duration, invert the phase.\n     d. Continue playback.\n4. Repeat the cycle indefinitely to create a continuous and evolving rhythmic phasing effect.\n\n\n3.4.2 Piano Phase Implementation in PD\n\n\n\nfig\n\n\n\n\n3.4.3 Patch Overview\nThe piano-phase.pd patch implements a minimalist phasing process inspired by Steve Reich’s Piano Phase. It uses two parallel sequencers, each reading from the same melodic sequence but advancing at slightly different rates. This gradual tempo difference causes the two sequences to drift out of phase, creating evolving rhythmic and melodic patterns.\n\n\n\n\n\nflowchart TD\n    Start([Start/Stop]) --&gt; M1[Clock 1 410ms]\n    Start --&gt; M2[Clock 2 406ms]\n    M1 --&gt; C1[Counter 1]\n    M2 --&gt; C2[Counter 2]\n    C1 --&gt; T1[read sequence]\n    C2 --&gt; T2[read sequence]\n    T1 --&gt; MK1[MIDI noteout]\n    T2 --&gt; MK2[MIDI noteout]\n\n\n\n\n\n\n\n\n3.4.4 Sequence Data\n[table sequence 12]\nThe sequence is stored in a table named sequence with 12 MIDI note values:\n64 66 71 73 74 66 64 73 71 66 74 73\nThis pattern is initialized at patch load.\n\n\n3.4.5 Data Flow\nInitialization\n\nThe sequence table is filled with 12 MIDI notes.\nTwo metro objects are set: one at 410 ms (left), one at 406 ms (right).\nBoth sequencers are started/stopped with a single toggle.\n\nPlayback\n\nEach metro triggers its own counter, which advances from 0 to 11 and wraps around.\nThe current index is used to read a note from the sequence table.\nThe note is played via makenote and sent to MIDI output.\nThe current step is visualized with a horizontal radio button (hradio).\n\nPhasing Effect\n\nThe right sequencer is slightly faster, so its step index gradually shifts ahead of the left sequencer.\nOver time, the two sequences drift out of phase, producing new rhythmic and melodic combinations.\nEventually, the faster sequencer laps the slower one, and the process repeats.\n\n\n\n3.4.6 Key Objects and Their Roles\nTwo identical sequences are played in parallel. Slightly different tempos cause the sequences to gradually shift out of phase. Emergent patterns arise from the interaction of the two sequences. Visual feedback helps track the phase relationship. This patch demonstrates how minimalist processes can generate complex musical results through simple, deterministic rules, echoing the core concept of Steve Reich’s Piano Phase.\n\n\n\nObject\nPurpose\n\n\n\n\nmetro\nSets the timing for each sequencer\n\n\nexpr int(60000/$f1)\nConverts BPM to milliseconds\n\n\nf, + 1, mod 12\nAdvances and wraps the step index\n\n\ntabread sequence\nReads the current note from the sequence\n\n\nmakenote\nGenerates MIDI note-on/off with duration\n\n\nnoteout\nSends MIDI notes to the output device\n\n\nhradio\nVisualizes the current step position\n\n\nloadbang\nInitializes sequence and metro intervals",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#random-melody-generator",
    "href": "chapters/sequencer.html#random-melody-generator",
    "title": "3  Sequencer",
    "section": "3.5 Random Melody Generator",
    "text": "3.5 Random Melody Generator\nThis chapter provides a step-by-step explanation of the music-scale-B.pd Pure Data patch. The patch demonstrates how to generate a melody using a musical scale, store it in an array, and play it back using MIDI.\n\n\n\nRandom Melody Generator\n\n\n\n3.5.1 Patch Overview\nThe Random Melody Generator patch demonstrates a comprehensive approach to algorithmic composition. This implementation showcases how to generate melodic content using musical scales, store the generated material in arrays for manipulation, and create a playback system with MIDI output capabilities.\nThe patch architecture is built around several interconnected functional blocks that work together to create a complete generative music system. The first component handles scale definition and melody generation, where musical scales are defined as interval patterns and used as the foundation for creating new melodic material. The melody storage system utilizes array objects to hold the generated sequences, providing both data persistence and visual feedback. The playback control mechanism employs metronome objects and indexing systems to sequence through the stored melodies at controllable tempos. Finally, the MIDI output section transforms the stored note data into playable MIDI messages that can be sent to external synthesizers or software instruments.\nThis modular design approach allows for easy modification and extension of the system. Each functional block can be adjusted independently, enabling composers and sound designers to experiment with different scales, generation algorithms, storage methods, and playback configurations without affecting the overall system stability.\nBelow is a simplified diagram of the main data flow:\n\n\n\n\n\nflowchart TD\n    S[Scale Message] --&gt; L[list Append]\n    L --&gt; R[random + Offset]\n    R --&gt; M[Modulo/Indexing]\n    M --&gt; A[Melody Array]\n    A --&gt; P[Playback - metro, tabread]\n    P --&gt; MN[makenote]\n    MN --&gt; NO[noteout]\n\n\n\n\n\n\nThe scale definition process begins with a message box containing the fundamental building blocks of Western harmony represented as semitone intervals.\nThe example scale 0 2 4 5 7 9 11 represents a major scale pattern, where each number corresponds to a specific semitone distance from a root note. This intervallic approach provides flexibility, as the same pattern can be transposed to any key by simply changing the root note offset. The scale intervals are processed through list manipulation objects, which append the values to an internal list structure and calculate the scale’s length for use in subsequent random selection processes.\nThe following table illustrates the mapping of musical notes to their corresponding intervals in semitones, which is crucial for understanding how the scale is constructed and how notes are generated from it.\n\n\n\n\nNote\nInterval\n\n\n\n\nC\n0\n\n\nC# / Db\n1\n\n\nD\n2\n\n\nD# / Eb\n3\n\n\nE\n4\n\n\nF\n5\n\n\nF# / Gb\n6\n\n\nG\n7\n\n\nG# / Ab\n8\n\n\nA\n9\n\n\nA# / Bb\n10\n\n\nB\n11\n\n\n\n\nThe random note selection mechanism forms the core of the melodic generation process. A random number generator produces values between 0 and 47, which are then shifted upward by 60 to place the resulting MIDI notes in a musically appropriate range around middle C. This range selection ensures that generated melodies will fall within a playable range for most instruments and remain audible in typical monitoring setups. The random values are then mapped to the predefined scale using modulo operations and list indexing, ensuring that all generated notes conform to the harmonic framework established by the scale definition.\nThe melody construction process utilizes looping mechanisms, specifically the until, i, and + 1 objects, to generate sequences of predetermined length. Each iteration of the loop calculates a new note based on the scale mapping process and stores the result in a growing list structure. This approach allows for the creation of melodic phrases of any desired length, from short motifs to extended melodic lines. The deterministic nature of the loop structure ensures predictable behavior while the random element within each iteration provides the variability necessary for interesting melodic content.\nFor melody storage, the patch employs array system, specifically an array named melody that serves as both a data repository and a visual display element. The generated melodic sequences are written to this array using the tabwrite object, which provides indexed storage capabilities. The array’s visual representation appears as a graph within the patch interface, allowing users to see the melodic contour and note relationships in real-time. This visual feedback proves invaluable for understanding the characteristics of generated material and making informed decisions about parameter adjustments.\nThe playback control system centers around a metro object that functions as a digital metronome, generating regular timing pulses that drive the sequencing process. The tempo of these pulses is controlled by a horizontal slider interface element, providing real-time tempo adjustment capabilities. Each pulse from the metronome advances an index counter that points to the next position in the melody array. This indexing system automatically wraps around to the beginning when it reaches the end of the stored sequence, creating seamless looping playback that continues indefinitely until stopped by the user.\nThe MIDI output stage transforms the stored melodic data into standard MIDI protocol messages that can interface with external hardware and software. The note values retrieved from the array are processed through a makenote object, which creates properly formatted MIDI note-on and note-off messages with specified velocity and duration parameters. These parameters can be adjusted to affect the musical expression of the playback, with velocity controlling the apparent loudness or intensity of notes and duration determining their length. The final noteout object handles the actual transmission of MIDI data to the system’s MIDI routing infrastructure, where it can be directed to synthesizers, samplers, or recording software.\nThe following is a simplified diagram of the data flow for the melody generation and playback process:\n\n\n\n\n\ngraph LR\n    MelodyList --&gt;|tabwrite| MelodyArray\n\n\n\n\n\n\nThe integration of these components creates a flexible and powerful tool for algorithmic composition. The system can generate new melodic material on demand, store multiple variations for comparison, and play back the results in real-time with full MIDI compatibility. This makes it suitable for both experimental composition work and practical music production applications, where generated material can serve as inspiration, accompaniment, or the foundation for further development.\n\n\n3.5.2 Key Objects and Their Roles\n\n\n\n\nObject\nPurpose\n\n\n\n\nrandom 48\nGenerates random note indices\n\n\n+ 60\nShifts notes to a higher MIDI octave\n\n\nmod 12\nMaps notes to scale degrees\n\n\nlist-idx\nRetrieves scale degree from the list\n\n\ntabwrite\nWrites notes to the melody array\n\n\nmetro\nControls playback timing\n\n\ntabread\nReads notes from the melody array\n\n\nmakenote\nCreates MIDI notes with velocity/duration\n\n\nnoteout\nSends MIDI notes to output\n\n\n\n\n\n\n3.5.3 Diagram: Melody Generation and Playback\n\n\n\n\n\nflowchart TD\n    S[Scale Message] --&gt; L[list Append]\n    L --&gt; R[random + Offset]\n    R --&gt; M[Modulo/Indexing]\n    M --&gt; A[Melody Array]\n    A --&gt; P[Playback - metro, tabread]\n    P --&gt; MN[makenote]\n    MN --&gt; NO[noteout]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#the-euclidean-algorithm-generates-traditional-musical-rhythms",
    "href": "chapters/sequencer.html#the-euclidean-algorithm-generates-traditional-musical-rhythms",
    "title": "3  Sequencer",
    "section": "3.6 The Euclidean Algorithm Generates Traditional Musical Rhythms",
    "text": "3.6 The Euclidean Algorithm Generates Traditional Musical Rhythms\nGodfried Toussaint (2005)\n\n\n\nEucliden image\n\n\nLink to the original article\nThe Euclidean rhythm in music was discovered by Godfried Toussaint in 2004 and is described in a 2005 paper “The Euclidean Algorithm Generates Traditional Musical Rhythms”.(Toussaint 2005) The paper presents a method for generating rhythms that are evenly distributed over a given time span, using the Euclidean algorithm. This method is particularly relevant in the context of world music, where such rhythms are often found. The Euclidean algorithm (as presented in Euclid’s Elements) calculates the greatest common divisor of two given integers. It is shown here that the structure of the Euclidean algorithm can be used to efficiently generate a wide variety of rhythms used as timelines (ostinatos) in sub-Saharan African music in particular, and world music in general. These rhythms, here called Euclidean rhythms, have the property that their onset patterns are distributed as evenly as possible. Euclidean rhythms also find application in nuclear physics accelerators and computer science and are closely related to several families of words and sequences of interest in the combinatorics of words, such as Euclidean strings, with which the rhythms are compared.(Toussaint 2005)\n\n3.6.1 Hypothesis\nSeveral researchers have observed that rhythms in traditional world music tend to exhibit patterns distributed as regularly or evenly as possible. The hypothesis is that the Euclidean algorithm can be used to generate these rhythms. According to the hypothesis, the Euclidean algorithm can be used to generate rhythms that are evenly distributed over a given time span. This is particularly relevant in the context of world music, where such rhythms are often found. The following is a summary of the main points:\n\nPatterns of maximal evenness can be described using the Euclidean algorithm on the greatest common divisor of two integers.\n\n\n\n3.6.2 Patterns of Maximal Evenness\nThe Pattern of Maximal Evenness is a concept used in music theory to create Euclidean rhythms. Euclidean rhythms are rhythmic patterns that evenly distribute beats over a time cycle.\nIn essence, the Pattern of Maximal Evenness seeks to distribute a specific number of beats evenly within a given time span. This is achieved by dividing the time into equal parts and assigning beats to these divisions uniformly.\nExample:\nx = beat\n· = rest\n[×· ×· ×· ×· ] → [1 0 1 0 1 0 1 0] (8,4) = 4 beats evenly distributed over 8 pulses\n[×· ·×· ·×·] → [1 0 0 1 0 0 1 0] (8,3) = 3 beats evenly distributed over 8 pulses\n\n\n3.6.3 Euclidean Algorithm\nOne of the oldest known algorithms, described in Euclid’s Elements (around 300 BCE) in Proposition 2 of Book VII, now known as the Euclidean algorithm, calculates the greatest common divisor of two given integers.\nThe idea is very simple. The smaller number is repeatedly subtracted from the larger until the larger becomes zero or smaller than the smaller one, in which case it becomes the remainder. This remainder is then repeatedly subtracted from the smaller number to obtain a new remainder. This process continues until the remainder is zero.\nTo be more precise, consider the numbers 5 and 8 as an example:\n\nFirst, we divide 8 by 5. This gives a quotient of 1 and a remainder of 3.\nThen, we divide 5 by 3, which gives a quotient of 1 and a remainder of 2.\nNext, we divide 3 by 2, which gives a quotient of 1 and a remainder of 1.\nFinally, we divide 2 by 1, which gives a quotient of 2 and a remainder of 0.\n\nThe idea is to keep dividing the previous divisor by the remainder obtained in each step until the remainder is 0. When we reach a remainder of 0, the previous divisor is the greatest common divisor of the two numbers.\nIn short, the process can be seen as a sequence of equations:\n8 = (1)(5) + 3\n5 = (1)(3) + 2\n3 = (1)(2) + 1\n2 = (1)(2) + 0\nNote: 8 = (1)(5) + 3 means that 8 is divided by 5 once, yielding a quotient of 1 and a remainder of 3.\n \nIn essence, it involves successive divisions to find the greatest common divisor of two positive numbers (GCD from now on).\nThe GCD of two numbers a and b, assuming a &gt; b, is found by first dividing a by b, and obtaining the remainder r.\nThe GCD of a and b is the same as that of b and r. When we divide a by b, we obtain a quotient c and a remainder r such that:\na = c · b + r\nExamples:\nLet’s compute the GCD of 17 and 7.\nSince 17 = 7 · 2 + 3, then GCD(17, 7) is equal to GCD(7, 3). Again, since 7 = 3 · 2 + 1, then GCD(7, 3) is equal to GCD(3, 1). Here, it is clear that the GCD between 3 and 1 is simply 1. Therefore, the GCD between 17 and 7 is also 1.\nGCD(17,7) = 1\n\n17 = 7 · 2 + 3\n\n7 = 3 · 2 + 1\n\n3 = 1 · 3 + 0\n \nAnother example:\nGCD(8,3) = 1\n\n8 = 3 · 2 + 2\n\n3 = 2 · 1 + 1\n\n2 = 1 · 2 + 0\n\n\n3.6.4 How does computing the GCD turn into maximally even distributed patterns?\nRepresent a binary sequence of k ones [1] and n − k zeros [0], where each [0] bit represents a time interval and the ones [1] indicate signal triggers.\nThe problem then reduces to:\nConstruct a binary sequence of n bits with k ones such that the ones are distributed as evenly as possible among the zeros.\nA simple case is when k evenly divides n (with no remainder), in which case we should place ones every n/k bits. For example, if n = 16 and k = 4, then the solution is:\n[1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0]\nThis case corresponds to n and k having a common divisor of k (in this case 4).\nMore generally, if the greatest common divisor between n and k is g, we would expect the solution to decompose into g repetitions of a sequence of n/g bits.\nThis connection with greatest common divisors suggests that we could compute a maximally even rhythm using an algorithm like Euclid’s.\n \n\n3.6.4.1 Example (13, 5)\nLet’s consider a sequence with n = 13 and k = 5.\nSince 13 − 5 = 8, we start with a sequence consisting of 5 ones, followed by 8 zeros, which can be thought of as 13 one-bit sequences:\n[1 1 1 1 1 0 0 0 0 0 0 0 0]\nWe begin moving zeros by placing one zero after each one, creating five 2-bit sequences, with three remaining zeros:\n[10] [10] [10] [10] [10] [0] [0] [0]\n13 = 5 · 2 + 3\nThen we distribute the three remaining zeros similarly, placing a [0] after each [10] sequence:\n[100] [100] [100] [10] [10]\n5 = 3 · 1 + 2\nWe now have three 3-bit sequences, and a remainder of two 2-bit sequences. So we continue the same way, placing one [10] after each [100]:\n[10010] [10010] [100]\n3 = 2 · 1 + 1\nThe process stops when the remainder consists of a single sequence (here, [100]), or we run out of zeros.\nThe final sequence is, therefore, the concatenation of [10010], [10010], and [100]:\n[1 0 0 1 0 1 0 0 1 0 1 0 0]\n2 = 1 · 2 + 0\n \n\n\n3.6.4.2 Example (17, 7)\nSuppose we have 17 pulses and want to evenly distribute 7 beats over them.\n1. We align the number of beats and silences (7 ones and 10 zeros):\n[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n\n\n\n1 1 1 1 1 1 1\n0 0 0 0 0 0 0 0 0 0\n\n\n\n\n\n2. We form 7 groups, corresponding to the division of 17 by 7; we get 7 groups of [1 0] and 3 remaining zeros [000], which means the next step forms 3 groups until only one or zero groups remain.\n[1 0] [1 0] [1 0] [1 0] [1 0] [1 0] [1 0] [0] [0] [0]\n17 = 7 · 2 + 3\n\n\n\n1\n1\n1\n1\n1\n1\n1\n0 0 0\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n \n3. Again, this corresponds to dividing 7 by 3. In our case, we are left with only one group and we are done.\n \n[1 0 0] [1 0 0] [1 0 0] [1 0] [1 0] [1 0] [1 0]\n\n\n\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n[1 0 0 1 0] [1 0 1 0 0] [1 0 0 1 0] [1 0]\n7 = 3 · 2 + 1\n\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n1\n1\n1\n\n\n\n\n\n\n0\n0\n0\n\n\n\n\n\n\n\n \n4. Finally, the rhythm is obtained by reading the grouping column by column, from left to right, step by step.\n[1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0]\n3 = 1 · 3 + 0\n \n\n\n\n3.6.5 Implementation in Pure Data - Euclidean sequencer\nThe following code implements the Euclidean algorithm in Pure Data to generate a Euclidean rhythm. The algorithm is based on the mathematical formula:\n(index * hits ) % steps\n↓\n[&lt; notes]\n\n\n\n\n\nflowchart TB\n    A[index * hits]\n    A --&gt; B[% steps]\n    B --&gt; C[&lt; notes]\n\n\n\n\n\n\nwhere:\nindex = index of the Euclidean series (array)\nhits = number of notes to be played\nsteps = array size\nYoy can check this Euclidean rhythm demo in order to interact and see how the algorithm works.\n\n3.6.5.1 Understanding the Mathematical Formula\nThe formula (index * hits) % steps &lt; hits provides a simple and efficient way to approximate the distribution of pulses (or beats) over a sequence of discrete steps. It works by multiplying the current position (represented by index) by the total number of hits (pulses) desired. This result is then taken modulo the total number of steps to ensure it wraps around properly in a cyclical pattern. Finally, the result of this modulo operation is compared to the number of hits. If the condition is true, we place a beat at that position; otherwise, we place a rest.\nThis method produces a rhythm by relying on modular arithmetic. The result is a mathematically regular distribution of beats that often approximates what we expect from an even rhythmic distribution. However, it does so without any recursive logic or iteration—it simply applies a consistent rule to each step in isolation. This makes the formula extremely efficient: it runs in constant time for any position, and it does not require any memory to store the pattern.\n\n\n3.6.5.2 The Bjorklund Algorithm Explained\nIn contrast, the Bjorklund algorithm is a more sophisticated procedure rooted in the Euclidean algorithm for computing the greatest common divisor (GCD). This algorithm begins with two values: the number of pulses (beats) and the number of rests (steps minus beats). It then recursively groups these elements in a way that maximizes the evenness of their distribution.\nThe method proceeds by repeatedly pairing elements—first grouping pulses with rests, then regrouping the leftovers, and so on—until the sequence cannot be subdivided further. The final pattern emerges from this recursive grouping, and it is typically rotated so that it starts with a pulse. The result is a rhythm that is maximally even, meaning that the beats are spaced as equally as possible given the constraints.\nThis process, while more computationally demanding and conceptually complex, produces the canonical Euclidean rhythms often cited in academic and musical literature.\n\n\n3.6.5.3 Comparing the Formula and the Algorithm\nThe core difference between the two approaches lies in how they arrive at the rhythmic pattern. The mathematical formula provides a direct, position-based method for determining beat placement. It does not take into account the context of previous or future beats—it treats each step in isolation. This is why it is so fast and well-suited for real-time applications, such as live audio processing in Pure Data or other creative coding environments.\nThe Bjorklund algorithm, on the other hand, is concerned with the global structure of the pattern. It ensures that beats are distributed with maximal evenness and follows a well-defined sequence of operations that are both recursive and stateful. This means it needs to store and manipulate arrays of data to arrive at the final rhythm. The computational complexity of this algorithm is higher, and it is not as straightforward to implement, but the results are musically and mathematically robust.\nOne major distinction is that the formula often produces a rotated version of the Bjorklund rhythm. That is, while the number and spacing of beats may be similar, the starting position may differ. The Bjorklund rhythm always starts with a pulse, ensuring that it adheres to a particular musical convention, while the formula does not guarantee this.\nAnother important distinction lies in the distribution logic. The formula uses a fixed mathematical rule to space out the beats, leading to regular but not necessarily optimal placement. The Bjorklund algorithm, however, iteratively rearranges beats and rests to achieve the best possible balance.\n\n\n3.6.5.4 A Concrete Example: (13, 5)\nLet’s consider the case of 13 steps with 5 beats.\nUsing the mathematical formula, we apply (index * 5) % 13 &lt; 5 for each position:\nPattern: 1 0 0 1 0 0 1 0 1 0 0 1 0\nIn contrast, the Bjorklund algorithm produces:\nPattern: 1 0 1 0 0 1 0 1 0 0 1 0 0\nBoth patterns contain five beats. Both distribute them fairly evenly. But the Bjorklund version achieves a more perceptually even spacing and aligns with theoretical expectations. The formula result is essentially a rotated variant.\nIn summary, the formula (index * hits) % steps &lt; hits offers a pragmatic and computationally efficient way to generate rhythm patterns that resemble Euclidean rhythms. It is well-suited for real-time use cases and environments where simplicity and speed are more important than strict accuracy. In contrast, the Bjorklund algorithm provides a mathematically rigorous method for generating rhythms with maximal evenness. It aligns with canonical definitions and is favored in theoretical and compositional contexts.The choice between these methods depends on your priorities: use the formula for lightweight, real-time approximation, and use Bjorklund when you need precision and adherence to the canonical Euclidean model.\n\n\n\n3.6.6 Euclidean Rhythm Generator\nThis section provides a detailed explanation of the Euclidean-basic-Serie.pd patch. This Pure Data patch demonstrates how to generate Euclidean rhythms using a straightforward mathematical formula.\n\n\n\nFig.\n\n\n\n3.6.6.1 Patch Overview\nThe patch is structured into several functional blocks. First, there are controls for setting the number of steps and the number of hits (pulses). Next, the patch calculates the Euclidean pattern using a mathematical formula. Finally, the resulting pattern is output and visualized as a list of pulses and rests.\nTo help visualize the flow of data through the patch, consider the following diagram:\n\n\n\n\n\nflowchart TD\n    S[Steps Slider] --&gt; A[Step Counter]\n    H[Hits Slider] --&gt; B[Hits Value]\n    A --&gt; C[Euclidean Formula: index * hits % steps]\n    B --&gt; C\n    C --&gt; D[Compare &lt; hits]\n    D --&gt; E[Pattern List]\n    E --&gt; F[Output/Display]\n\n\n\n\n\n\n\n\n3.6.6.2 Data Flow\nThe process begins with the user setting the number of steps in the sequence using a horizontal slider labeled “Steps.” This determines the total length of the rhythmic cycle. The user also sets the number of hits, or pulses, using another slider labeled “HITS.” These two values are stored and sent to the rest of the patch, where they are used to calculate the rhythmic pattern.\nOnce the parameters are set, the patch uses a loop, implemented with the until object, to iterate through each step index from 0 up to one less than the total number of steps. For each index, the patch calculates a value using the formula (index * hits) % steps. This formula determines the position of each pulse within the cycle by multiplying the current index by the number of hits and taking the result modulo the total number of steps. The result of this calculation is then compared to the number of hits. If the result is less than the number of hits, a pulse (represented by a 1) is added to the pattern. Otherwise, a rest (represented by a 0) is added. This process is repeated for every step in the sequence, gradually building up the complete Euclidean rhythm as a list of ones and zeros.\nAfter the pattern has been calculated, it is displayed as a list, allowing you to see the sequence of pulses and rests. The patch also includes a reset mechanism, so that the pattern can be recalculated and updated whenever the user changes the number of steps or hits.\n\n\n3.6.6.3 Example: (Steps = 13, Hits = 5)\nTo illustrate how the patch works, consider the case where the number of steps is 13 and the number of hits is 5. The following table shows the calculation for each step:\n\n\n\nIndex\nCalculation\nResult\n&lt; Hits?\nOutput\n\n\n\n\n0\n(0 * 5) % 13 = 0\n0\nYes\n1\n\n\n1\n(1 * 5) % 13 = 5\n5\nYes\n1\n\n\n2\n(2 * 5) % 13 = 10\n10\nNo\n0\n\n\n3\n(3 * 5) % 13 = 2\n2\nYes\n1\n\n\n4\n(4 * 5) % 13 = 7\n7\nNo\n0\n\n\n5\n(5 * 5) % 13 = 12\n12\nNo\n0\n\n\n6\n(6 * 5) % 13 = 4\n4\nYes\n1\n\n\n7\n(7 * 5) % 13 = 9\n9\nNo\n0\n\n\n8\n(8 * 5) % 13 = 1\n1\nYes\n1\n\n\n9\n(9 * 5) % 13 = 6\n6\nNo\n0\n\n\n10\n(10 * 5) % 13 = 11\n11\nNo\n0\n\n\n11\n(11 * 5) % 13 = 3\n3\nYes\n1\n\n\n12\n(12 * 5) % 13 = 8\n8\nNo\n0\n\n\n\nThe resulting pattern is:\n1 1 0 1 0 0 1 0 1 0 0 1 0\nThis sequence distributes five pulses as evenly as possible across thirteen steps.\n\n\n3.6.6.4 Key Objects and Their Roles\nThe following table summarizes the key objects used in the patch and their roles:\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nhsl (slider)\nSets number of steps and hits\n\n\nuntil\nLoops through each step index\n\n\nexpr\nCalculates (index * hits) % steps\n\n\n&lt;\nCompares result to hits, outputs 1 or 0\n\n\nadd2\nAppends each result to the output pattern list\n\n\nset\nResets the output list for new calculations\n\n\nmod\nEnsures index wraps around for correct calculation\n\n\n\n\n\n\n3.6.7 Euclidean Rotation Generator\nThis section explains the Euclidean-Rotation.pd patch. This Pure Data patch builds on the basic Euclidean rhythm generator by introducing a rotation parameter, allowing you to shift the starting point of the rhythm pattern. This feature is useful for exploring different phase relationships and rhythmic variations without changing the underlying distribution of pulses.\n\n\n\nFig.\n\n\n\n3.6.7.1 Patch Overview\nThe patch allows you to set the number of steps, the number of hits (pulses), and the rotation amount. The rotation parameter shifts the pattern cyclically, so the rhythm can start at any point in the sequence. The patch calculates the Euclidean pattern using a formula that incorporates the rotation, and then outputs the resulting pattern for visualization and further use.\nThe following diagram illustrates the data flow for the rhythm and rotation:\n\n\n\n\n\nflowchart TD\n    Steps[Steps Slider] --&gt; Counter[Step Counter]\n    Hits[Hits Slider] --&gt; HitsVal[Hits Value]\n    Rot[Rotation Slider] --&gt; RotVal[Rotation Value]\n    Counter --&gt; Formula[Euclidean Formula: index_plus_rotation_times_hits_mod_steps]\n    RotVal --&gt; Formula\n    HitsVal --&gt; Formula\n    Formula --&gt; Compare[Compare to Hits]\n    Compare --&gt; Pattern[Pattern List]\n    Pattern --&gt; Output[Output/Display]\n\n\n\n\n\n\n\n\n3.6.7.2 Data Flow\nThe process begins with the user setting the number of steps, hits, and rotation using horizontal sliders. The steps slider determines the total length of the rhythmic cycle, the hits slider sets the number of pulses to be distributed, and the rotation slider specifies how many positions to shift the pattern.\nA loop, implemented with the until object and a counter, iterates through each step index. For each index, the patch calculates the value using the formula ((index + rotation) * hits) % steps. This formula determines the position of each pulse within the cycle, taking into account the rotation. The result is then compared to the number of hits: if it is less than the number of hits, a pulse (represented by a 1) is added to the pattern; otherwise, a rest (represented by a 0) is added. This process is repeated for every step, building up the complete rotated Euclidean rhythm.\nThe resulting pattern is displayed as a list, showing the sequence of pulses and rests after rotation. The patch also provides a reset mechanism to clear and recalculate the pattern when parameters change.\n\n\n3.6.7.3 Example: Rotating a Euclidean Rhythm\nSuppose you set 8 steps, 3 hits, and a rotation of 2. The main rhythm without rotation might be:\n1 0 0 1 0 0 1 0\nApplying a rotation of 2 shifts the pattern two steps to the right, resulting in:\n0 1 0 0 1 0 0 1\nThis allows you to experiment with different phase offsets and rhythmic feels.\n\n\n3.6.7.4 Key Objects and Their Roles\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nhsl (slider)\nSets number of steps, hits, and rotation\n\n\nuntil\nLoops through each step index\n\n\ncounter\nIncrements the step index for each iteration\n\n\nexpr\nCalculates the Euclidean formula with rotation\n\n\n&lt;\nCompares the formula result to hits, outputs 1 or 0\n\n\nadd2\nAppends each result to the output pattern list\n\n\nset\nResets the output list for new calculations\n\n\nmod\nEnsures index wraps around for correct calculation\n\n\ntabwrite\nWrites the final pattern to an array for visualization\n\n\nprint\nPrints the final pattern to the console (for debugging)\n\n\n\n\n\n\n3.6.8 Euclidean Accents Generator\nThis section explains the Euclidean-Accents.pd patch. This Pure Data patch extends the basic Euclidean rhythm generator by adding a second, independent Euclidean pattern to control accents. The result is a two-layer sequencer: one layer for the main rhythm (hits) and another for accentuation, allowing for more expressive and dynamic rhythmic patterns.\n\n\n\nFig.\n\n\n\n3.6.8.1 Patch Overview\nThe patch is organized into two parallel sections. The first section generates the main Euclidean rhythm, while the second section generates an accent pattern using the same mathematical approach. Both sections allow for independent control of steps, hits, rotation, and accents. The outputs of both patterns are then combined to produce a final sequence where steps can be silent, regular, or accented.\nThe following diagram illustrates the data flow for both rhythm and accent layers:\n\n\n\n\n\nflowchart TD\n    S[Steps Slider] --&gt; A[Step Counter]\n    H[Hits Slider] --&gt; B[Hits Value]\n    R[Rotation Slider] --&gt; C[Rotation Value]\n    A --&gt; D[Euclidean Formula: index+rotation*hits%steps]\n    B --&gt; D\n    C --&gt; D\n    D --&gt; E[Compare &lt; hits]\n    E --&gt; F[Main Pattern List]\n    ACC[Accents Slider] --&gt; ACCV[Accents Value]\n    ACCROT[Accents Rotation Slider] --&gt; ACCROTVAL[Accents Rotation Value]\n    A2[Step Counter] --&gt; D2[Euclidean Formula: index+acc_rotation*accents %steps]\n    ACCV --&gt; D2\n    ACCROTVAL --&gt; D2\n    D2 --&gt; E2[Compare &lt; accents]\n    E2 --&gt; F2[Accent Pattern List]\n    F & F2 --&gt; G[Combine Patterns]\n    G --&gt; H[Output/Display]\n\n\n\n\n\n\n\n\n3.6.8.2 Data Flow\nThe process begins with the user setting the number of steps, hits, and rotation for the main rhythm using horizontal sliders. These parameters determine the length of the sequence, the number of pulses, and the rotation (offset) of the pattern. The patch uses a loop (until) and a counter to iterate through each step index. For each step, it calculates the value using the formula ((index + rotation) * hits) % steps. This value is compared to the number of hits; if it is less, a pulse (1) is added to the main pattern, otherwise a rest (0).\nIn parallel, the user can set the number of accents and their rotation using additional sliders. The accent pattern is generated in the same way as the main rhythm, but with its own independent parameters. The formula ((index + acc_rotation) * accents) % steps is used to determine the placement of accents. The result is a second pattern of 1s (accent) and 0s (no accent).\nAfter both patterns are generated, they are combined step by step. If both the main pattern and the accent pattern have a 1 at the same step, the output for that step is set to 2 (indicating an accented hit). If only the main pattern has a 1, the output is 1 (regular hit). If neither has a 1, the output is 0 (rest). The final combined pattern is written to an array and displayed, allowing for visualization and further use in sequencing.\n\n\n3.6.8.3 Example: Combining Rhythm and Accents\nSuppose you set 8 steps, 3 hits, and 2 accents. The main rhythm might produce a pattern like:\n1 0 0 1 0 0 1 0\nThe accent pattern, with its own rotation, might produce:\n1 0 1 0 0 1 0 0\nCombining these, the output would be:\n2 0 1 1 0 1 1 0\nHere, “2” indicates an accented hit, “1” a regular hit, and “0” a rest.\n\n\n3.6.8.4 Key Objects and Their Roles\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nhsl (slider)\nSets number of steps, hits, rotation, accents, and accent rotation\n\n\nuntil\nLoops through each step index\n\n\ncounter\nIncrements the step index for each iteration\n\n\nexpr\nCalculates the Euclidean formula for both rhythm and accent patterns\n\n\n&lt;\nCompares the formula result to hits or accents, outputs 1 or 0\n\n\nadd2\nAppends each result to the output pattern list\n\n\nset\nResets the output list for new calculations\n\n\nlist-idx\nRetrieves values from the generated lists for combination\n\n\nexpr if($f1+$f2==2, 2, $f1)\nCombines main and accent patterns into a single output\n\n\ntabwrite\nWrites the final pattern to an array for visualization\n\n\nprint\nPrints the final pattern to the console (for debugging)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#cellular-automata",
    "href": "chapters/sequencer.html#cellular-automata",
    "title": "3  Sequencer",
    "section": "3.7 Cellular Automata",
    "text": "3.7 Cellular Automata\n\n\n\nFig. Cellular Automaton\n\n\nCellular automata are simple machines consisting of cells that update in parallel at discrete time steps. In general, the state of a cell depends on the state of its local neighborhood at the previous time step. The earliest known examples were engineered for specific purposes, such as the two-dimensional cellular automaton constructed by von Neumann in 1951 to model biological self-replication (Brummitt and Rowland 2012)\nA cellular automaton is a mathematical and computational model for a dynamic system that evolves in discrete steps. It is suitable for modeling natural systems that can be described as a massive collection of simple objects interacting locally with each other.\nA cellular automaton is a collection of “colored” cells on a grid of specified shape that evolves through a series of discrete time steps according to a set of rules based on the states of neighboring cells. The rules are then applied iteratively over as many time steps as desired.\nCellular automata come in a variety of forms and types. One of the most fundamental properties of a cellular automaton is the type of grid on which it is computed. The simplest such grid is a one-dimensional line. In two dimensions, square, triangular, and hexagonal grids can be considered.\nOne must also specify the number of colors (or distinct states) k that a cellular automaton can assume. This number is typically an integer, with k=2 (binary, [1] or [0]) being the simplest choice. For a binary automaton, color 0 is commonly referred to as “white” and color 1 as “black”. However, cellular automata with a continuous range of possible values can also be considered.\nIn addition to the grid on which a cellular automaton resides and the colors its cells can assume, one must also specify the neighborhood over which the cells influence each other.\n\n3.7.1 One-Dimensional Cellular Automata (1DCA)\nThe simplest option is \"nearest neighbors\", where only the cells directly adjacent to a given cell can influence it at each time step. Two common neighborhoods in the case of a two-dimensional cellular automaton on a square grid are the so-called Moore neighborhood (a square neighborhood) and the von Neumann neighborhood (a diamond-shaped neighborhood).\nCellular automata are considered as a vector. Each component of the vector is called a cell. Each cell is assumed to take only two states:\n\n[0] (white)\n[1] (black)\n\nThis type of automaton is known as an elementary one-dimensional cellular automaton (1DCA). A dynamic process is performed, starting from an initial configuration C(0) of each of the cells (stage 0), and at each new stage, the state of each cell is calculated based on the state of the neighboring cells and the cell itself in the previous stage.\nThe cellular automata that we study in this section are one-dimensional. A one-dimensional cellular automaton consists of:\n\nan alphabet Σ of size k,\na positive integer d,\na function i from the set of integers to Σ, and\na function f from Σd (d-tuples of elements in Σ) to S.\n\nThe function i is called the initial condition, and the function f is called the rule. We think of the initial condition as an infinite row of discrete cells, each assigned one of k colors. To evolve the cellular automaton, we update all cells in parallel, where each cell updates according to f, a function of d cells in its vicinity on the previous step. I adopt the usual convention of naming a cellular automaton’s rule by the number whose base-k digits consist of the outputs of the rule under the kd possible inputs of d cells, ordered reverse-lexicographically. For example, the two-color rule depending on three cells that maps the eight possible inputs according to the rule 00011110 = 30 in this numbering. Here we have identified 0 = [ ] and 1 = [x].\n\n\n3.7.2 The Case of Rule 30\nRule 30 is a binary one-dimensional cellular automaton introduced by Stephen Wolfram (Wolfram 1983). It considers an infinite one-dimensional array of cellular automaton cells with only two states, with each cell in some initial state.\nAt discrete time intervals, each cell changes state spontaneously based on its current state and the state of its two neighbors.\nWhat it consists of:\n\nEach cell can be in one of two states: alive or dead.\nThe next generation of a cell is determined by the current state of the cell and the state of its two neighboring cells.\nThere are 8 possible configurations of neighboring states (3^2), and for each, Rule 30 defines whether the cell lives or dies in the next generation.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is called Rule 30 because in binary, 000111102 = 30\n\n\nFor Rule 30, the set of rules that governs the automaton’s next state is:\n\n\n\n\n\n\n\n\n\nPattern (decimal)\nPattern (binary)\nNext State(center cell)\nNew state(center cell)\n\n\n\n\n0\n000\nDead\n000\n\n\n1\n001\nAlive\n011\n\n\n2\n010\nDead\n000\n\n\n3\n011\nAlive\n011\n\n\n4\n100\nAlive\n011\n\n\n5\n101\nDead\n000\n\n\n6\n110\nAlive\n011\n\n\n7\n111\nDead\n000\n\n\n\nIn the following diagram, the top row shows the state of the central cell (cell i) and its two neighboring cells at a given stage, and the bottom row shows the state of the central cell in the next stage.\nFor example, in the first case of the figure:\n\nif the state of a cell at a given stage is [1] (black) and\nits two neighbors at that stage are also [1] (black),\nthen the cell’s state in the next stage will be [0] (white).\n\n\n\n\nFig. Rule 30\n\n\nLet’s break down the procedure for determining the next state in the 8 combinations:\nInput configurations: Consider the 8 possible input combinations of the three cells (a central cell and its left and right neighbors). Since each cell can be in one of two possible states (0 or 1), the combinations are 000, 001, 010, 011, 100, 101, 110, and 111.\nBinary representation of the rule: Rule 30 is represented by the number 30 in binary, which is 00011110. This binary representation determines the rules for the next state of the central cell for each of the 8 possible combinations.\nBit correspondence: The 8 bits of the binary representation (00011110) correspond to the 8 input combinations in order. From right to left, the bits represent the next state of the central cell for each input combination.\nFor example, the least significant bit of 00011110 (the rightmost bit) is 0. This means that when the input combination is 000, the next state of the central cell will be 0.\nNext state determination: For each input combination (e.g., 000, 001, 010, 011, 100, 101, 110, 111), the corresponding bit in the binary representation of Rule 30 [00011110] indicates the next state of the central cell.\n\n\n\nInput Configuration\nNext State\n\n\n\n\n000\n0\n\n\n001\n1\n\n\n010\n1\n\n\n011\n1\n\n\n100\n1\n\n\n101\n0\n\n\n110\n0\n\n\n111\n0\n\n\n\nFor example, if the input combination is 000 and the corresponding bit in Rule 30 is 0, then the next state of the central cell will be 0.\nTherefore, the rules are not arbitrary but are determined by the binary representation of Rule 30, which specifies the next state of the central cell for each input combination of its neighbors.\n\n\n\nFig. Rule 30\n\n\n\n\n3.7.3 One-Dimensional Cellular Automata Implementation\nhis implementation demonstrates how to construct a one-dimensional cellular automaton system that can generate intricate sequences based on elementary rule sets. The patch showcases advanced concepts including rule-based pattern generation, boundary condition handling, state evolution tracking, and data persistence mechanisms.\n\n\n\nFig.\n\n\n\n\n\n\nIteration\nBinary Output\n\n\n\n\n0\n0 0 0 0 0 0 1 0 0 0 0 0\n\n\n1\n0 0 0 0 0 1 1 1 0 0 0 0\n\n\n2\n0 0 0 0 1 1 0 0 1 0 0 0\n\n\n3\n0 0 0 1 1 0 1 1 1 1 0 0\n\n\n4\n0 0 1 1 0 0 1 0 0 0 1 0\n\n\n5\n0 1 1 0 1 1 1 1 0 1 1 1\n\n\n6\n0 1 0 0 1 0 0 0 0 1 0 0\n\n\n7\n1 1 1 1 1 1 0 0 1 1 1 0\n\n\n8\n1 0 0 0 0 0 1 1 1 0 0 0\n\n\n9\n1 1 0 0 0 1 1 0 0 1 0 1\n\n\n10\n0 0 1 0 1 1 0 1 1 1 0 1\n\n\n11\n1 1 1 0 1 0 0 1 0 0 0 1\n\n\n12\n0 0 0 0 1 1 1 1 1 0 1 1\n\n\n13\n1 0 0 1 1 0 0 0 0 0 1 0\n\n\n14\n1 1 1 1 0 1 0 0 0 1 1 0\n\n\n15\n1 0 0 0 0 1 1 0 1 1 0 0\n\n\n16\n1 1 0 0 1 1 0 0 1 0 1 1\n\n\n17\n0 0 1 1 1 0 1 1 1 0 1 0\n\n\n18\n0 1 1 0 0 0 1 0 0 0 1 1\n\n\n19\n0 1 0 1 0 1 1 1 0 1 1 0\n\n\n20\n1 1 0 1 0 1 0 0 0 1 0 1\n\n\n21\n0 0 0 1 0 1 1 0 1 1 0 1\n\n\n22\n1 0 1 1 0 1 0 0 1 0 0 1\n\n\n23\n0 0 1 0 0 1 1 1 1 1 1 1\n\n\n\n\n\n3.7.3.1 Patch Overview\nThe one-dimensional cellular automaton patch implements a comprehensive system for simulating elementary cellular automata, particularly focusing on Stephen Wolfram’s classification of 256 possible rules. The system operates by maintaining a linear array of cells, each containing a binary state (0 or 1), and applying transformation rules based on local neighborhoods of three consecutive cells. The patch provides sophisticated control over initial conditions (axioms), boundary behaviors, rule selection, and iteration management, making it suitable for both educational exploration and practical generative applications.\nThe architecture follows a modular design pattern with distinct functional subsystems that handle different aspects of the cellular automaton simulation. The rule management subsystem converts numerical rule codes into binary lookup tables that define state transitions for all possible three-cell neighborhoods. The boundary condition handler manages edge cases when the automaton reaches the limits of its defined space, supporting both loop-around and reflection modes. The iteration engine coordinates the step-by-step evolution of the cellular array, maintaining state history and providing real-time feedback on the generation process.\nThe patch interface accommodates various input methods for configuring the automaton parameters, including direct rule code entry (0-255), symbolic mode specification (loop, reflect, or fixed), and custom axiom definition through list messages. The system provides multiple output streams for accessing generated data, including real-time state information, complete iteration histories, and formatted data suitable for external analysis or visualization tools.\n\n\n\n\n\nflowchart TD\n    A[Rule Code Input] --&gt; B[Binary Rule Generation]\n    C[Axiom Definition] --&gt; D[Initial State Setup]\n    E[Mode Selection] --&gt; F[Boundary Configuration]\n    B --&gt; G[Rule Lookup Table]\n    D --&gt; H[Current State Array]\n    F --&gt; I[Edge Handler]\n    J[Iterate Bang] --&gt; K[State Evolution Engine]\n    H --&gt; K\n    G --&gt; K\n    I --&gt; K\n    K --&gt; L[Next Generation]\n    L --&gt; M[State History]\n    L --&gt; N[Data Output]\n    L --&gt; H\n\n\n\n\n\n\n\n\n3.7.3.2 Data Flow\nThe cellular automaton’s operation begins with initialization phase where the rule code, boundary mode, and initial axiom are processed and stored in the system’s internal data structures. The rule code undergoes binary conversion through a specialized subpatch that transforms decimal values (0-255) into their eight-bit binary representations, which correspond to the output states for all possible three-cell neighborhood configurations (000, 001, 010, 011, 100, 101, 110, 111).\nThe axiom processing system accepts list-formatted initial conditions that define the starting configuration of the cellular array. This initial state is stored in a text object that serves as both the current generation reference and the foundation for subsequent iterations. The boundary mode configuration determines how the system handles edge conditions when calculating neighborhoods for cells at the array boundaries, with options for periodic (loop), reflective, or fixed boundary conditions.\nWhen an iteration is triggered through the iterate bang, the system enters its core evolution cycle. The current state array is processed through a neighborhood analysis routine that examines each cell in conjunction with its immediate neighbors. For each three-cell neighborhood, the system constructs a lookup key by treating the binary states as a three-bit number, which is then used to index into the previously generated rule table to determine the next state for the center cell.\nThe boundary condition handler plays a crucial role during this process, providing appropriate neighbor values for cells at the array edges based on the selected boundary mode. In loop mode, the array is treated as circular, with the leftmost and rightmost cells considered adjacent. Reflect mode mirrors the edge values inward, while fixed mode uses predetermined constant values for out-of-bounds references.\nThe resulting next generation is simultaneously output for immediate access and stored in the iteration history system. This dual-path approach allows for real-time monitoring of the automaton’s evolution while maintaining a complete record of all generated states. The history management system can accommodate both memory-efficient storage (retaining only the current state) and comprehensive archival (preserving all iterations) depending on the intended application.\n\n\n3.7.3.3 Key Objects and Their Roles\nThe following table summarizes the essential objects and subsystems used in the cellular automaton implementation:\n\n\n\n\n\n\n\n\nObject/Subsystem\nPurpose\n\n\n\n\nmake.code subpatch\nConverts rule codes to binary lookup tables and manages initialization\n\n\nfloat.to.binary subpatch\nTransforms decimal rule codes into 8-bit binary representations\n\n\ntext define $0-rules\nStores rule lookup table and configuration parameters\n\n\ntext define $0-outcome\nMaintains current generation state and iteration history\n\n\nlist-drip\nProcesses cell arrays sequentially for neighborhood analysis\n\n\nlist split 3\nExtracts three-cell neighborhoods from the current state\n\n\ntext search\nPerforms rule lookups based on neighborhood patterns\n\n\ntext get\nRetrieves rule outcomes and state information\n\n\nboundary.mode subpatch\nHandles edge conditions and neighbor calculation\n\n\nlist append/prepend\nManages array concatenation and state assembly\n\n\nspigot\nControls data flow during iteration cycles\n\n\nroute\nDirects messages to appropriate processing subsystems\n\n\nv $0-iteration.n\nTracks current iteration number and generation count\n\n\nsave subpatch\nProvides data export functionality for generated sequences\n\n\n\n\nThe patch demonstrates several advanced programming techniques, including dynamic text object manipulation for rule storage, sophisticated list processing for array operations, and modular subpatch design for complex algorithmic implementations. The use of local variables ($0- prefix) ensures proper encapsulation when multiple instances of the patch are used simultaneously, while the declare objects manage external file dependencies for rule and outcome storage.\nThe cellular automaton implementation showcases how it can be used to create sophisticated algorithmic systems that bridge computational theory and practical creative applications. The modular architecture allows for easy experimentation with different rule sets, boundary conditions, and initial configurations, making it an excellent tool for exploring the rich behavioral space of elementary cellular automata and their potential applications in generative music, visual art, and algorithmic composition.\n\n\n\n3.7.4 Two-Dimensional Cellular Automata (2DCA)\nTwo-dimensional cellular automata are an extension of one-dimensional cellular automata, where cells not only have neighbors to the left and right, but also above and below. This allows modeling of more complex and structurally rich systems, such as two-dimensional phenomena like wave propagation, growth patterns in biology, fire spread, fluid simulation, among others.\nTwo-dimensional cellular automata (2DCA) are computational models that simulate dynamic systems on a two-dimensional grid.\nThey consist of:\n\nCells: Each cell in the grid can have a finite state, such as alive or dead, and can change state according to the automaton’s rules.\nNeighborhood: Each cell has a neighborhood, which is a set of adjacent cells that influence its state. The neighborhood can be rectangular, hexagonal, circular, or of any other shape.\nTransition rule: The transition rule defines how the state of a cell changes based on its current state and the state of the cells in its neighborhood. The rule can be deterministic or probabilistic.\nEvolution: The cellular automaton evolves through iterations. In each iteration, the state of each cell is updated according to the transition rule.\n\n\n\n3.7.5 Case Study: Conway’s Game of Life\n\n\n\nGame of life Source\n\n\nThe most well known cellular automaton is the Game of Life, a two-dimensional cellular automaton which has been invented by John Horton Conway in 1970. It is a simulation that describes the evolution of a population of cells across a two-dimensional grid of squares by three (or four, depending on the wording) simple rules. Every cell has one of the two states ”dead” and ”alive”, and the state changes of the cells depend on the number of living neighbor cells. When it was first published by Martin Gardner in the journal Scientific American in 1970, it was praised to have ”fantastic combinations” (Gardner 1970). Since its invention, thousands of patterns have been found by many people, and there seems to be no end in sight. Even after more than 50 years of history, interesting new patterns are still discovered.\nThe “game” is actually a zero-player game, meaning its evolution is determined by its initial state, requiring no further input from human players. One interacts with the Game of Life by creating an initial configuration and observing how it evolves.\nView Original Article - MATHEMATICAL GAMES - The fantastic combinations of John Conway’s new solitaire game life (Martin Gardner)\n\n\n\n\n\n\nNote\n\n\n\nGOL (Game of Life) and CGOL (Conway’s Game of Life) are commonly used acronyms.\n\n\n\n3.7.5.1 Rules\nThe universe of the Game of Life is an infinite two-dimensional orthogonal grid of square cells, each of which (at any given time) is in one of two possible states, alive (alternatively “on”) or dead (alternatively “off”). At each time step, the following transitions occur:\nThe four rules of Conway’s Game of Life:\n1. Overpopulation: Any live cell with more than three live neighbors dies due to overpopulation.\n2. Underpopulation: Any live cell with fewer than two live neighbors dies due to underpopulation.\n3. Stability: Any live cell with two or three live neighbors survives to the next generation.\n4. Reproduction: Any dead cell with exactly three live neighbors becomes a live cell.\n \nWe can also summarize the rules in a table:\n\n\n\n\n\n\n\n\n\n\nRule\nDescription\nCurrent State\nLive Neighbors\nNext State\n\n\n\n\nOverpopulation\nDeath by overcrowding\nAlive\nMore than 3\nDead\n\n\nUnderpopulation\nDeath by isolation\nAlive\nFewer than 2\nDead\n\n\nStability\nSurvival\nAlive\n2 or 3\nAlive\n\n\nReproduction\nBirth\nDead\n3\nAlive\n\n\n\n \nOr summarized as:\n0 → 3 live neighbors → 1 1 → &lt; 2 or &gt; 3 live neighbors → 0\nWhere 0 represents a dead cell and 1 a live cell.\n\n\n\nGame of life rules Source\n\n\nJohn Conway’s Game of Life is made of an infinite grid of square cells, each of which is alive or dead. The rules below govern the evolution of the system. Every cell has eight neighbors, and all cells update simultaneously as time advances.\n\n\n\nFig. Game of life rules Source\n\n\nThe initial pattern constitutes the system’s ‘seed’. The first generation is created by applying the above rules simultaneously to each cell in the seed; births and deaths occur simultaneously, and the discrete moment in which this occurs is sometimes called a step. (In other words, each generation is a pure function of the previous one.) The rules continue to be applied repeatedly to create more generations.\n\n\n\n3.7.6 Origins\nConway was interested in a problem presented in the 1940s by renowned mathematician John von Neumann, who was trying to find a hypothetical machine that could build copies of itself and succeeded when he found a mathematical model for such a machine with very complicated rules on a rectangular grid. The Game of Life emerged as Conway’s successful attempt to simplify von Neumann’s ideas.\nThe game made its first public appearance in the October 1970 issue of Scientific American, in Martin Gardner’s “Mathematical Games” column, under the title “The fantastic combinations of John Conway’s new solitaire game ‘Life’”.\nSince its publication, Conway’s Game of Life has attracted significant interest due to the surprising ways patterns can evolve.\nLife is an example of emergence and self-organization. It is of interest to physicists, biologists, economists, mathematicians, philosophers, generative scientists, and others, as it shows how complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counterintuitive notion that “design” and “organization” can spontaneously arise in the absence of a designer.\nConway carefully selected the rules, after considerable experimentation, to meet three criteria:\n\nThere should be no initial pattern for which a simple proof exists that the population can grow without limit.\nThere must be initial patterns that appear to grow indefinitely.\nThere should be simple initial patterns that evolve and change for a considerable period before ending in one of the following ways:\n\ndying out completely (due to overcrowding or becoming too sparse); or\nsettling into a stable configuration that remains unchanged thereafter, or entering an oscillating phase in which they repeat a cycle endlessly of two or more periods.\n\n\n\n\n3.7.7 Patterns\nMany different types of patterns occur in the Game of Life, including static patterns (“still lifes”), repeating patterns (“oscillators” – a superset of still lifes), and patterns that move across the board (“spaceships”). Common examples of these three classes are shown below, with live cells in black and dead cells in white.\n\n\n\nGosper glider gun\n\n\nUsing the provided rules, you can investigate the evolution of simple patterns:\n\n\n\n3 cells Source\n\n\n\n\n\n4 cells Source\n\n\nPatterns that evolve for long periods before stabilizing are called Methuselahs, the first of which discovered was the R-pentomino.\n\n\n\nR-pendomino source\n\n\nDiehard is a pattern that eventually disappears, rather than stabilizing, after 130 generations, which is believed to be the maximum for initial patterns with seven or fewer cells.\n\n\n\nDiehard source\n\n\nAcorn takes 5,206 generations to produce 633 cells, including 13 escaping gliders.\n\n\n\nAcorn source\n\n\nConway originally conjectured that no pattern could grow indefinitely; that is, for any initial configuration with a finite number of live cells, the population could not grow beyond some finite upper bound. The Gosper glider gun pattern produces its first glider in the 15th generation, and another every 30 generations thereafter.\n\n\n\nGosper’s glider gun source\n\n\n\n\n\nGosper glider gun\n\n\nFor many years, this pattern was the smallest known. In 2015, a gun called Simkin glider gun was discovered, which emits a glider every 120 generations, and has fewer live cells but is spread across a larger bounding box at its ends.\n\n\n\nSimkin glider gun\n\n\nFor a more detailed overview of gliders and other patterns, you can refer and extensive post by Swaraj Kalbande Conway’s Game of Life-Life on Computer (Kalbande 2022).\n\n\n3.7.8 Python Implementation of Game of Life\nimport time\nimport pygame\nimport numpy as np\n\nCOLOR_BG = (10, 10, 10,)  # Color de fondo\nCOLOR_GRID = (40, 40, 40)  # Color de la cuadrícula\nCOLOR_DIE_NEXT = (170, 170, 170)  # Color de las células que mueren en la siguiente generación\nCOLOR_ALIVE_NEXT = (255, 255, 255)  # Color de las células que siguen vivas en la siguiente generación\n\npygame.init()\npygame.display.set_caption(\"conway's game of life\")  # Título de la ventana del juego\n\n# Función para actualizar la pantalla con las células\ndef update(screen, cells, size, with_progress=False):\n    updated_cells = np.zeros((cells.shape[0], cells.shape[1]))  # Matriz para almacenar las células actualizadas\n\n    for row, col in np.ndindex(cells.shape):\n        alive = np.sum(cells[row-1:row+2, col-1:col+2]) - cells[row, col]  # Cálculo de células vecinas vivas\n        color = COLOR_BG if cells[row, col] == 0 else COLOR_ALIVE_NEXT  # Color de la célula actual\n\n        if cells[row, col] == 1:  # Si la célula actual está viva\n            if alive &lt; 2 or alive &gt; 3:  # Si tiene menos de 2 o más de 3 vecinos vivos, muere\n                if with_progress:\n                    color = COLOR_DIE_NEXT\n            elif 2 &lt;= alive &lt;= 3:  # Si tiene 2 o 3 vecinos vivos, sigue viva\n                updated_cells[row, col] = 1\n                if with_progress:\n                    color = COLOR_ALIVE_NEXT\n        else:  # Si la célula actual está muerta\n            if alive == 3:  # Si tiene exactamente 3 vecinos vivos, revive\n                updated_cells[row, col] = 1\n                if with_progress:\n                    color = COLOR_ALIVE_NEXT\n\n        pygame.draw.rect(screen, color, (col * size, row * size, size - 1, size - 1))  # Dibuja la célula en la pantalla\n\n    return updated_cells  # Devuelve las células actualizadas\n\n# Función principal del programa\ndef main():\n    pygame.init()\n    screen = pygame.display.set_mode((800, 600))  # Crea la ventana del juego\n\n    cells = np.zeros((60, 80))  # Crea una matriz de células muertas\n    screen.fill(COLOR_GRID)  # Rellena la pantalla con el color de la cuadrícula\n    update(screen, cells, 10)  # Actualiza la pantalla con las células\n\n    pygame.display.flip()\n    pygame.display.update()\n\n    running = False  # Variable para controlar si el juego está en ejecución\n\n    while True:\n        for Q in pygame.event.get():\n            if Q.type == pygame.QUIT:  # Si se cierra la ventana, termina el programa\n                pygame.quit()\n                return\n            elif Q.type == pygame.KEYDOWN:\n                if Q.key == pygame.K_SPACE:  # Si se presiona la tecla espacio, se inicia o pausa el juego\n                    running = not running\n                    update(screen, cells, 10)\n                    pygame.display.update()\n            if pygame.mouse.get_pressed()[0]:  # Si se presiona el botón izquierdo del ratón\n                pos = pygame.mouse.get_pos()  # Obtiene la posición del ratón\n                cells[pos[1] // 10, pos[0] // 10] = 1  # Marca la célula correspondiente como viva\n                update(screen, cells, 10)\n                pygame.display.update()\n\n        screen.fill(COLOR_GRID)  # Rellena la pantalla con el color de la cuadrícula\n\n        if running:  # Si el juego está en ejecución\n            cells = update(screen, cells, 10, with_progress=True)  # Actualiza las células con progreso\n            pygame.display.update()\n\n        time.sleep(0.001)  # Espera un breve tiempo para controlar la velocidad del juego\n\nif __name__ == \"__main__\":\n    main()\n\nThis Python script implements Conway’s Game of Life, a cellular automaton devised by British mathematician John Horton Conway. It is a zero-player game, meaning its evolution is determined entirely by its initial state, with no further input required.\nThe script begins by importing the necessary modules: time, pygame for the graphical interface, and numpy to handle the game grid as a 2D matrix. It then defines color constants used for visualizing the game.\nThe pygame.init() function is called to initialize all imported Pygame modules. The window title is set to “Conway’s Game of Life” using pygame.display.set_caption().\nThe update() function updates the game state and redraws the grid. It takes four arguments: screen (the Pygame surface to draw on), cells (the current game state as a 2D numpy array), size (the pixel size of each cell), and with_progress (a boolean indicating whether to display cells that will change in the next generation).\nThe function creates a new 2D matrix updated_cells filled with zeros, matching the shape of cells. It then iterates over each cell, calculates the number of live neighbors, and applies the game rules to determine whether the cell will be alive in the next generation. The function draws each cell on the screen using the appropriate color and returns updated_cells.\nThe main() function initializes Pygame, creates the game window, and initializes the game state as a 2D numpy array of zeros (representing dead cells). It then enters the main loop, which handles Pygame events (such as closing the window or key presses), updates the game state if it is running, and redraws the grid. The game can be started or paused by pressing the spacebar, and cells can be toggled manually by clicking on them.\nFinally, the script calls main() to launch the game. Press the spacebar to begin.\n\n\n3.7.9 Further Reading\n\nThe Game Of Life – Emergence In Generative Art (2020)\nConway’s Game of Life – Life on Computer by Swaraj Kalbande\nWikipedia – Conway’s Game of Life\n\n\n\n3.7.10 Interactive Websites\n\nJohn Conway’s Game of Life – An Introduction to Cellular Automata\nconwaylife.com\nCellular Automata Megathread",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/sequencer.html#references-.",
    "href": "chapters/sequencer.html#references-.",
    "title": "3  Sequencer",
    "section": "References .",
    "text": "References .\n\n\n\n\nBrown, Andrew. 2023. “Live Coding Patterns and a Toolkit for Pure Data.” Organised Sound 28: 1–12. https://doi.org/10.1017/S1355771823000365.\n\n\nBrummitt, Charles D., and Eric Rowland. 2012. “Boundary Growth in One-Dimensional Cellular Automata.” Complex Systems 21: 85–116. https://doi.org/10.48550/arXiv.1204.2172.\n\n\nEpstein, Paul. 1986. “Pattern Structure and Process in Steve Reich’s Piano Phase.” The Musical Quarterly 72 (4): 494–502.\n\n\nGardner, Martin. 1970. “MATHEMATICAL GAMES - the Fantastic Combinations of John Conway’s New Solitaire Game Life.” Scientific American 223 (4): 120–23. http://www.jstor.org/stable/24927642.\n\n\nKalbande, Swaraj. 2022. “Conway’s Game of Life – Life on Computer.” Medium. https://medium.com/@swarajkalbande123/conways-game-of-life-life-on-computer-b7edfc85d21a.\n\n\nReich, Steve. 2002. “Music as a Gradual Process.” In Writings on Music 1965–2000, 34–36. Oxford; New York: Oxford University Press.\n\n\nToussaint, Godfried. 2005. “The Euclidean Algorithm Generates Traditional Musical Rhythms.” In, 47–56.\n\n\nWolfram, Stephen. 1983. “Statistical Mechanics of Cellular Automata.” Reviews of Modern Physics 55 (3): 601–44. https://doi.org/10.1103/RevModPhys.55.601.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencer</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html",
    "href": "chapters/synthesis.html",
    "title": "4  Synthesis",
    "section": "",
    "text": "4.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#oscillatory-movements",
    "href": "chapters/synthesis.html#oscillatory-movements",
    "title": "4  Synthesis",
    "section": "4.2 Oscillatory Movements",
    "text": "4.2 Oscillatory Movements\nOscillatory motion will be analyzed in detail—not only in terms of technical implementation but also through its creative and conceptual applications. Using example patches provided in both Pure Data and VCV Rack, we will explore how oscillation becomes an expressive tool in interactive works and live sound experiments. This approach will form the core of the module’s practical component, where students are encouraged to adapt the exercises to their own interests: whether by replicating existing works, developing new compositions, or analyzing the results of comparative experiments between different oscillators and their spectral behaviors.\n\n4.2.1 Between Technique and Aesthetics\nThis chapter begins with the idea of delving deeper into oscillatory movements from both a technical and aesthetic perspective. Building upon tools introduced in previous chapters we will work with oscillators and explore their many applications. Drawing on the text The Poetics of Signal Processing by Jonathan Sterne and Tara Rodgers, we will approach signal processing not merely as a set of mathematical operations, but as a form of artistic expression. In this context, signal manipulation becomes more than just a means to produce sound—it becomes a way to articulate broader cultural ideas about sound in the digital age.\nWe will explore what could be described as the “rawness” of analog oscillators—that tangible, organic quality inherent in their operation. We will examine how this rawness both contrasts with and complements the digital techniques we’ve developed so far. The use of oscillators in their various forms will serve as a starting point for a series of hands-on exercises, where theory and practice are deeply intertwined.\nThrough this combination of theory, practice, and critical reflection on signal processing, the module aims to open up new sonic possibilities. The references provided are intended not only to contextualize student work, but also to inspire expanded thinking about how these principles might inform personal projects.\nThis practical and flexible framework is reflected in the module’s initial activity on oscillatory movement. Students will have the freedom to choose their working environment and the context in which they wish to develop their experiments. Oscillator implementation can be approached from multiple angles: waveform manipulation, waveshaper design, or even the creation of interactive sound pieces. Each student will be invited to present a short description of their process, along with their code or patch, in order to foster dialogue between theory and practice.\nIn this way, the module not only deepens our understanding of the technical aspects of signal processing, but also invites critical reflection on the role of these technologies in shaping meaning and sonic aesthetics. Special emphasis is placed on the oscillator’s capacity to act as a driver of creativity and expression.\n\n\n4.2.2 Poetics of Signal Processing\nOne of the most important aspects of this chapter is the idea of “poetics” in signal processing. This concept is not just about the technical aspects of sound manipulation, but also about the cultural and artistic implications of these processes. The term “poetics” suggests a deeper engagement with the materiality of sound and its representation in various contexts. I’m going to talk a bit about my reflections and observations on the text Poetics of Signal Processing (Sterne and Rodgers 2011). First, I want to mention the authors, who I find very interesting. The first is Jonathan Sterne, a professor and director of the Culture and Technology program at McGill University in Canada. His work focuses on the cultural dimension of communication technologies, and he specializes in the history and theory of sound in the modern Western world. Then we have another author, a sound artist and musician named Tara Rodgers. She’s an electronic music composer, programmer, and historian of electronic music. She holds a PhD in Communication and has worked in the field of Women’s Studies. She created the platform Pink Noises, which is worth checking out—especially because it gathers a series of interviews with improvisers, composers, and instrument builders. There’s a cross-section of gender, sexuality, feminism, music, sound studies, theater, performance, and performative arts in general.\nLet’s get into some comments on specific sections of the article. The first is “The Sonic Turn,” which describes the emergence of a new culture of listening beginning in the second half of the 20th century. This is discussed through the work of authors like Cox and Kahn. There’s also a growing interest in oral history and anthropology among social scientists, and the emergence of sound art within the art world during the 20th century.Another key point is the growing interest in listening itself, and in the creative possibilities enabled by recording, reproduction, and other forms of sound transmission. This leads us to ask: where do today’s sound technologies come from? What are we going to address in this course?\nWe can start by discussing the “audible past,” or what the article refers to as the “auditory past.” The text mentions that between 1900 and 1925, sound becomes an object of thought and practice. Before this period, sound was thought of in more idealized terms—mainly through the lens of voice and music, along with all the structures they imply. During this period, there were significant socioeconomic and cultural shifts—capitalism, rationalism, science, and colonialism—that influenced ideas and practices related to sound and listening. These changes were not only cultural but social as well. In this “audible past,” even the most basic mechanical elements of sound reproduction technologies were shaped by how they had been used up until then. In this way, sound technologies are tied to habits—they sometimes enable new habits, like new ways of listening, or sometimes they solidify and reinforce existing ones.\nSo let’s reflect on the “poetics” of signal processing. At first, that might sound surprising—poetics? But we can start with a very basic unit: the signal. Signals have a certain materiality. Sound has materiality—it occupies space in a transmission, recording, or playback channel. It exists in a medium and can be manipulated in various ways. There’s a clear distinction between analog electrical signals, electronic signals, and digital ones. Each implies different content and meanings. So, signal processing occurs in the medium of sound transmission, but in a technologized era—that is, the present. This involves manipulating sound in what we might call a “translucent state.” Here, the transducer becomes central: we’ll talk a lot in the course about this idea of converting one form of energy—measurable by a certain magnitude—into another. This concerns almost everything in sound or image that reaches our senses through electronic media. This also exists in the domains of the musician, the playback device, the listener, and the interstices between them.\nRegarding the poetics of signal processing as signal [music plays briefly], the article refers mostly to the figurative dimensions of the process itself. These are the ways that processing is represented in the discourse of audio technology, particularly from a technical or engineering perspective. Signal processing carries cultural meanings—it’s not an isolated technical fact. It has cultural significance. Two metaphorical frameworks commonly used in everyday language among users and creators are discussed in the article: cooking and travel. These are two metaphors I find fascinating.\nLet’s begin with cooking—specifically, “the raw and the cooked.” This draws from the work of anthropologist Claude (Lévi-Strauss 1964), who analyzed the raw, the cooked, and the rotten. The axis of raw/cooked belongs to culture, while fresh/rotten relates more to nature. This is a very important reference: cooking is a cultural operation. Fire—the act of cooking—is the basis of a social order, of stability.\nIn this sense, when we talk about the raw and the cooked in relation to sound, rawness doesn’t mean purity. It’s a relative condition—it refers to the availability of audio for further processing. This is very useful when thinking in opposition to the Hi-Fi culture. We might even think of a scale of rawness, or various degrees of it, which relate to how sound can be manipulated and used.\nWithin this raw/cooked metaphor, terms like slicing (cutting into slices) and dicing (cutting into cubes) appear—these are actual signal processing terms and very fitting metaphors.\nThinking further with this metaphor: raw audio might be seen as passive, something that must be “cooked” through technological processes. This reveals the technologized nature of music technologies, where composition becomes a kind of masculine performance of technological mastery. And I want to stress this point again: composition is often framed as a male-dominated act of technical skill.\nPaul Théberge (Théberge 1997) analyzes musicians as consumers within the sound tech industry. In one chapter of his book, he studies advertising in music tech magazines, showing how the marketing of music technologies has been directed predominantly at men. Fortunately, this is changing slowly. In the raw/cooked metaphor, the idea of sound as a material to be processed and preserved for future use emerges in the late 19th century—just like technologies developed to preserve and can food. It’s a very strong metaphor: processed food and processed sound were both invented to extend and control organic life through technological preservation.\nNow let’s move to the other metaphor—travel. Signal processing can be thought of as a journey, which I find very exciting. We can connect this to topology—a mathematical field that studies spatial relationships. The term topos refers to place. In this sense, we can think of electronics as the arrangement and interaction of various components. A synthesizer circuit, or an oscillator, can be conceived as a space in itself—a map. Early texts in electroacoustics from the late 19th and early 20th centuries started to describe sound and electricity as fluid media. They used water metaphors—talking about waves, oscillations, flow, and current. This “processing as travel” metaphor involves the idea of particles moving through space, with the destination originally being the human ear. Today, that destination could be a transducer—or a computer.\nEven the inner ear was once conceptualized as a terrain made up of interconnected parts through which vibrations would travel. These metaphors matter: sea voyages during the historical periods mentioned in the article also symbolized scientific exploration and the conquest of the unknown. One example I love is that in the 1800s, Lord Kelvin created what could be considered the first synthesizer—but it didn’t produce sound. It was a mechanical device designed to predict tides. I’ll share some images to illustrate this. It essentially summed simple waves into one more complex waveform.\nSo, to conclude this idea of processing as travel, the text also reflects on how maritime metaphors privilege a particular kind of subject—a white, Western male as the ideal “navigator” of synthetic sound waves. This is a clearly colonial and masculinist rhetoric. Generating and controlling electronic sounds becomes associated with a kind of pleasure aligned with capitalism, and also with danger—of disobedient or unruly sounds.\nSwitching to a less metaphorical aspect, the text discusses Helmholtz’s On the Sensations of Tone (Helmholtz 1954). It laid the epistemological foundations for synthesis techniques. Helmholtz argued that any sound could be broken down into volume, pitch, and timbre. For him, sound was a material with clearly defined properties. These properties could be analyzed and then mimicked using synthesis techniques. However, other researchers, like Jessica Roland, explored different approaches. She compared sound to things like rain and wind. Her approach emphasized experience, memory, and the use of synthesis as a kind of onomatopoeia—imitation of natural phenomena. For Roland, unpredictability and chaos are at the heart of synthesis.\nIn conclusion, one of the key points of the article is that metaphors in audio-technical discourse—supposedly neutral or instrumental—are actually shaped by the cultural positions of specific subjects living in specific societies. They are deeply entangled with issues of gender, race, class, and culture. The language of technical culture is highly metaphorical and filled with implicit assumptions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#sound-sources",
    "href": "chapters/synthesis.html#sound-sources",
    "title": "4  Synthesis",
    "section": "4.3 Sound Sources",
    "text": "4.3 Sound Sources\nOscillators are a fundamental component of sound synthesis and play a crucial role in the creation of electronic music. They generate periodic waveforms, which can be manipulated to produce a wide range of sounds. In this section, we will explore the different types of oscillators, their characteristics, and how they can be used creatively in sound design.\n\n4.3.1 Oscillators\nSound sources in synthesizers are largely based on mathematics. There are two fundamental types: waveforms and random signals. Waveforms are typically described as simple geometric shapes—sawtooth, square, pulse, sine, and triangle being the most common. These shapes are mathematically straightforward and electronically feasible to generate. On the other hand, random waveforms produce noise, a constantly shifting mixture of all frequencies.\n\n\n\nFig. Sine Oscillator & White Noise Generator\n\n\nOscillators are one of the core building blocks of synthesizers, often implemented as function generators. A function generator produces a waveform that may be continuous or triggered and can take arbitrary shapes. In a basic analog subtractive synthesizer, an oscillator usually outputs a few continuous waveforms, with frequency controlled by voltage. Since these sources typically output a continuous signal, modifiers must be applied to shape timbre or envelope the sound.\n\n\n4.3.2 Sine & Cosine\nA “pure tone” consists of a single frequency and is produced by a sine wave oscillator, which can be implemented using either the sine or cosine function. These functions take an angle value, or “phase,” as input. Below, we see the angle “alpha,” the sine’s amplitude value, and the cosine’s output denoted as x.\n\n\n\nFig.\n\n\nIn the resulting graph, amplitude is on the vertical axis and angle on the horizontal axis. The cosine output traces the same function as the sine but starts at 1, meaning it has a different initial phase. Sine and cosine are essentially the same waveform offset by 90 degrees of phase.\n\n\n\nFig.\n\n\nPure Data’s native [sin] and [cos] objects take angle values in radians (0 to 2π). However, the audio object [cos~] uses a linear range from 0 to 1 to represent a full cycle. The ELSE library provides the [pi] object, which outputs the constant π. This can be stored in a [value] object and accessed within [expr]. To convert a linear 0–1 range into radians, multiply it by 2 * pi. Then, [cos] and [sin] yield amplitude values accordingly.\n\n\n\n\nFig.\n\n\n\n\n\n4.3.3 Phasor\nIn the following example, we implement a sine oscillator using the [sin~] object and the native [phasor~] object.\n\n\n\nFig.\n\n\nTwo graphs illustrate this: in the top one, the horizontal axis is time, and the vertical axis shows a steadily increasing phase, forming a linear ramp. In the bottom graph, this ramp is transformed into a sine waveform, with amplitude on the vertical axis.\nThe [phasor~] object outputs a linear ramp from 0 to just under 1, representing a complete cycle. It is ideal for driving objects like [cos~] and [sin~], which expect a 0–1 input representing phase progression.\nThe input to [phasor~] is frequency, expressed in cycles per second (hertz). This defines how many full 0–1 cycles occur per second.\nNote that [phasor~] never actually reaches 1—it wraps around to 0. Due to its cyclic nature, 1 is functionally equivalent to 0, just like 360° equals 0° in circular geometry.\nThe output of [phasor~] can be described as a “running phase.” It defines the angular increment applied to the phase at every audio sample.\n\n\n\n\nFig.\n\n\n\n\n\n4.3.4 Oscillator\nIn the analog domain, oscillators are commonly referred to as VCOs (Voltage Controlled Oscillators). VCOs allow frequency or pitch to be controlled via voltage. Some VCOs also feature voltage control inputs for modulation (typically FM) and for altering the waveform shape—usually the pulse width of square waves, although some VCOs allow shaping other waveforms as well.\nMany VCOs include an additional input for synchronization with another VCO’s signal. Phase sync forces the VCO to reset its phase in sync with the incoming signal, limiting operation to harmonics of the input frequency. This results in a harsh, buzzy tone. Softer sync techniques can yield timbral variations rather than locking to an exact frequency.\nA typical VCO offers controls for coarse and fine tuning, waveform selection (often sine, triangle, square, sawtooth, and pulse), pulse width modulation (PWM), and output level. Some VCOs also provide multiple simultaneous waveform outputs and sub-octave outputs one or two octaves below the main signal. Pulse width modulation (PWM) allows dynamic alteration of the pulse waveform shape.\nTo summarize, an oscillator is typically defined by:\n\nWaveform function (sine, sawtooth, square, triangle)\nFrequency (Hz)\nInitial phase (degrees)\nPeak amplitude (optional)\n\nHow do we control these parameters in our model?\n\n\n\nFig.\n\n\n\n\n4.3.5 VCO\nIn this example, [phasor~] and [cos~] form an oscillator. The [cos~] object outputs amplitude values from -1 to 1, yielding a maximum amplitude of 1 without additional gain control.\n\n\n\nFig.\n\n\nThe waveform produced is a cosine. While [phasor~] sets the frequency, it can also define the initial phase. Since sine and cosine are essentially phase-shifted versions of the same function, we can easily produce sine waves as well. However, the initial phase does not affect the perceived pitch of a pure tone.\nTry this patch with different phase offsets. Note that [phasor~] also accepts negative frequencies, reversing the phase direction.\nConnecting [phasor~] to [cos~] replicates the functionality of the [osc~] object.\n\n\n\nFig.\n\n\n\n\n4.3.6 Waveforms\nThe sine wave is the simplest oscillator, generating a pure tone. Other basic and musically useful waveforms include triangle, sawtooth, and square.\nThe sine wave is a smooth, rounded waveform based on the sine function. It contains only one harmonic—the fundamental—which makes it less suitable for subtractive synthesis as it lacks overtones to filter.\nA triangle wave consists of two linear slopes. It contains small amounts of odd harmonics, providing just enough spectral content for filtering.\nA square wave contains only odd harmonics and produces a hollow, synthetic sound. A sawtooth wave contains both odd and even harmonics and sounds bright. Some pulse waves may contain even more harmonic content than basic sawtooth waves. Variants like “super-saw” replace linear slopes with exponential ones and alternate teeth with gaps, producing an even richer harmonic spectrum.\n\n\n4.3.7 Waveshapers\nThis section focuses on creating oscillators in Pure Data (Pd) using the [phasor~] object. The only true oscillator in Pd Vanilla is [osc~], a sine wave oscillator. Even standard waveforms must be built manually.\nAs mentioned earlier, [osc~] is essentially [phasor~] connected to [cos~]. The [phasor~] object outputs a 0–1 ramp, functionally similar to a sawtooth wave with half the amplitude and an offset. [cos~] multiplies this ramp by 2π and computes its cosine. The result is a sine wave oscillator.\n\n\n\nFig.\n\n\nAnother way to construct this oscillator is by using [expr~] and [value]. Here we use the [pi] abstraction to calculate π (or approximate it by sending 1 to [atan] and multiplying the result by 4). We then multiply by 2 and store it in a [value] object. [value] acts like a global variable: any object using the same name accesses the same value. [expr~] can then use this to calculate the cosine, just like [cos~]. While this approach may be more CPU-intensive, it helps deepen understanding of oscillator construction in Pd.\n\n\n\nFig.\n\n\n\n4.3.7.1 Sawtooth Oscillator\nSince [phasor~] already produces a ramp, generating a sawtooth wave is straightforward. Simply multiply [phasor~] by 2 to get the correct amplitude, then subtract 1 to shift the range to -1 to 1.\n\n\n\nFig.\n\n\n\n\n4.3.7.2 Square Wave Oscillator\nTo create a square wave, you can use [expr~] (included in Pd Vanilla), but [&gt;~] is faster and more CPU-efficient. A square wave toggles between -1 and 1. The [&gt;~] object compares its left input signal to a threshold (right input or argument). It outputs 1 if the input is greater than the threshold, and 0 otherwise. Using 0.5 as the threshold with a [phasor~] input yields a square wave: 0 for half the cycle, 1 for the other half.\n\n\n\nFig.\n\n\n\n\n4.3.7.3 Triangle Wave Oscillator\nAmong standard waveforms, the triangle wave is the most complex to construct. Starting with [phasor~], which is an upward ramp from 0 to 1, we create an inverted version by multiplying it by -1 and then adding 1. This gives us a descending ramp from 1 to 0.\nNow we have both ascending and descending ramps. Sending both to [min~] (which outputs the smaller of two values) gives us a triangle waveform spanning 0 to 0.5. [min~] effectively splices the ascending and descending ramps to form a symmetric triangle wave.\n\n\n\nFig.\n\n\n\n\n\n4.3.8 Frequency\nFrequency is presented here in terms of angular velocity! One common unit of measurement is the hertz (Hz), which equals “cycles per second.” Frequency also determines a period of oscillation, which is simply the inverse of frequency. For example, a frequency of 100 Hz corresponds to a period of 0.01 seconds (or 10 milliseconds):\nPeriod = 1 / Frequency\nPeriod = 1 / 100 Hz = 0.01 s = 10 ms\n\n\n\nFig.\n\n\nWe can convert between Hz and milliseconds using this same relationship. Another way to express period is in number of samples, which requires the sampling rate to perform the conversion:\n\n\n\nFig.\n\n\nAngular velocity units require both an angle and a time unit. One cycle per second defines a full cycle (360 degrees) as the angular unit, and seconds as the time unit. Other units are also possible. For example, the angle may be expressed in radians and the time unit as a single sample, yielding a unit of “radians per sample.”\nTo convert Hz to radians per sample, multiply by 2π and divide by the sampling rate. See below for this conversion and the [hz2rad] and [rad2hz] objects from the ELSE library that handle it.\n\n\n4.3.9 Phase\nThe term phase can be used in various contexts, often making it ambiguous and potentially confusing. A useful strategy is to adopt more specific terminology instead of simply referring to “phase” in isolation. On its own, “phase” refers to a stage within a cycle—much like the four phases of the moon. Sound waveforms are cyclical, and we can speak of a positive or negative phase, as shown below. However, this original meaning is rarely used in music theory. Here, we focus on other more relevant applications and interpretations of phase (see right and below).\n\n\n\nFig.\n\n\nInitial phase refers to the point in the cycle where the oscillation begins.\nInstantaneous phase: In music theory, “phase” often refers to the instantaneous phase—a specific point in time, not a stage in a sequence. It’s helpful to adopt the term instantaneous explicitly to denote a single position within a given cycle.\nSince instantaneous phase refers to a single position within a cycle, it can also be represented as an angle. This leads to a synonymous relationship between phase and angle, though it’s important to stress that both denote a position within the cycle.\n\n\n\nFig.\n\n\nTwo oscillators operating at the same frequency can be in phase or out of phase. Being in phase means they are synchronized—there is no phase difference. Being out of phase indicates a lack of synchronization, i.e., a phase difference. This difference can take many forms, but two specific cases are of particular interest: quadrature phase and phase opposition.\nQuadrature phase is the phase difference between sine and cosine waves, which equals a quarter of a cycle (90 degrees).\nPhase opposition is the maximum possible phase difference—half a cycle or 180 degrees.\n\n\n4.3.10 Polarity\nAs we’ve seen, phase opposition leads to signal cancellation—but only under certain waveform conditions! This occurs with sine waves, for instance, but not with all waveforms or signals. Note how, in the figure to the right, inverting the sign of every amplitude value in a waveform results in cancellation when added to the original signal.\nInverting polarity means changing the sign, or multiplying by -1. For sine waves, this results in the same effect as a 180-degree phase opposition. However, a true phase inversion is different, as it involves a time or phase shift.\nDespite this distinction, the term phase inversion is often misused when it really refers to a polarity inversion—which is neither a time shift nor a phase shift.\nYes, this can be very confusing and requires careful attention. That’s why this tutorial prefers the term polarity inversion, although many audio devices refer to a 180-degree phase shift when they are, in fact, performing a polarity inversion.\nBoth phase and polarity inversion produce the same result for sine waves due to their symmetrical waveforms, where the second half of the cycle mirrors the first with opposite sign. Other waveforms with this property include triangle and square waves (with a 0.5 pulse width).\n\n\n\nFig.\n\n\nSawtooth waves, however, do not share this symmetry. Therefore, phase opposition is not equivalent to polarity inversion in this case. The only way to achieve full cancellation of a sawtooth wave is through polarity inversion. Refer to the graphs below: only the original sawtooth combined with its polarity-inverted version results in complete cancellation.\nThe native [phasor~] and [osc~] objects include a right inlet that accepts control data to reset the phase. Whenever the inlet receives a number from 0 to 1, the waveform resets to that initial phase position. Note that this is unrelated to phase modulation techniques discussed earlier.\n\n\n\nFig.\n\n\nThe [osc~] object does not support phase modulation; we implemented it using [phasor~] in the previous examples. Hence, using a [phasor~] together with a [cos~] enables both phase modulation and oscillator resetting.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#additivity",
    "href": "chapters/synthesis.html#additivity",
    "title": "4  Synthesis",
    "section": "4.4 Additivity",
    "text": "4.4 Additivity",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#amplitude-ring-modulation",
    "href": "chapters/synthesis.html#amplitude-ring-modulation",
    "title": "4  Synthesis",
    "section": "4.5 Amplitude & Ring Modulation",
    "text": "4.5 Amplitude & Ring Modulation\nAmplitude modulation (AM) and ring modulation (RM) are two techniques that manipulate the amplitude of a signal using another signal. While they share similarities, they produce distinct results and are used in different contexts. In this section, we will explore the differences between AM and RM, their applications, and how they can be implemented in sound synthesis.\n\n4.5.1 Amplitude Modulation\nWe can modulate the amplitude of any signal—referred to as the carrier—by multiplying it with an oscillating signal, called the modulator. The modulator is typically another oscillator, and its frequency determines the modulation frequency.\n\n\n\nAmplitude Modulation\n\n\nIn the example provided, both the carrier and modulator are sine wave oscillators. This is what we call “classic” amplitude modulation (AM), where the modulator signal includes a DC offset, making it unipolar, ranging only from 0 to 1. This unipolarity ensures that the carrier’s amplitude is scaled without ever becoming negative.\n\n\n4.5.2 Ring Modulation\nRing modulation is a particular form of amplitude modulation where both the carrier and modulator signals are bipolar, meaning they oscillate between -1 and 1 without any DC offset. In this configuration, there’s no functional distinction between carrier and modulator—both behave symmetrically.\n\n\n\nRing Modulation\n\n\nNonetheless, in practical terms, the carrier is usually an audio signal such as a musical instrument, while the modulator remains a simple oscillator. A key technical detail is that when the modulator signal is negative, it inverts the polarity of the carrier signal—producing a unique and often metallic timbre.\n\n\n4.5.3 DC Offset\nAmplitude modulation schemes can involve various DC offset settings—not just limited to AM or RM. In our example, preset configurations for AM and RM are available, but one can also manually adjust peak levels and DC offset using sliders.\nObserve how, in the frequency spectrum of classic AM, we see two sidebands—above and below the carrier frequency—each at half the amplitude of the original carrier. These sidebands are spaced apart by the modulation frequency.\n\n\n\nFig.\n\n\nIn contrast, ring modulation removes the original carrier frequency entirely from the spectrum, leaving only the sidebands, which typically carry more energy than in AM.\nBy adjusting amplitude and DC offset with sliders, we can morph continuously between AM and RM modes, gaining nuanced control over the presence of the original frequency component and the energy distribution in the sidebands.\n\n\n4.5.4 Audio Samples & Modulation\nIn this example, an audio sample replaces the oscillator as the carrier signal. This demonstrates how amplitude modulation can function as an audio effect processor rather than just a synthesis technique. In fact, much of what we traditionally associate with synthesis techniques is often more accurately described as audio processing.\nConversely, many effects processors—such as filters—are integral to sound synthesis. The boundary between synthesis and processing is thus fluid and contextual.\n\n\n\nFig.\n\n\nTry both the classic AM and RM examples. In both cases, sidebands are generated for each sine wave component within the carrier signal. AM retains the carrier’s original sine components, which coexist and interact with the generated sidebands. For this reason, AM is commonly used for tremolo effects (which we’ll examine later). On the other hand, RM removes the original sine components entirely, yielding a more sonically distinctive result.\n\n\n4.5.5 Other Waveforms\nUsing more complex waveforms for the modulator signal leads to the creation of additional partials within any amplitude modulation patch—including ring modulation. Sine waves are typically favored as modulators since they offer clean and controlled results, especially when applying AM as an audio effect.\n\n\n\nFig.\n\n\nHowever, in synthesis contexts, more intricate and harmonically rich methods—such as frequency and phase modulation—offer more efficient and versatile approaches for generating complex timbres. We’ll explore these in the following sections.\n\n\n4.5.6 Tremolo\nTremolo is essentially amplitude modulation using a low-frequency modulator. The key addition is a depth parameter, ranging from 0 to 1, which determines the modulation intensity. At 0, no modulation occurs (dry signal), while a depth of 1 results in full tremolo, where the carrier’s amplitude is modulated across its full range.\n\n\n\nFig.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#frequency-modulation",
    "href": "chapters/synthesis.html#frequency-modulation",
    "title": "4  Synthesis",
    "section": "4.6 Frequency Modulation",
    "text": "4.6 Frequency Modulation\nIn general terms, to modulate a signal means to alter it in some way. In the context of this course, however, we refer specifically to using a modulating signal to control a parameter—such as amplitude, as previously discussed. We now turn to the basic structure of frequency modulation (FM), where an oscillator acts as the modulator.\nThe signal being modulated is called the carrier; in the case of frequency modulation, it is also referred to as the carrier frequency. In contrast, we have the modulating frequency, which corresponds to the frequency of the modulating oscillator. The depth of frequency variation is determined by the amplitude of the modulator and is commonly referred to as the modulation index. The modulation process itself is straightforward: we add the modulating signal to the frequency input of the carrier oscillator. See the example on the right\n\n\n\nFig.\n\n\nBy default, we have a carrier frequency of 400 Hz, a low modulation frequency of 1 Hz, and a modulation index of 100. This means the modulating signal oscillates between -100 and +100 Hz, causing the carrier frequency to vary between 300 and 500 Hz. Note that when the modulation frequency is low, the result is a vibrato-like effect.\n\n4.6.1 FM Simple\nWe apply the same structure as before. Besides a vibrato example, this section includes fully developed FM examples. As with amplitude modulation, FM produces sidebands spaced by intervals equal to the modulation frequency. However, FM can generate many more sidebands, potentially resulting in a much richer spectrum.\n\n\n\nFig.\n\n\nThe higher the modulation index, the greater the number of resulting partials—enabling the creation of dense and complex waveforms. When the carrier and modulator frequencies share a simple harmonic ratio, the resulting waveform is harmonic. Otherwise, it tends to be inharmonic. Click on the message boxes to hear the preset examples.\n\n\n4.6.2 Other Waveforms\nIn this patch, we experiment with different oscillator combinations. Waveforms are arranged according to spectral complexity—from simple sine waves to rich sawtooth waves (the only waveform here containing both even and odd harmonics). The more complex the waveform used, the more intricate the FM result becomes.\n\n\n\nFig.\n\n\nOn the modulator side, the waveforms are idealized and band-unlimited—perfect in theory. However, the frequency-modulated oscillator has a limited bandwidth, which imposes practical constraints on the resulting spectrum.\n\n\n4.6.3 Exponential Frequency\nWe can also use exponential pitch values (such as MIDI note numbers) instead of linear frequency input. The key difference here is that frequency deviation—the modulation index—is now expressed in semitones, not Hertz. This shift affects the entire modulation behavior.\n\n\n\nFig.\n\n\nAs a result, the output waveform becomes asymmetrical and significantly different in character. The main change in the patch is the use of [mtof~] to convert MIDI pitch to frequency in Hz.\n\n\n4.6.4 Ratio\nIt is common practice to define the modulating frequency as a ratio of the carrier frequency. This allows us to work with a single frequency input while maintaining a consistent sonic character across different pitches.\n\n\n\nFig.\n\n\nThis approach preserves the relationship between the carrier and modulator frequencies, which is crucial since this ratio determines how the additional spectral components—partials—are distributed. Harmonic ratios (such as 0.5 or 2) yield harmonic results, while non-integer ratios lead to inharmonic spectra.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#references",
    "href": "chapters/synthesis.html#references",
    "title": "4  Synthesis",
    "section": "References",
    "text": "References\n\n\n\n\nHelmholtz, Hermann. 1954. On the Sensations of Tone as a Physiological Basis for the Theory of Music. 2nd ed. New York: Dover Publications.\n\n\nLévi-Strauss, Claude. 1964. Le Cru Et Le Cuit. Vol. 1. Mythologiques. Paris: Plon.\n\n\nSterne, Jonathan, and Tara Rodgers. 2011. “Poetics of Signal Processing.” Differences 22 (2-3): 31–53. https://doi.org/10.1215/10407391-1428834.\n\n\nThéberge, Paul. 1997. Any Sound You Can Imagine: Making Music/Consuming Technology. Hanover & London: Wesleyan University Press.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html",
    "href": "chapters/sonification.html",
    "title": "5  Sonification",
    "section": "",
    "text": "5.1 Introduction\nIn this chapter, we will explore the concept of sonification, its applications, and how it can be implemented using Pure Data (Pd). We will also discuss the importance of understanding data types and classifications in the context of sonification.\nImagine hearing the changes in global temperature over the past thousand years. What does a brainwave sound like? How can sound be used to enhance a pilot’s performance in the cockpit? These intriguing questions, among many others, fall within the realm of auditory display and sonification.\nResearchers in Auditory Display explore how the human auditory system can serve as a primary interface channel for communicating and conveying information. The goal of auditory display is to foster a deeper understanding or appreciation of the patterns and structures embedded in data beyond what is visible on the screen.\nAuditory display encompasses all aspects of human-computer interaction systems, including the hardware setup (speakers or headphones), modes of interaction with the display system, and any technical solutions for data collection, processing, and computation needed to generate sound in response to data.\nIn contrast, sonification is a core technique within auditory display: the process of rendering sound from data and interactions. Unlike voice interfaces or artistic soundscapes, auditory displays have gained increasing attention in recent years and are becoming a standard method alongside visualization for presenting data across diverse contexts.\nThe international research effort to understand every aspect of auditory display began with the founding of the International Community for Auditory Display (ICAD) in 1992. It is fascinating to observe how sonification and auditory display techniques have evolved in the relatively short time since their formal definition, with development accelerating steadily since 2011.\nAuditory display and sonification are now employed across a wide range of fields. Applications include chaos theory, biomedicine, interfaces for visually impaired users, data mining, seismology, desktop and mobile computing interaction, among many others.\nEqually diverse is the set of research disciplines required for successful sonification: physics, acoustics, psychoacoustics, perceptual research, sound engineering, and computer science form the core technical foundations. However, psychology, musicology, cognitive science, linguistics, pedagogy, social sciences, and philosophy are also essential for a comprehensive, multifaceted understanding of the description, technical implementation, usage, training, comprehension, acceptance, evaluation, and ergonomics of auditory displays and sonification in particular.\nIt is clear that in such an interdisciplinary field, a narrow focus on any single discipline risks “seeing the trees but missing the forest.” As with all interdisciplinary research efforts, auditory display and sonification face significant challenges, ranging from differing theoretical orientations across fields to even the very vocabulary used to describe our work.\nInterdisciplinary dialogue is crucial to advancing auditory display and sonification. However, the field must overcome the challenge of developing and employing a shared language that integrates many divergent disciplinary ways of speaking, thinking, and approaching problems. On the other hand, this very challenge often unlocks great creative potential and new ideas, as these varied perspectives can spark innovation and fresh insights.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#introduction",
    "href": "chapters/sonification.html#introduction",
    "title": "5  Sonification",
    "section": "",
    "text": "Fig.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#data-types",
    "href": "chapters/sonification.html#data-types",
    "title": "5  Sonification",
    "section": "5.2 Data Types",
    "text": "5.2 Data Types\nIn the realm of digital data, understanding the nature and classification of data types is essential for their effective processing, storage, and analysis. The table above presents a detailed taxonomy of data types broadly categorized into Static and Stream / Realtime data, further subdivided into various subtypes. This section will unpack these classifications, elaborating on their characteristics, common formats, and sources.\n\n5.2.1 Static Data\nStatic data refers to data that is stored and remains unchanged until explicitly modified. It is typically collected, stored, and then analyzed or processed offline or asynchronously. This type of data is fundamental in many computational tasks, including machine learning, data mining, and archival storage.\n\n5.2.1.1 Structured Data\nStructured data is organized in a predefined manner, typically conforming to a schema or model that allows easy querying and manipulation.\nExamples:\n\nDatasets (CSV, XLS): Tabular data with rows and columns, where each column has a defined datatype (e.g., numeric, string, datetime).\nFields: The basic elements of structured data that have specific data types, such as integers, floating-point numbers, text strings, or timestamps.\nClasses: Labels used in supervised learning, often binary (two classes) or multiclass (multiple categories).\nImages: Digital images can be structured if stored with metadata or standardized file formats. Examples include compressed formats like JPG and uncompressed formats like BMP.\nMIDI Files: Musical Instrument Digital Interface files encode structured note and control information.\nAudio: Raw waveforms or extracted descriptors (e.g., Mel-frequency cepstral coefficients - MFCCs, Fourier transforms) represent audio signals in structured formats.\nAudio Formats: Common audio file formats such as MP3 (compressed), FLAC (lossless compressed), and WAV (uncompressed).\n\nSources: Data portals (e.g., Kaggle, UCI Machine Learning Repository), social media platforms like Instagram, Reddit, Flickr (for images), and audio repositories.\n\n\n5.2.1.2 Semi-structured Data\nSemi-structured data does not conform to a rigid schema like structured data but still contains tags or markers to separate semantic elements.\nExamples:\n\nMarkup Languages: HTML, XML, JSON, and YAML files are examples of semi-structured data because they contain hierarchical tags and attributes describing the data, but the content may be variable.\n\nSources: Web content, APIs that deliver data in JSON or XML formats.\n\n\n5.2.1.3 Unstructured Data\nUnstructured data lacks a predefined data model or schema. It is the most common form of data generated and can be more challenging to analyze without preprocessing.\nExamples:\n\nTexts: Documents, emails, articles, or social media posts are typical unstructured data examples.\n\nSources: Document collections, text corpora, email archives.\n\n\n\n5.2.2 Stream / Realtime Data\nStream or realtime data refers to continuous data generated in real-time, often requiring immediate or near-immediate processing. These data types are crucial in applications like live monitoring, interactive systems, and real-time analytics.\nExamples:\n\nAudio Streams: Continuous audio feeds such as online radio broadcasts.\nVideo Streams: Live video feeds from CCTV cameras or autonomous vehicles.\nSensor Data: Real-time telemetry or environmental data from IoT devices, Arduino boards, or embedded systems.\nLive MIDI: Streaming musical performance data, used in live concerts or interactive installations.\nOSC (Open Sound Control): A protocol used for networking sound synthesizers, computers, and other multimedia devices, enabling real-time control.\n\nSources: Online radio platforms, surveillance systems, smart vehicles, embedded IoT devices, live music performance setups.\n\n\n5.2.3 Data Classification\nThe classification of data into static and stream/realtime reflects fundamentally different operational paradigms in digital systems. Static data often allows batch processing, archival, and complex analysis without stringent timing constraints. In contrast, stream data demands continuous, low-latency processing to support live feedback, monitoring, or interaction.\nThe structured / semi-structured / unstructured distinction highlights the complexity of dealing with data formats:\n\nStructured data is well-suited for traditional databases and straightforward analysis.\nSemi-structured data requires flexible parsers and understanding of nested or tagged data.\nUnstructured data often necessitates advanced techniques such as natural language processing or computer vision for meaningful extraction.\n\nUnderstanding these data types and their sources is critical when designing systems for data ingestion, storage, processing, and analysis — especially in fields such as machine learning, multimedia processing, and IoT applications.\n\n\n\n\n\n\n\n\n\nType\nSubtype\nExamples\nSources\n\n\n\n\nStatic\nStructured\nSemi-structured Unstructured\n\nDatasets (CSV, XLS)\nFields: numeric, string, datetime\nClasses: binary, multiclass\nImages (compressed: JPG, uncompressed: BMP)\nMIDI files\nAudio: raw, descriptors, Fourier\nAudio formats: MP3, FLAC, WAV\nMarkup languages: HTML, XML, JSON, YAML\nTexts\n\n\nData portals\nInstagram, Reddit, Flickr\nAudio repositories\nWeb, APIs\nDocument collections\n\n\n\nStream / Realtime\n—\n\nAudio streams (e.g. online radio)\nVideo streams (e.g. CCTV, autonomous vehicles)\nSensor data (e.g. Arduino, real-time telemetry)\nLive MIDI\nOSC (Open Sound Control)\n\n\nOnline radio platforms\nSurveillance systems, smart vehicles\nIoT devices, embedded systems\nLive performance setups\nInteractive art and media systems\n\n\n\n\n\n5.2.3.1 References\nGantz, J., & Reinsel, D. (2012). The Digital Universe in 2020: Big Data, Bigger Digital Shadows, and Biggest Growth in the Far East. IDC iView.\nMarr, B. (2016). Big Data in Practice: How 45 Successful Companies Used Big Data Analytics to Deliver Extraordinary Results. Wiley.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#sonification-as-a-creative-framework",
    "href": "chapters/sonification.html#sonification-as-a-creative-framework",
    "title": "5  Sonification",
    "section": "5.3 Sonification as a Creative Framework",
    "text": "5.3 Sonification as a Creative Framework\nSonification, traditionally defined as the systematic, objective, and reproducible translation of data into sound, finds itself at a dynamic intersection between scientific inquiry and creative expression. While often framed within the methodological rigor of data representation, its deeper potential lies equally in its artistic embodiment—an expressive convergence of logic, perception, and auditory imagination. This duality is vital when considering the use of software in the development of creative code practices for sonification. In its essence, sonification provides a means to explore and interpret data temporally. It transforms static information into dynamic sonic experiences, revealing patterns not just to the analytical mind but to the aesthetic ear. As Gresham-Lancaster (Gresham-Lancaster 2012) argues, the field must extend beyond scientific formalism to embrace a broader spectrum of cultural and musical meaning.\nA foundational concept introduced by Gresham-Lancaster is the distinction between first-order and second-order sonification. First-order sonification typically involves straightforward mapping of data points to sound parameters—e.g., a numerical value controlling oscillator frequency or filter cutoff. This direct translation preserves the integrity of the data but may lack emotional resonance or contextual clarity for listeners unfamiliar with the dataset or mappings. Second-order sonification introduces a higher level of abstraction. Here, data is not merely converted to sound but is used to control compositional structures such as rhythm, harmony, timbre, and formal development. This could manifest through software designs that incorporate statistical analysis of datasets to determine musical motifs or drive stochastic processes that evolve over time. By embedding data within culturally familiar or stylistically resonant frameworks—be it ambient textures, rhythmic patterns, or harmonic progressions allows the sonification to become more legible and emotionally impactful.\nA key insight is that listeners inevitably interpret sound within a cultural and stylistic frame. Whether intentional or not, sonification software maps data to sonic outputs will be perceived through the lens of genre conventions—ambient, noise, glitch, minimalism, etc. This reinforces the necessity for designers of sonification systems to consciously engage with aesthetic decisions. Rather than viewing these as distractions from scientific rigor, they should be recognized as essential design variables that determine how well the sonification communicates. For instance, layering the sonified stream within an evolving drone texture or embedding it in rhythmic pulses tied to diurnal cycles may enhance the intelligibility of subtle shifts. These choices, while extramusical, profoundly affect the user’s ability to detect and interpret meaningful changes.\nThe expressive capacity of sonification technics invites an analogy with sculpture, as cited in Xenakis’ conception of raw mathematical outputs as “virtual stones” to be artistically shaped. The creative coder similarly refines the raw outputs of data-driven patches, sculpting auditory forms through iterative experimentation, parameter tuning, and aesthetic discernment. This process reveals sonification not as a deterministic output of data, but as a collaboration between the structure of information and the intuition of the artist-programmer. Moreover, by incorporating techniques such as timbral mapping, adaptive filtering, and data-driven granular synthesis, it becomes a laboratory for crafting sonic experiences that honor both the source data and the listener’s perceptual world. This is particularly effective when the goal is to embed sonification within broader artistic or performative contexts—installations, interactive systems, or generative concerts—where expressivity and audience engagement are critical.\nThe evolution of sonification within the creative coding domain demands a reconceptualization of what constitutes success in sonification. Beyond reproducibility, the true measure lies in perceptual clarity, aesthetic engagement, and communicative power. As Gresham-Lancaster asserts, the inclusion of musical form, stylistic awareness, and cultural embedding are not luxuries, but necessities for sonification to become a widely adopted and meaningful practice Pure Data, with its flexibility, open-ended structure, and visual coding paradigm, provides an ideal environment to develop and test both first-order and second-order sonification systems. By embracing both scientific precision and artistic insight, Pd-based sonification becomes a model for interdisciplinary practice—where the auditory exploration of data is not just informative but transformative.\n\n5.3.1 Data Humanism: A Visual Manifesto of Giorgia Lupi\n\n\n\nHumanism of Data\n\n\nData is now recognized as one of the foundational pillars of our economy, and the idea that the world is exponentially enriched with data every day has long ceased to be news.\nBig Data is no longer a distant dystopian future; it is a commodity and an intrinsic, iconic feature of our present—alongside dollars, concrete, automobiles, and Helvetica. The ways we relate to data are evolving faster than we realize, and our minds and bodies are naturally adapting to this new hybrid reality built from physical and informational structures. Visual design, with its unique power to reach deep into our subconscious instantly—bypassing language—and its inherent ability to convey vast amounts of structured and unstructured information across cultures, will play an even more central role in this quiet yet inevitable revolution.\nPioneers of data visualization like William Playfair, John Snow, Florence Nightingale, and Charles Joseph Minard were the first to harness and codify this potential in the 18th and 19th centuries. Modern advocates such as Edward Tufte, Ben Shneiderman, Jeffrey Heer, and Alberto Cairo have been instrumental in the field’s renaissance over the past twenty years, supporting the transition of these principles into the world of Big Data.\nThanks to this renewed interest, an initial wave of data visualization swept across the web, reaching a wider audience beyond the academic circles where it had previously been confined. Unfortunately, this wave was often ridden superficially—used as a linguistic shortcut to cope with the overwhelming nature of Big Data.\n“Cool” infographics promised a key to mastering this untamable complexity. When they inevitably failed to deliver on this overly optimistic expectation, we were left with gigabytes of illegible 3D pie charts and cheap, translucent user interfaces cluttered with widgets that even Tony Stark or John Anderton from Minority Report would struggle to understand.\nIn reality, visual design is often applied to data merely as a cosmetic gloss over serious and complicated problems—an attempt to make them appear simpler than they truly are. What made cheap marketing infographics so popular is perhaps their greatest contradiction: the false claim that a few pictograms and large numbers inherently have the power to “simplify complexity.” The phenomena that govern our world are, by definition, complex, multifaceted, and often difficult to grasp. So why would anyone want to dumb them down when making critical decisions or delivering important messages?\nYet, not all is bleak in this sudden craze for data visualization. We are becoming increasingly aware that there remains a considerable gap between the real potential hidden within vast datasets and the superficial images we typically use to represent them. More importantly, we now recognize that the first wave succeeded in familiarizing a broader audience with new visual languages and tools.\nHaving moved past what we might call the peak of infographics, we are left with a general audience equipped with some of the necessary skills to welcome a second wave of more meaningful, thoughtful visualization.\nWe are ready to question the impersonality of a purely technical approach to data and begin designing ways to connect numbers with what they truly represent: knowledge, behaviors, and people.\nCertainly. Here’s a refined and academically styled rephrasing of the provided section, maintaining clarity, structure, and alignment with the tone of an academic book:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#data-humanism-and-personal-data",
    "href": "chapters/sonification.html#data-humanism-and-personal-data",
    "title": "5  Sonification",
    "section": "5.4 Data Humanism and Personal Data",
    "text": "5.4 Data Humanism and Personal Data\nData Humanism is a perspective initially articulated by Giorgia Lupi (Lupi 2017), grounded in the belief that meaningful engagement with data necessitates attention to its underlying contexts and the inclusion of subjective perspectives throughout the processes of data collection, analysis, and representation—particularly when the data reflects human experience. Rather than reducing information to mere numbers or abstractions, Data Humanism emphasizes the value of personalized, context-rich, and narratively driven interpretations. It advocates for a relationship with data that is not only analytical but also emotional, aesthetic, and reflective (Canossa et al. 2022).\nThis orientation has gained considerable traction within the domains of personal informatics and personal visualization, which investigate how individual-centered representations—visual or tangible—can support self-awareness, reflective thinking, and behavior change. Within these fields, Data Humanism has informed two primary dimensions: data representation and the sensemaking process.\n\n5.4.1 Data Representation\nFrom the perspective of Data Humanism, effective data representation involves the creation of complex and personalized visual forms. “Complexity” here does not imply obfuscation, but rather a deliberate move beyond conventional charts and graphs, toward expressive visual metaphors capable of revealing unexpected connections and enriching the narrative potential of the data (Kim et al. 2019).\nPersonalization plays a crucial role in enabling individuals to define and structure data in accordance with their own conceptual frameworks, thus making the resulting representations more relevant and resonant. Moreover, contextual information—embedded at all stages of the data pipeline from collection to display—is essential for constructing coherent and situated personal narratives. This approach aligns with broader discourses in data visualization research that call for expressive and nuanced visual forms, capable of communicating layered meanings rather than merely delivering rapid information.\n\n\n5.4.2 Sensemaking Process\nData Humanism also foregrounds the importance of deep, deliberate engagement in the interpretive act of making sense of data. As Lupi (Lupi 2017) observes, insight does not emerge from superficial scanning but from a sustained exploration of context and meaning. In this light, the process of data sensemaking is framed as investigative, interpretive, and relational, acknowledging the imperfection and approximation that are often intrinsic to human data.\nThis approach encourages individuals to participate actively in the shaping of their own data narratives—through exploration, translation, and imaginative visualization—which in turn facilitates deeper personal connections and empathetic understanding of others. These practices resonate with the concept of slow technology (Hallnäs and Redström 2001), which posits that slower, more reflective interactions can enrich user experience, enabling more space for contemplation and insight. Here, “slowness” is not a limitation but a strategic feature, fostering sustained attention and interpretive depth.\nRecent work in personal visualization has explored how Data Humanism can be applied in practice. One avenue involves sketch-based visualization tools, which leverage the intuitive, open-ended nature of drawing to support the creation of personalized visual representations. Another involves constructive visualizations: physical, often non-digital artifacts that users assemble and manipulate to give form to data. Additionally, digital platforms have been developed that enable the design of expressive visualizations capable of capturing qualitative aspects of personal context, broadening the expressive range of data visual design.\nOverall, Data Humanism advocates for data representations that embrace subjectivity and slowness as virtues rather than limitations. It proposes that personal data is best understood not through abstract generalization, but through thoughtful, expressive, and often collaborative processes. In this book, we extend this line of inquiry by examining how Data Humanism principles can be integrated into collaborative design practices for personal visualization—exploring new ways to humanize, materialize, and narrativize the data that defines and reflects our lives.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#electrocardiogram-data",
    "href": "chapters/sonification.html#electrocardiogram-data",
    "title": "5  Sonification",
    "section": "5.5 Electrocardiogram Data",
    "text": "5.5 Electrocardiogram Data\nThis Pure Data patch represents an innovative approach to biomedical data sonification, transforming electrocardiogram (ECG) readings into audible waveforms. By treating physiological data as audio samples, the patch creates a unique intersection between medical diagnostics and sound synthesis, offering both artistic and analytical possibilities for exploring cardiac rhythms through auditory perception.\n\n\n\nFig.\n\n\n\n5.5.1 Patch Overview\nThe electrocardiogram synthesis patch implements a complete data-to-audio pipeline that processes filtered ECG data stored in text format. The system loads cardiac waveform data into Pure Data arrays and uses audio synthesis techniques to render the biological signals as sound, enhanced with spatial reverberation effects.\n\n\n5.5.2 System Architecture\n\n\n\n\n\ngraph TD\n    A[electrocardiogram.txt] --&gt; B[readtxt subpatch]\n    B --&gt; C[Array Sizing]\n    A --&gt; D[Array Loading]\n    D --&gt; E[tabread4~ Audio Reader]\n    F[Phasor~ Oscillator] --&gt; E\n    E --&gt; G[freeverb~ Reverb]\n    G --&gt; H[Audio Output]\n    B --&gt; I[Data Count Output]\n\n\n\n\n\n\n\n\n5.5.3 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nParameters\n\n\n\n\nreadtxt\nSubpatch for file analysis\nCounts data points in text file\n\n\ntextfile\nFile reading mechanism\nLoads ECG data from external source\n\n\ntable/array\nData storage buffer\nHolds ECG samples for playback\n\n\nphasor~\nPlayback control oscillator\nSweeps through array data\n\n\ntabread4~\nAudio sample interpolator\nReads array as continuous audio\n\n\nfreeverb~\nSpatial audio processor\nAdds reverberation to output\n\n\n\n\n\n5.5.4 ECG Data Format and Structure\nThe electrocardiogram.txt file contains preprocessed electrocardiogram (ECG) data representing cardiac electrical activity captured from a real patient or medical recording device.\n\n5.5.4.1 Format Specification\n\nFile Type: Plain text format (.txt)\nData Points: 47,499 individual samples\nValue Range: Approximately 544 to 1,023\nSampling Structure: One numerical value per line\nData Type: Integer values representing digitized voltage measurements\n\n\n\n5.5.4.2 Signal Properties\nThe ECG data exhibits characteristic patterns of cardiac electrical activity:\n\n\n\n\n\n\n\n\nParameter\nValue\nDescription\n\n\n\n\nBaseline Level\n~830-850\nResting electrical potential\n\n\nPeak Amplitude\n1,023\nMaximum QRS complex values\n\n\nMinimum Values\n544\nDeep S-wave deflections\n\n\nDuration\nVariable\nComplete cardiac cycles with P-QRS-T patterns\n\n\n\nThe high sample count (47,499 points) provides sufficient temporal resolution to capture multiple complete heartbeat cycles, enabling both detailed analysis of individual cardiac events and broader rhythm pattern recognition through auditory display techniques.\n\n\n\n5.5.5 Data Flow\nThe readtxt subpatch serves as a preprocessing module that:\n\nOpens the electrocardiogram.txt file\nCounts the total number of data points\nCalculates appropriate array dimensions\nProvides sizing information for memory allocation\n\n\n\n\n\n\ngraph LR\n    A[readtxt subpatch] --&gt; B[Array Sizing]\n    B --&gt; C[Array Loading]\n    C --&gt; D[Audio Conversion]\n\n\n\n\n\n\nThe electrocardiogram sonification patch orchestrates a sophisticated data transformation pipeline that converts static physiological measurements into dynamic audio experiences. This process involves multiple interconnected stages of data handling, each contributing essential functionality to the overall system’s ability to render cardiac rhythms as meaningful sonic representations.\nThe initial phase of data flow centers on the comprehensive analysis and preparation of the source electrocardiogram dataset. The readtxt subpatch performs a preliminary scan of the electrocardiogram.txt file, systematically counting the total number of individual data points contained within the text document. This preprocessing step proves crucial for subsequent memory allocation decisions, as Pure Data requires explicit array dimensioning before data can be loaded into memory buffers. The subpatch communicates the determined data quantity to the main patch, enabling dynamic array sizing that accommodates datasets of varying lengths without requiring manual configuration adjustments. Following this analysis phase, the system initiates the actual data transfer process, where numerical values representing cardiac electrical activity are systematically loaded from the text file into Pure Data’s internal array structures. This loading operation transforms the static file-based data into a format suitable for real-time audio processing, creating a bridge between stored physiological measurements and live sonic synthesis.\nThe audio synthesis stage represents the core transformation where biological data becomes audible waveform. The phasor~ object generates a continuous ramp signal that serves as the fundamental timing mechanism for array traversal, essentially functioning as a playback head that moves through the stored ECG data at a controllable rate. This oscillator’s frequency parameter directly determines the speed at which the cardiac data is sonified, allowing for temporal scaling that can compress hours of ECG recordings into minutes of audio or extend brief cardiac events for detailed auditory analysis. The tabread4~ object performs the critical function of converting discrete array values into continuous audio signals through sophisticated interpolation algorithms. Rather than simply stepping through individual data points, which would produce harsh discontinuities and aliasing artifacts, the four-point interpolation scheme creates smooth transitions between adjacent values, resulting in audio output that maintains the natural flow characteristics of the original cardiac rhythms while remaining suitable for human auditory perception.\nThe final processing stage introduces spatial and timbral enhancements that transform the raw sonified data into aesthetically coherent audio suitable for both analytical and artistic applications. The freeverb~ object applies algorithmic reverberation that adds spatial depth and acoustic warmth. This reverberation processing serves multiple purposes: it masks potential digital artifacts from the sonification process, creates a more immersive listening environment that encourages extended auditory analysis, and provides timbral coherence that helps listeners focus on meaningful patterns rather than being distracted by the mechanical quality of direct data-to-audio conversion. The spatial processing also contributes to the patch’s potential for integration into larger sonic environments, where the ECG sonification might coexist with other audio elements in installations or performance contexts.\nThroughout this entire data flow pathway, the system maintains precise temporal relationships between the original cardiac measurements and their auditory representations, ensuring that the sonification preserves the essential rhythmic and morphological characteristics that define cardiac health and pathology. This fidelity to source data temporal structure enables the sonification to serve legitimate diagnostic and educational purposes while simultaneously opening creative possibilities for artistic exploration of physiological processes.\n\n\n\n\n\ngraph LR\n    A[Data Size] --&gt; B[Array Efficiency]\n    B --&gt; C[Playback Smoothness]\n    C --&gt; D[Audio Quality]\n\n\n\n\n\n\nThe patch maintains signal integrity through:\n\n4-point interpolation for anti-aliasing\nConfigurable playback rates for temporal scaling\nReverb processing for spatial contextualization\n\nThis electrocardiogram synthesis patch demonstrates Pure Data’s versatility in scientific data visualization through sound, creating meaningful auditory representations of biological processes while maintaining the temporal and amplitude characteristics essential for medical interpretation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#ecg-controlled-step-sequencer",
    "href": "chapters/sonification.html#ecg-controlled-step-sequencer",
    "title": "5  Sonification",
    "section": "5.6 ECG-Controlled Step Sequencer",
    "text": "5.6 ECG-Controlled Step Sequencer\nThis patch demonstrates an innovative approach to data-driven rhythm generation, where electrocardiogram readings function as temporal control signals for a step sequencer. By interpreting cardiac rhythm data as beat timing information, the patch creates a unique hybrid system that transforms physiological patterns into musical sequences, establishing a direct relationship between biological rhythms and electronic music production.\n\n\n\nFig.\n\n\n\n5.6.1 Patch Overview\nThe ECG-controlled step sequencer implements a rhythm generation system that uses cardiac data as the fundamental timing source for audio playback. Rather than sonifying the ECG waveform directly, this patch extracts temporal information from the physiological data to control the rhythmic structure of a step sequencer, creating polyrhythmic patterns that reflect the natural variability of human heartbeat intervals.\n\n\n5.6.2 System Architecture\n\n\n\n\n\ngraph TD\n    A[electrocardiogram.txt] --&gt; B[readtxt subpatch]\n    B --&gt; C[counter subpatch]\n    C --&gt; D[ECG Value Reader]\n    D --&gt; E[BPM Scaling]\n    E --&gt; F[Delay Object]\n    F --&gt; G[Bang Trigger]\n    G --&gt; H[Audio Playback]\n    I[Audio File] --&gt; J[audiotable Array]\n    J --&gt; K[tabplay~ Object]\n    G --&gt; K\n    K --&gt; L[Audio Output]\n\n\n\n\n\n\n\n\n5.6.3 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nParameters\n\n\n\n\nreadtxt\nECG data file reader\nLoads and counts data points from text file\n\n\ncounter\nSequential data access\nSteps through ECG values line by line\n\n\ntabread\nArray value retrieval\nReads individual ECG measurements\n\n\ndel\nRhythmic timing control\nCreates delays based on ECG-derived BPM\n\n\ntabplay~\nAudio sample playback\nTriggers audio file samples\n\n\nsoundfiler\nAudio file loader\nLoads external audio into array\n\n\nspigot\nGate control\nEnables/disables sequencer operation\n\n\n\n\n5.6.3.1 Data Processing Components\nReadTxt Subpatch\nThe readtxt subpatch functions as the primary data interface, managing both file loading and data quantification:\n\nOpens and reads the electrocardiogram.txt file\nCounts total data points for array sizing\nLoads data into local table for sequential access\nProvides file path management and error handling\n\nCounter Subpatch\nThe counter mechanism implements sequential data traversal with automatic cycling:\n\nMaintains current position in ECG dataset\nIncrements through data points on each trigger\nResets to beginning when reaching data endpoint\nProvides configurable upper limits for partial data playback\n\n\n\n\n5.6.4 Data Flow\nThe ECG-controlled step sequencer orchestrates a multi-stage data transformation process that converts static cardiac measurements into dynamic rhythmic triggers. This system demonstrates how biological timing patterns can be extracted and repurposed as musical control data, creating compositions that maintain organic temporal characteristics while serving structured musical functions.\nThe initial data preparation phase centers on the comprehensive loading and organization of the electrocardiogram dataset. The readtxt subpatch performs dual functions: it analyzes the text file structure to determine data quantity and simultaneously loads the numerical values into Pure Data’s table system for efficient random access. This preprocessing ensures that the cardiac data becomes immediately available for real-time sequential reading without file system delays that would disrupt musical timing. The subpatch communicates essential metadata to the main patch, including total data point count and successful loading confirmation, enabling the counter system to establish appropriate cycling boundaries for continuous operation.\nThe rhythmic control generation stage represents the core innovation where cardiac data becomes musical timing information. The counter subpatch maintains a position index that advances through the ECG dataset on each bang trigger, creating a sequential reading mechanism that treats the physiological data as a tempo map. Each ECG value retrieved through tabread undergoes scaling multiplication to convert the raw cardiac measurement into a meaningful delay time for the del object. This scaling factor determines the relationship between cardiac electrical activity levels and musical tempo, allowing for both literal interpretations where higher cardiac values produce faster rhythms, or inverted mappings where increased cardiac activity corresponds to longer inter-beat intervals, mimicking cardiac refractory periods.\nThe audio playback coordination stage transforms the ECG-derived timing signals into actual sound events through sophisticated sample triggering mechanisms. Each bang generated by the delay object initiates playback of audio material stored in the audiotable array via the tabplay~ object. The soundfiler object ensures that external audio files are properly loaded and resized within the array structure, while the spigot gate provides manual control over the sequencer’s operational state. This architecture enables the system to function as a complete musical instrument where the natural variability of cardiac rhythms generates complex polyrhythmic patterns that would be difficult to achieve through conventional programmed sequencing methods.\n\n\n\n\n\ngraph LR\n    A[ECG Data Loading] --&gt; B[Sequential Access]\n    B --&gt; C[BPM Conversion]\n    C --&gt; D[Delay Generation]\n    D --&gt; E[Audio Triggering]\n\n\n\n\n\n\n\n\n5.6.5 Processing Chain Details\nThe patch maintains sonic coherence through several design considerations: the scaling factor allows adjustment of the overall tempo range to match musical contexts, the counter’s cycling behavior ensures continuous operation without interruption, and the audio loading system supports various sample types from percussive hits to sustained tones. This flexibility enables the ECG step sequencer to function across diverse sound applications, from experimental compositions exploring biological rhythms to dance music productions requiring organic timing variations.\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nText File\nData parsing and counting\nArray population\n\n\n2\nArray Position\nSequential value retrieval\nRaw ECG values\n\n\n3\nECG Values\nMultiplication scaling\nDelay times (ms)\n\n\n4\nDelay Times\nTemporal scheduling\nBang triggers\n\n\n5\nBang Triggers\nAudio sample initiation\nSound output\n\n\n\n\n\n5.6.6 Creative Applications\n\nCardiac rhythm layering: Load different ECG datasets from multiple subjects to create polyrhythmic compositions where each person’s heartbeat triggers different audio samples\nMedical data sonification: Use ECG data from various cardiac conditions (arrhythmia, tachycardia, bradycardia) to generate distinct rhythmic patterns for educational or artistic purposes\nBiometric beat matching: Sync live performances to pre-recorded cardiac rhythms, creating music that literally follows the pulse of the performer or subject\nPhysiological drum machines: Replace traditional step sequencer programming with real cardiac data to generate organic, non-repetitive percussion patterns\nHeart rate variability exploration: Use ECG datasets recorded during different emotional states or physical activities to trigger corresponding audio textures\nInteractive health installations: Design gallery pieces where visitors’ real-time heart rates control sample playback, creating personalized sonic experiences\nTemporal scaling experiments: Multiply ECG values by extreme factors to compress hours of cardiac data into minutes of rapid-fire triggering or extend brief arrhythmias into extended compositions\nCollaborative cardiac compositions: Network multiple ECG step sequencers where participants’ combined heart rhythms create collective musical pieces that reflect group physiological states",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#image-sonification-with-rgba-data",
    "href": "chapters/sonification.html#image-sonification-with-rgba-data",
    "title": "5  Sonification",
    "section": "5.7 Image Sonification with RGBA Data",
    "text": "5.7 Image Sonification with RGBA Data\nDigital images contain vast amounts of visual information encoded as pixel data, where each pixel represents color values across different channels. The RGBA image sonification patch transforms this visual data into auditory experiences by converting pixel color values into audio samples. This approach creates a direct mapping between visual and auditory domains, where the Red, Green, Blue, and Alpha channels of an image become independent audio streams that can be manipulated and mixed in real-time.\nThe sonification process reveals hidden patterns and textures within images that may not be immediately apparent through visual inspection alone. By treating pixel data as audio samples, we can explore the rhythmic, harmonic, and textural qualities inherent in visual compositions, creating a synesthetic bridge between sight and sound.\n\n\n\nFig.\n\n\n\n5.7.1 Patch Overview\nThe RGBA image sonification system consists of three main components working in concert:\n\nImage Loading and Analysis: Handles file import and pixel data extraction\nRGB-arrays (Data Processing): Normalizes and stores channel data in arrays\nSonification Engine: Converts pixel data to audio with real-time controls\n\nThe patch workflow follows a clear data pipeline: images are loaded and decomposed into RGBA channels, pixel values are normalized and stored in arrays, and finally converted to audio samples with frequency and amplitude modulation capabilities.\n\n\n\n\n\nflowchart TD\n    A[Image File] --&gt; B[pix_dump]\n    B --&gt; C[RGBA Pixel Stream]\n    C --&gt; D[RGB-arrays Subpatch]\n    D --&gt; E[Normalized Arrays R,G,B,A]\n    E --&gt; F[Sonification Subpatch]\n    F --&gt; G[Audio Output]\n    H[Frequency Control] --&gt; F\n    I[Amplitude Control] --&gt; F\n    \n    style A fill:#e1f5fe\n    style G fill:#f3e5f5\n    style D fill:#fff3e0\n    style F fill:#fff3e0\n\n\n\n\n\n\n\n\n5.7.2 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Sonification\n\n\n\n\npix_dump\nPixel data extraction\nConverts image to sequential RGBA values\n\n\narray\nData storage\nHolds normalized pixel values for each channel\n\n\ntabread4~\nArray interpolation\nSmooth audio sample reading with interpolation\n\n\nphasor~\nTiming control\nGenerates read position for array traversal\n\n\n*~\nAudio mixing\nAmplitude control and channel mixing\n\n\ndac~\nAudio output\nFinal audio rendering\n\n\n\n\n5.7.2.1 Critical Data Processing Objects\npix_dump: This object serves as the bridge between GEM’s visual processing and Pure Data’s audio domain. It outputs a continuous stream of RGBA values (typically 0-255 range) that represent the complete image dataset.\nArray Management: Each RGBA channel requires its own array for independent manipulation. The arrays store normalized values (-1 to 1) making them suitable for direct audio interpretation.\ntabread4~: Provides crucial interpolation between array values, creating smooth audio transitions rather than harsh digital stepping when reading pixel data as audio samples.\n\n\n\n5.7.3 Data Flow\nThe RGBA image sonification patch orchestrates a comprehensive data transformation pipeline that converts visual information into auditory experiences through multiple interconnected processing stages. This sophisticated workflow demonstrates how digital image data can be systematically decomposed, analyzed, and reconstituted as meaningful sonic representations while preserving the essential characteristics of the original visual content.\nThe initial image decomposition stage establishes the foundation for the entire sonification process through the critical function of the pix_dump object. When an image file is loaded into the system, whether in JPEG, PNG, TIFF, or other supported formats, the pix_dump object performs a comprehensive analysis that extracts every pixel’s color information as a sequential stream of numerical values. This extraction process follows a systematic pattern where each pixel contributes four distinct values representing its Red, Green, Blue, and Alpha channel intensities. The resulting data stream takes the form of R1 G1 B1 A1 R2 G2 B2 A2 R3 G3 B3 A3, creating a linear representation of the two-dimensional visual information. This linearization process effectively flattens the spatial relationships of the image into a temporal sequence, transforming visual space into a time-based data structure suitable for audio processing. The pixel values extracted during this stage typically range from 0 to 255, representing the standard 8-bit color depth used in most digital image formats.\nThe channel separation and normalization stage represents a critical transformation where the raw pixel data becomes suitable for audio synthesis applications. The continuous RGBA stream generated by the image decomposition process undergoes systematic demultiplexing, where individual color channel values are separated and directed into dedicated processing pathways. This separation enables independent manipulation of each color channel, allowing for sophisticated sonic textures that can emphasize specific visual characteristics of the original image. The Red channel array captures the warm color information and often contains significant luminance data, while the Green channel typically holds the most visually sensitive information due to human perception characteristics. The Blue channel frequently contains fine detail and cooler color information, and the Alpha channel represents transparency data that may reveal composition and layering information within the original image. Each separated channel undergoes normalization processing that converts the original 0-255 range into the -1 to 1 range required for audio synthesis, ensuring that the pixel data can be directly interpreted as audio sample values without amplitude clipping or distortion.\nThe audio synthesis stage transforms the normalized pixel arrays into dynamic audio streams through sophisticated control mechanisms that enable real-time manipulation of the sonification parameters. Each of the four normalized arrays becomes an independent audio source controlled by tabread4~ objects that perform interpolated reading of the stored pixel data. The frequency parameter, typically ranging from 0.1 to 20 Hz, determines the rate at which the system traverses through the pixel data, effectively controlling the temporal scaling of the visual information. Lower frequency values in the 0.1 to 1 Hz range reveal macro-structures and overall compositional elements of the image, allowing listeners to perceive broad color transitions and major visual forms through gradual sonic evolution. Higher frequencies in the 5 to 20 Hz range expose fine details and textures, creating rapid sonic fluctuations that correspond to pixel-level variations in the original image. The amplitude control parameter provides independent volume adjustment for each RGBA channel, enabling dynamic balancing that can emphasize particular color information or create complex sonic textures through channel interaction. The array index parameter allows for positional control within the image data, enabling targeted exploration of specific regions or systematic scanning of the entire visual content.\nThe signal mixing stage represents the culmination of the sonification process, where the four independent audio channels combine to create a unified sonic representation of the original image. This mixing process employs configurable amplitude controls that enable sophisticated real-time adjustment of the channel balance, allowing users to isolate individual color channels for detailed analysis or create weighted combinations that emphasize particular visual characteristics. Individual channel isolation proves particularly valuable for understanding how specific color information contributes to the overall visual composition, while weighted mixing enables the creation of custom sonic interpretations that highlight particular aspects of the image content. Dynamic balance adjustment during playback allows for evolving sonic textures that can reveal temporal patterns within the visual data or create artistic interpretations that maintain connection to the source material while achieving aesthetic coherence. The mixing stage also incorporates safeguards against amplitude overflow and provides signal conditioning that ensures the final audio output remains within acceptable dynamic ranges for both analytical listening and artistic presentation contexts.\nThroughout this entire data flow pathway, the system maintains precise correspondence between visual and auditory domains, ensuring that the sonification preserves meaningful relationships between image characteristics and sonic parameters. This fidelity enables the patch to serve both analytical purposes, where visual patterns can be detected through auditory analysis, and creative applications, where image data becomes source material for musical composition and sound design. The modular architecture of the data flow also supports extension and customization, allowing users to insert additional processing stages or modify the parameter mappings to achieve specific analytical or artistic objectives.\nThe data transformation process involves several critical stages:\n\n5.7.3.1 Stage 1: Image Decomposition\n\n\n\n\n\nflowchart LR\n    A[Image File] --&gt; B[pix_dump] --&gt; C[R1 G1 B1 A1 R2 G2 B2 A2 R3 G3 B3 A3]\n\n\n\n\n\n\n\n\n5.7.3.2 Stage 2: Channel Separation and Normalization\n\n\n\n\n\nflowchart LR\n    A[RGBA Stream] --&gt; B[Channel Demux]\n    B --&gt; C[Red Array]\n    B --&gt; D[Green Array]\n    B --&gt; E[Blue Array]\n    B --&gt; F[Alpha Array]\n    \n    C --&gt; G[Normalize -1 to 1]\n    D --&gt; H[Normalize -1 to 1]\n    E --&gt; I[Normalize -1 to 1]\n    F --&gt; J[Normalize -1 to 1]\n\n\n\n\n\n\n\n\n5.7.3.3 Stage 3: Audio Synthesis\nEach normalized array becomes an audio source with independent control parameters:\n\n\n\nParameter\nControl Range\nAudio Effect\n\n\n\n\nFrequency\n0.1 - 20 Hz\nPlayback speed through pixel data\n\n\nAmplitude\n0 - 1\nVolume level for each RGBA channel\n\n\nArray Index\n0 - Array Size\nPosition within image data\n\n\n\n\n\n5.7.3.4 Stage 4: Signal Mixing\nThe four audio channels (RGBA) are mixed using configurable amplitude controls, allowing for: - Individual channel isolation - Weighted mixing of color information - Dynamic balance adjustment during playback\n\n\n\n5.7.4 Creative Applications\n\nVisual Music Composition: Transform paintings, photographs, or digital art into musical compositions where visual elements directly correlate to audio characteristics\nColor-Sound Synesthesia: Explore the relationship between color perception and auditory experience through real-time sonification\nInteractive Installations: Create responsive environments where visual input generates immediate audio feedback\nPattern Recognition: Audio playback can reveal repetitive structures, gradients, and textures in images that may be difficult to perceive visually\nData Archaeology: Sonify hidden or corrupted image data to identify patterns or anomalies\nComparative Analysis: Compare multiple images by listening to their sonified representations\n\n\n\n5.7.5 Technical Considerations\nFrequency Scaling: The frequency control determines how quickly the system traverses through the pixel data. Lower frequencies (0.1-1 Hz) reveal macro-structures and overall image composition, while higher frequencies (5-20 Hz) expose fine details and textures.\nChannel Weighting: Different RGBA channels contribute varying amounts of perceptual information:\n\nRed channel often contains luminance information\nGreen channel typically has the highest visual sensitivity\nBlue channel may contain fine detail information\nAlpha channel represents transparency data\n\nArray Size Optimization: Large images produce extensive arrays that may require memory management considerations. The patch can be optimized for specific image dimensions based on available system resources.\n\n\n5.7.6 Performance Tips\n\nStart with low frequencies (0.1-0.5 Hz) to understand overall image structure\nIsolate individual channels to understand their unique contribution to the image\nUse amplitude controls to balance channels based on their visual prominence\nExperiment with different image types - photographs, abstract art, and technical diagrams produce distinctly different sonic characteristics\n\nThis sonification approach opens new possibilities for cross-modal artistic expression and analytical exploration, demonstrating how Pure Data’s flexible architecture enables innovative connections between disparate data domains.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#image-scanner",
    "href": "chapters/sonification.html#image-scanner",
    "title": "5  Sonification",
    "section": "5.8 Image Scanner",
    "text": "5.8 Image Scanner\nThis patch implements a linear scanning sonification system that transforms visual image data into audio through systematic pixel sampling. By creating multiple scan points that traverse an image from left to right, the patch converts spatial color information into frequency-controlled oscillators, creating a real-time audio representation of visual content. This approach enables users to “hear” images through systematic exploration of pixel data across defined scan lines.\nThe scanning process reveals the vertical distribution of visual information within images, where different scan lines capture distinct horizontal slices of the visual content. Each scan point operates independently, creating polyphonic textures that reflect the complexity and variation present in the source image.\n\n\n\nFig.\n\n\n\n5.8.1 Patch Overview\nThe linear image scanner consists of four primary subsystems that work together to create a comprehensive image-to-audio conversion pipeline:\n\nImage Loading and Display: Manages file import and visual presentation through GEM\nDynamic Scan Point Generation: Creates configurable numbers of scanning instances\nPixel Data Extraction: Samples color values at specific image coordinates\nAudio Synthesis: Converts pixel data into frequency-controlled oscillators\n\nThe patch employs dynamic patching techniques to generate scalable numbers of scan points, each implemented as an instance of the linear-scan abstraction. This modular approach enables flexible configuration while maintaining efficient processing of multiple simultaneous scan lines.\n\n\n\n\n\nflowchart TD\n    A[Image File] --&gt; B[pix_image/pix_texture]\n    B --&gt; C[Visual Display]\n    D[Number of Scan Points] --&gt; E[Dynamic Patch Generation]\n    E --&gt; F[linear-scan Abstractions]\n    F --&gt; G[pix_data Objects]\n    G --&gt; H[Pixel Color Extraction]\n    H --&gt; I[Frequency Mapping]\n    I --&gt; J[Oscillator Control]\n    J --&gt; K[Audio Output]\n    L[Scan Duration Control] --&gt; M[Temporal Coordination]\n    M --&gt; F\n    \n    style A fill:#e1f5fe\n    style K fill:#f3e5f5\n    style E fill:#fff3e0\n    style F fill:#fff3e0\n\n\n\n\n\n\n\n\n5.8.2 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Scanning\n\n\n\n\npix_image\nImage loading\nLoads image files into GEM processing chain\n\n\npix_texture\nTexture rendering\nEnables visual display and pixel access\n\n\npix_data\nPixel sampling\nExtracts color values at specified coordinates\n\n\nlinear-scan\nScan line abstraction\nImplements individual scanning instances\n\n\nosc~\nAudio oscillator\nGenerates frequency-controlled sine waves\n\n\nline\nTemporal interpolation\nControls scanning position over time\n\n\nmetro\nTiming control\nCoordinates scan timing across instances\n\n\n\n\n5.8.2.1 Critical Processing Components\npix_data Object: This GEM object serves as the core interface between visual and audio domains. It samples pixel color values at specified x,y coordinates within the loaded image. The normalize message controls coordinate interpretation: when enabled (default), coordinates range from (0.0, 0.0) at bottom-left to (1.0, 1.0) at top-right; when disabled, coordinates use actual pixel dimensions.\nDynamic Patch Generation: The generate-scanpoints subpatch implements dynamic patching techniques to create configurable numbers of linear-scan instances. This system calculates y-position arguments for each scan line and generates the corresponding patch objects programmatically.\nlinear-scan Abstraction: Each scan line is implemented as an abstraction that receives a y-position argument determining its vertical position within the image. The abstraction contains the pixel sampling logic and frequency mapping calculations specific to that scan line.\n\n\n\n5.8.3 Data Flow\nThe linear image scanner a real-time processing pipeline that transforms static visual information into dynamic audio representations through systematic pixel sampling and frequency mapping. This process demonstrates how spatial image data can be converted into temporal audio experiences while preserving meaningful relationships between visual characteristics and sonic parameters.\nThe image loading and preparation stage establishes the foundation for the entire scanning process through comprehensive file management and visual processing setup. When an image file is loaded via the import-image subpatch, the system employs GEM’s pix_image object to read various image formats including JPEG, PNG, and TIFF into Pure Data’s visual processing environment. The loaded image undergoes texture processing through the pix_texture object, which prepares the pixel data for efficient random access while simultaneously enabling visual display within the GEM rendering window. This dual functionality ensures that users can see the image being processed while the scanning system extracts pixel information for audio synthesis. The image data becomes available to multiple pix_data objects that will sample specific pixel locations, with the texture object maintaining the image in memory for continuous access throughout the scanning process.\nThe dynamic scan point generation stage represents a sophisticated implementation of Pure Data’s dynamic patching capabilities, enabling user-configurable scanning resolution. The generate-scanpoints subpatch analyzes the user-specified number of scan points and calculates the appropriate y-coordinate distribution across the image height. For each scan point, the system generates a new instance of the linear-scan abstraction with a specific y-position argument that determines where that scan line will sample pixels vertically within the image. This y-position argument undergoes normalization calculation to ensure proper distribution from 0.0 (bottom of image) to 1.0 (top of image), creating evenly spaced scan lines that cover the entire vertical dimension of the source image. The dynamic generation process creates the necessary patch connections and initializes each scan line instance with its unique vertical position parameter, enabling parallel processing of multiple image regions simultaneously.\nThe pixel sampling and coordinate management stage implements the core data extraction functionality where visual information becomes available for audio processing. Each linear-scan abstraction contains a pix_data object configured to sample pixels at its assigned y-coordinate while the x-coordinate varies continuously during the scanning process. The temporal control system coordinates the scanning motion through line objects that interpolate x-position values from 0.0 to 1.0 over the user-specified scan duration. This creates synchronized left-to-right motion across all scan lines, with each line sampling pixels at its fixed y-position while traversing the full width of the image. The pix_data objects output both grayscale and RGB color values for each sampled pixel, providing multiple data streams that can be used for different aspects of the audio synthesis process. The temporal coordination ensures that all scan lines move in synchronization, creating coherent scanning motion that preserves the spatial relationships present in the original image.\nThe frequency mapping and audio synthesis stage transforms the extracted pixel data into meaningful audio parameters through sophisticated scaling and oscillator control. Each scan line’s pixel sampling output undergoes frequency mapping that converts color intensity values into specific frequency ranges for audio oscillators. The frequency distribution system maps the y-position of each scan line to a specific frequency band, creating a spectral arrangement where scan lines at the bottom of the image control lower frequencies while scan lines at the top control higher frequencies. This vertical frequency mapping creates intuitive correspondence between image position and audio pitch, enabling listeners to perceive spatial relationships within the image through pitch relationships in the audio output. The pixel intensity values modulate the oscillator frequencies within each scan line’s assigned frequency band, so brighter pixels produce higher frequencies while darker pixels produce lower frequencies within that band. Multiple oscillators operating simultaneously create polyphonic textures that reflect the complexity and variation present across different regions of the source image.\n\n\n\n\n\nflowchart LR\n    A[Image Loading] --&gt; B[Scan Point Generation]\n    B --&gt; C[Pixel Sampling]\n    C --&gt; D[Frequency Mapping]\n    D --&gt; E[Audio Synthesis]\n\n\n\n\n\n\n\n\n5.8.4 Processing Chain Details\nThe scanning system maintains precise temporal and spatial coordination through several sophisticated control mechanisms:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nImage File\nGEM loading and texture preparation\nAccessible pixel data\n\n\n2\nScan Point Count\nDynamic abstraction generation\nMultiple linear-scan instances\n\n\n3\nTime Position\nSynchronized x-coordinate interpolation\nPixel sampling coordinates\n\n\n4\nPixel Color Values\nIntensity to frequency conversion\nOscillator control signals\n\n\n5\nMultiple Frequencies\nPolyphonic audio mixing\nCombined audio output\n\n\n\n\n5.8.4.1 Temporal Control Parameters\n\n\n\nParameter\nControl Range\nAudio Effect\n\n\n\n\nScan Duration\n100-90000 ms\nSpeed of left-to-right traversal\n\n\nFrequency Range\n100-5000 Hz\nSpan of available frequencies\n\n\nScan Point Count\n1-256 points\nNumber of simultaneous scan lines\n\n\nY-Position Distribution\n0.0-1.0\nVertical spacing of scan lines\n\n\n\n\n\n5.8.4.2 Coordinate Mapping System\nThe patch implements sophisticated coordinate management that ensures proper spatial relationships between image pixels and audio parameters:\nSpatial Mapping: Each scan line occupies a fixed y-coordinate determined by its position in the overall distribution, while x-coordinates vary temporally from left to right during scanning.\nFrequency Allocation: The vertical position of each scan line determines its base frequency range, creating spectral separation that preserves spatial information in the audio domain.\nTemporal Synchronization: All scan lines move simultaneously from left to right, maintaining spatial coherence while revealing temporal evolution of visual content.\n\n\n\n5.8.5 Creative Applications\n\nMusical Score Scanning: Transform written musical notation into audio by scanning staff lines, converting note positions into corresponding pitches and rhythms\nTexture Analysis: Explore the sonic characteristics of different visual textures by scanning patterns, fabrics, or surface details to reveal their rhythmic and harmonic qualities\nArchitectural Sonification: Scan building facades, floor plans, or structural diagrams to create audio representations of architectural spaces and proportions\nData Visualization Audio: Convert scientific charts, graphs, and data visualizations into audio to provide alternative access methods for visually impaired users\nArtistic Collaboration: Enable visual artists to hear their compositions while creating, providing real-time audio feedback during the visual creative process\nInteractive Installations: Create gallery pieces where visitors can upload images and immediately hear their sonic representation through the scanning system\nEducational Tools: Teach concepts of frequency, spatial relationship, and data representation by demonstrating direct correlations between visual and audio domains\nLive Performance Integration: Use the scanner with live video feeds or real-time image manipulation to create dynamic audio responses to visual performance elements\nAstronomical Data Exploration: Scan telescope images, star charts, or planetary surfaces to create audio representations of cosmic phenomena and celestial structures",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#camera-scanner",
    "href": "chapters/sonification.html#camera-scanner",
    "title": "5  Sonification",
    "section": "5.9 Camera Scanner",
    "text": "5.9 Camera Scanner\nThis patch extends the linear image scanning concept to real-time video input, creating dynamic sonification of live camera feeds. Unlike the static image scanner previously discussed, this system processes continuous video streams from web cameras, enabling real-time exploration of moving visual content through audio. The patch maintains the core scanning methodology while introducing temporal dynamics that reflect the changing nature of live video input.\nThe live camera scanner transforms the temporal dimension of video into immediate sonic feedback, where movements, lighting changes, and scene transitions become audible through frequency modulation across multiple scan lines. This creates an interactive relationship between the physical environment captured by the camera and the generated audio landscape.\n\n\n\nFig.\n\n\n\n5.9.1 Patch Overview\nThe live camera scanner builds upon the established linear scanning framework while incorporating several enhancements specific to real-time video processing:\n\nLive Video Capture: Continuous camera input with adjustable contrast and color inversion\nEnhanced Scanning Controls: Random scanning modes and interpolated scan transitions\nReal-time Processing: Immediate response to visual changes in the camera feed\nExtended User Interface: Additional controls for camera manipulation and scanning behavior\n\nThe system maintains the same dynamic patching architecture and frequency mapping strategies established in the static image scanner, ensuring consistency in the sonification approach while adapting to the temporal nature of video input.\n\n\n5.9.2 Key Objects and Their Roles\nBuilding upon the previously established scanning framework, the live camera scanner introduces several additional components:\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Live Scanning\n\n\n\n\npix_video\nCamera input capture\nProvides continuous video stream from webcam\n\n\npix_contrast\nImage enhancement\nAdjusts pixel intensity for better scanning contrast\n\n\npix_invert\nColor inversion\nCreates alternative visual representations\n\n\npix_2grey\nGrayscale conversion\nSimplifies color data for processing\n\n\nrandom\nStochastic positioning\nGenerates unpredictable scan patterns\n\n\nspigot\nControl gating\nEnables/disables random scanning behavior\n\n\n\n\n5.9.2.1 Live Video Processing Components\npix_video Object: Captures real-time video input from connected cameras, providing the continuous data stream that replaces static image loading. The object supports multiple camera devices and delivers frame-by-frame pixel data to the scanning system.\nImage Enhancement Chain: The pix_2grey → pix_contrast → pix_invert processing chain offers real-time image manipulation capabilities that enhance the scanning process by adjusting visual characteristics to optimize sonic output.\nRandom Scanning System: Unlike static image scanning, the live camera scanner includes stochastic elements that introduce unpredictability to the scanning patterns, creating evolving audio textures that respond to both visual content and algorithmic variation.\n\n\n\n5.9.3 Data Flow\nThe live camera scanner orchestrates a continuous real-time processing pipeline that transforms streaming video data into dynamic audio representations. While maintaining the core scanning methodology established in the static image scanner, this system introduces temporal continuity and enhanced control mechanisms that respond to the evolving nature of live video input.\nThe live video capture and preprocessing stage establishes a continuous data source through integrated camera management and image enhancement systems. The pix_video object maintains a persistent connection to the selected camera device, delivering frame-by-frame pixel data at standard video refresh rates. This continuous stream undergoes real-time preprocessing through a sophisticated image enhancement chain that optimizes the visual content for effective sonification. The pix_2grey object converts color video to grayscale, reducing computational complexity while preserving luminance information essential for frequency mapping. The pix_contrast object provides real-time contrast adjustment, enabling users to emphasize or diminish visual details based on scanning requirements. The optional pix_invert stage creates alternative visual representations that can reveal hidden patterns or provide aesthetic variations in the sonic output. This preprocessing chain ensures that the video data reaches the scanning system in an optimized format that maximizes the effectiveness of the pixel-to-frequency mapping process.\nThe enhanced scanning control stage introduces sophisticated temporal management systems that extend beyond the basic left-to-right scanning pattern. The system maintains the established dynamic scan point generation methodology while incorporating additional control mechanisms for temporal behavior. The random scanning subsystem employs random and spigot objects to introduce stochastic elements into the scanning pattern, creating unpredictable position variations that generate evolving audio textures independent of visual content changes. The scan interpolation system provides smooth transitions between scanning positions, reducing audio artifacts that might result from abrupt position changes in the video stream. The temporal coordination mechanisms ensure that all scan lines maintain synchronized motion while accommodating the additional complexity introduced by random positioning and interpolated movement patterns.\nThe real-time frequency mapping stage adapts the established pixel-to-frequency conversion methodology to accommodate the continuous nature of video input. Each scan line continuously samples pixels at its assigned y-coordinate while the x-coordinate evolves through the scanning pattern, creating ongoing frequency modulation that reflects both spatial relationships within the current video frame and temporal changes across successive frames. The frequency distribution system maintains the vertical mapping where scan lines at the bottom of the image control lower frequencies while scan lines at the top control higher frequencies, preserving the spatial-to-spectral correspondence established in the static scanner. However, the continuous nature of video input introduces temporal frequency modulation that creates evolving harmonic textures as visual content changes over time. The amplitude and frequency controls provide real-time adjustment capabilities that enable users to respond to changing lighting conditions, scene transitions, or desired aesthetic effects during live performance or analysis situations.\n\n\n\n\n\nflowchart LR\n    A[Live Video Input] --&gt; B[Image Enhancement]\n    B --&gt; C[Dynamic Scanning]\n    C --&gt; D[Frequency Mapping]\n    D --&gt; E[Real-time Audio]\n\n\n\n\n\n\n\n\n5.9.4 Processing Chain Details\nThe live camera scanner maintains the established processing architecture while incorporating real-time enhancements:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nCamera Stream\nVideo capture and preprocessing\nEnhanced video frames\n\n\n2\nEnhanced Frames\nDynamic scan point coordination\nSynchronized scan positions\n\n\n3\nScan Positions\nReal-time pixel sampling\nContinuous color values\n\n\n4\nColor Values\nLive frequency mapping\nDynamic oscillator control\n\n\n5\nAudio Signals\nReal-time mixing and output\nLive audio stream\n\n\n\n\n5.9.4.1 Enhanced Control Parameters\n\n\n\n\n\n\n\n\nParameter\nControl Range\nLive Scanning Effect\n\n\n\n\nContrast\n0-10\nVisual enhancement for improved frequency mapping\n\n\nRandom Scan\nOn/Off\nStochastic scanning pattern generation\n\n\nInvert Colors\nOn/Off\nAlternative visual representation\n\n\nScan Interpolation\nSmooth/Linear\nTransition quality between scan positions\n\n\n\n\n\n5.9.4.2 Real-time Performance Considerations\nFrame Rate Coordination: The scanning system synchronizes with video frame rates to ensure smooth audio generation without dropouts or artifacts caused by timing mismatches between video capture and audio processing.\nAdaptive Frequency Scaling: Real-time adjustment capabilities enable the system to respond to varying lighting conditions and scene content, maintaining optimal frequency mapping across different visual environments.\nComputational Efficiency: The live processing requirements demand efficient algorithms that can maintain real-time performance while processing multiple scan lines simultaneously across continuous video streams.\n\n\n\n5.9.5 Creative Applications\n\nLive Performance Visualization: Create real-time audio responses to performer movements, gestures, or choreographed actions captured by the camera system\nEnvironmental Sonification: Transform ambient visual environments into evolving soundscapes that reflect changing lighting, weather conditions, or natural phenomena\nInteractive Installation Experiences: Enable gallery visitors to influence the sonic environment through their physical presence and movements within the camera’s field of view\nSurveillance Audio: Convert security camera feeds into audio monitoring systems that provide auditory alerts for visual changes or movement detection\nAccessibility Applications: Provide real-time audio descriptions of visual environments for visually impaired users, translating spatial and temporal visual information into sonic representations\nDance and Movement Analysis: Create audio feedback systems for movement training where dancers can hear the sonic representation of their spatial positioning and temporal dynamics\nTelepresence Audio: Enable remote participants to experience distant visual environments through real-time sonification of camera feeds from other locations\nMachine Vision Integration: Combine computer vision algorithms with live scanning to create responsive systems that sonify specific detected objects, faces, or movement patterns",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#references-1",
    "href": "chapters/sonification.html#references-1",
    "title": "5  Sonification",
    "section": "References",
    "text": "References\n\n\n\n\nCanossa, Alessandro, Luis Laris Pardo, Michael Tran, and Alexis Lozano Angulo. 2022. “From Data Humanism to Metaphorical Visualization–an Educational Game Case Study.” In International Conference on Human-Computer Interaction, 109–17. Cham: Springer.\n\n\nGresham-Lancaster, Scot. 2012. “Relationships of Sonification to Music and Sound Art.” AI and Society 27 (2): 207–12.\n\n\nHallnäs, Lars, and Johan Redström. 2001. “Slow Technology–Designing for Reflection.” Personal and Ubiquitous Computing 5: 201–12.\n\n\nKim, Nam Wook, Hyejin Im, Nathalie Henry Riche, Alicia Wang, Krzysztof Gajos, and Hanspeter Pfister. 2019. “Dataselfie: Empowering People to Design Personalized Visuals to Represent Their Data.” In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, 1–12. New York, NY, USA: Association for Computing Machinery.\n\n\nLupi, Giorgia. 2017. “Data Humanism: The Revolutionary Future of Data Visualization.” PrintMag. https://www.printmag.com/article/data-humanism-future-of-data-visualization/.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/space.html",
    "href": "chapters/space.html",
    "title": "6  Space",
    "section": "",
    "text": "6.1 The Music Sound Space\nIn the realm of sound, the concept of space is often overlooked. Yet, it plays a crucial role in shaping our auditory experience. The spatial dimension of sound is not merely an acoustic phenomenon; it is a fundamental aspect of how we perceive sound. This chapter explores the multifaceted nature of sound space, its historical evolution, and its implications for contemporary electroacoustic music.\nSound always unfolds within a specific time and space. From the very moment sound is generated in music, space is implicitly present, enabling the possibility of organizing, reconstructing, and shaping that space to influence musical form. Thus, the use of space in music responds to concerns that go far beyond mere acoustic considerations.\nConceiving space as a structural element in the construction of sonic discourse requires us to address a complex notion—one that touches on multiple dimensions of composition, performance, and perception. This idea rests on the argument that space is a composite musical element, one that can be integrated into a compositional structure and assume a significant role within the formal hierarchy of sound discourse.\nSpatiality in music represents a compositional variable with a long historical lineage. The treatment of sound space on stage can be traced back to classical Greek theatre, where actors and chorus members used masks to amplify vocal resonance and enhance vocal directionality (Knudsen 1932). However, while compositional techniques addressing spatial sound have been developing for centuries, it is not until the early 20th century that a systematic use of spatial dimensions becomes central to musical structure. Landmark works such as Universe Symphony (1911–51) by Charles Ives, Déserts (1954) and Poème électronique (1958) by Edgard Varèse, Gruppen (1955–57) by Karlheinz Stockhausen, and Persephassa (1969) by Iannis Xenakis approach spatiality as an independent and structural dimension. In these works, space is interrelated with other sonic parameters such as timbre, dynamics, duration, and pitch.\nParticularly noteworthy is that these pieces do not merely establish sonic space as a new musical dimension—they also develop theoretical frameworks for the spatialization of sound. These theories consider physical, poetic, pictorial, and perceptual aspects of spatial sound. As a result, in recent decades the areas of research and application related to artistic-musical knowledge have expanded significantly, fostering interdisciplinary inquiry into sound space and its parameters.\nBeyond its central role in music, in recent years spatial sound has also become a subject of study across a variety of disciplines. One example of this is found in research into sound spatialization techniques in both real and virtual acoustic environments. These techniques have largely evolved based on principles of psychoacoustics, cognitive modeling, and technological advancements, leading to the development of specialized hardware and software for spatial sound processing. In the scientific realm, theses and research articles have explored, in great detail, the cues necessary to create an acoustic image of a virtual source and how these cues must be simulated—whether by electronic circuitry or computational algorithms.\nHowever, it must be noted that musical techniques for managing sound space have not yet fully integrated findings from perceptual science. As we will explore later, many artistic approaches rely on only a subset of the cues involved in auditory spatial perception, overlooking others that are essential for producing a convincing acoustic image.\nIn general terms, the spatial dimension of a musical composition can function as the primary expressive and communicative attribute for the listener. For instance, from the very first moment, the acoustic characteristics of a venue—real or virtual—directly affect how we perceive the sonic discourse. This dimension of musical space is further shaped by the placement of sound sources, whether instrumentalists or loudspeakers. A historical precedent for this can be found in the Renaissance period through the use of divided choirs in the Basilica of San Marco in Venice. This stylistic practice, known as antiphonal music, represents a clear antecedent of the deep connection between musical construction and architectural space (Stockhausen 1959). The style of Venetian composers was strongly influenced by the acoustics and architectural features of San Marco, involving spatially separated choirs or instrumental groups performing in alternation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Space</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#the-music-sound-space",
    "href": "chapters/space.html#the-music-sound-space",
    "title": "6  Space",
    "section": "",
    "text": "Figure 1: Floor plan of the Basilica of San Marco in Venice (Italy).\n\n\n\n6.1.1 Spatial Distribution and the Evolution of Listening Spaces\nDuring the European Classical and Romantic periods, the spatial distribution of musicians generally adhered to the French ideal inspired by 19th-century military band traditions. In this widely adopted setup, space was typically divided into two main zones: musicians arranged on a frontal stage and the audience seated facing them. This concert model—linear, frontal, and fixed—reflected a formalized, hierarchical approach to the musical experience.\nBy the 20th century, this front-facing paradigm began to be reexamined and reimagined. Composers and sound artists started to question the limitations of conventional concert hall layouts. A notable example is found in 1962, when German composer Karlheinz Stockhausen proposed a radical redesign of the concert space to accommodate the evolving needs of contemporary composition. His vision included a circular venue without a fixed podium, flexible seating arrangements, adaptable ceiling and wall surfaces for mounting loudspeakers and microphones, suspended balconies for musicians, and configurable acoustic properties. These directives were not merely architectural; they were compositional in nature, meant to transform the listening experience by embedding spatiality into the core of musical structure.\nThis reconceptualization of space enabled a new way of thinking about the sensations and experiences that could be elicited in the listener through the manipulation of spatial audio. The concert hall was no longer a passive container of sound, but an active participant in its articulation.\nIn parallel developments, particularly in the field of electroacoustic music during the mid-20th century, spatial concerns were often shaped by the available technology of the time. Spatiality lacked a clearly defined theoretical vocabulary and was not yet integrated systematically with other familiar musical parameters. Nonetheless, from its inception, electroacoustic music leveraged technology to expand musical boundaries—either through electronic signal processing or by interacting with traditional instruments.\nHowever, the notion of space in electroacoustic music differs significantly from that of instrumental music. In this context, spatiality is broad, multidimensional, and inherently difficult to define. It encompasses not only the individual sonic identity of each source—typically reproduced through loudspeakers—but also the relationships between those sources and the acoustic space in which they are heard. Critically, the audible experience of space unfolds through the temporal evolution of the sonic discourse itself.\nLoudspeakers in electroacoustic music afford a uniquely flexible means of organizing space as a structural element. The spatial potential enabled by electroacoustic technologies has been, and continues to be, highly significant. With a single device (see Fig. 2), it is possible to transport the listener into a wide range of virtual environments, expanding the scope of musical space beyond the physical limitations of the performance venue. This opens the door to sonic representations of distance, movement, and directionality—factors that can be fully integrated into the musical structure and treated as compositional dimensions in their own right.\n\n\n\nFigure 2: Multichannel electroacoustic concert hall.\n\n\n\n\n\n6.1.2 Spatial Recontextualization and Auditory Expectation\nOne of the defining characteristics of electroacoustic music is its capacity to recontextualize sound. Through virtual spatialization, a sound can acquire new meaning depending on how it functions within diverse contexts. For example, two sounds that would never naturally coexist—such as a thunderstorm and a mechanical engine in the same acoustic scene—can be juxtaposed to create a landscape that defies ecological realism. This deliberate disjunction can trigger a complex interaction between what is heard and the listener’s prior knowledge of those sound sources, drawn from personal experience.\nOur ability to interpret spatial cues in sound is deeply shaped by the patterns of interpersonal communication we engage in, the lived experience of urban or rural environments, and the architectural features of our surroundings. These formative influences are so embedded in our perceptual systems that we are often unaware of their role in shaping how we understand sensory information. Spatial cues are constantly processed in our everyday auditory experience and play a vital role in shaping our listening behaviors.\nIn the context of electroacoustic music, environmental cues may not only suggest associations with a physical location but also inform more abstract properties of the spatial discourse articulated by the piece. Jean-Claude Risset (Risett 1969) noted a tension in using environmental recordings in electroacoustic composition: the recognizable identity of “natural” sounds resists transformation without diminishing their spatial content. Nevertheless, identifiable sounds remain central to electroacoustic works, particularly those aligned with the tradition of musique concrète.\nUnder normal listening conditions, perception of a singular sound source is shaped by what Pierre Schaeffer (Schaeffer 2003) defined as the “sound object”—a sonic phenomenon perceived as a coherent whole, grasped through a form of reduced listening that focuses on the sound itself, independent of its origin or meaning. Our auditory system has evolved to identify and locate such sound objects in space by mapping them according to the physical attributes of their sources. A barking sound is understood as coming from a dog; a voice is mapped to a human speaker. As a result, the spatial interpretation of sound is closely linked to the listener’s expectations, shaped not only by the inherent acoustic characteristics of the signal but also by the listener’s accumulated learning and experience.\nUnlike incidental listening in everyday life, attentive listening in electroacoustic music generally occurs from a fixed position. Apart from minor head movements, the spatial information available to the listener depends entirely on the acoustic cues encoded in the sound. While spatial hearing has been extensively researched in the field of psychoacoustics, its compositional potential as a carrier of musical form remains underexplored. In particular, the dimension of distance has received little attention compared to azimuth (horizontal angle) and elevation (vertical angle). These two spatial dimensions have been rigorously studied and well-documented, forming the basis of many standard models of spatial perception.\nBy contrast, the auditory perception of distance remains one of the most enigmatic topics in both music and psychophysics. It involves a complex array of cues—many of which are still not fully understood or integrated into compositional practice. As such, distance remains an open and promising field of inquiry for creative and scientific exploration alike (Abregú, Calcagno, and Vergara 2012).\n\n\n\nFigure 3: Schematic representation of the azimuth, elevation, and distance axes.\n\n\n\n\n\n6.1.3 The Expanded Spatial Palette of Electroacoustic Music\nUnlike instrumental music, which relies on the physicality of pre-existing sound sources, electroacoustic music is not constrained by such limitations. This fundamental distinction allows composers not only to design entirely new sounds tailored to their spatial intentions but also to explore acoustic environments that defy conventional physical logic. While a performance of acoustic music offers visual and sonic cues grounded in the material world—a stage, a hall, visible instruments—electroacoustic music invites a more abstract engagement, where the listener navigates a virtual sound world.\nIn acoustic settings, physical space provides the listener with consistent spatial information. In contrast, electroacoustic environments—particularly those utilizing multichannel reproduction—radically extend the spatial canvas. These environments challenge and expand our auditory expectations, offering a broader spectrum of spatial strategies shaped by the interaction between sound diffusion technologies and the perceptual mechanisms of the listener.\nYet, this very potential also brings complications. The ephemeral and immaterial nature of loudspeaker-based sound makes it difficult to generate a spatial image as vivid or intuitive as that encountered in the physical world. Despite technological precision, virtual spatial environments often lack the tactile immediacy of real acoustic spaces.\nResearch in the field of computer music typically falls into two major domains: the study of spatial hearing and cognition, and the development of sound reproduction technologies. One of the advantages in electroacoustic contexts is the ability to isolate and manipulate spatial cues independently—a task nearly impossible with traditional acoustic instruments. This level of control opens up possibilities for treating space as a fully sculptable compositional parameter. However, practical results vary greatly, and many perceptual questions remain unresolved. It is within this intersection—where psychophysics, spatialization technologies, and compositional imagination meet—that the most fertile ground for innovation lies.\nA classic example is John Chowning’s Turenas (1972), which employed quadraphonic speaker arrangements and a mathematical model to simulate virtual spatial motion. Chowning implemented a distance-dependent intensity multiplier according to the inverse square law, applied independently to each channel. The outcome was a virtual “phantom source” whose movement through space was dynamically shaped and perceptually engaging. Interestingly, Chowning discovered that simple, well-structured paths—such as basic geometric curves—produced the most convincing spatial gestures. In On Sonic Art (Wishart 1996), one finds multiple visual representations of two-dimensional spatial trajectories; however, these diagrams often fail to translate into equally perceptible auditory experiences. For instance, while Lissajous curves are visually complex and elegant, their perceptual distinctiveness can be minimal. Clarity, it turns out, is key to audible spatial expression.\nSince the mid-20th century, various spatialization systems have emerged to simulate acoustic spaces through loudspeaker arrays. Among the most notable are Intensity Panning, Ambisonics, and Dolby 5.1. Although a detailed technical comparison of these systems lies beyond the scope of this section (see (Basso, Di Liscia, and Pampin 2009) for an extensive overview), it is worth noting that electroacoustic space only becomes meaningful if its structural characteristics—such as reverberation—can be clearly perceived. In orchestral music, spatial features are often inherent to the instrument layout and performance context. With fixed instrument positions, the spatial configuration is relatively stable. In virtual space, however, reverberation must be explicitly created and shaped to define the acoustic character of the environment in which sonic events unfold.\nUnlike the relatively fixed reverberation properties of physical rooms, virtual spaces offer remarkable plasticity. Composers can craft distinct acoustic identities for different sections of a piece, allowing each space to function as a compositional agent in its own right. This flexibility implies that in electroacoustic music, space must often be invented—not merely inherited from the sounding objects, as is typically the case with acoustic instruments.\nIt becomes clear, then, that spatial design can be a powerful structural element in music. Sound space carries poetic, aesthetic, and expressive dimensions. At the same time, convincing spatial construction often depends on our embodied experience of real-world acoustics. This dual grounding—in perceptual realism and creative abstraction—opens new theoretical and practical avenues for artists. The potential to reconceptualize spatiality through an expanded compositional lens can empower creators to engage more fully with the multidimensionality of sonic art. In doing so, it enables the development of richer analytical and taxonomical criteria that extend beyond sound itself, embracing broader cultural, perceptual, and technological contexts.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Space</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#spatial-localization-of-sound",
    "href": "chapters/space.html#spatial-localization-of-sound",
    "title": "6  Space",
    "section": "6.2 Spatial Localization of Sound",
    "text": "6.2 Spatial Localization of Sound\n\n6.2.1 Psychoacoustic Factors in Sound Localization\nWhen we perceive a sound event, we rely on a variety of auditory cues to infer the position of its source. These cues are associated with both direction and distance, and are typically classified into two main categories: binaural cues, which arise from comparisons between the two ears, and monaural cues, which are perceived by a single ear and are still critical for spatial localization.\nTo illustrate the binaural mechanism, consider a point sound source moving in a horizontal circular path around a listener’s head. At any given moment, the distance from the source to each ear will differ depending on the angular position of the source. This spatial configuration results in time and intensity differences between the two ears. These discrepancies are key to our spatial perception of sound.\nWhen a sound wave reaches the right ear before the left, the slight delay in arrival time is known as the Interaural Time Difference (ITD), while the difference in loudness is called the Interaural Level Difference (ILD). These cues are extremely subtle—at 90°, the maximum ITD is around 630 microseconds—yet the brain is remarkably adept at interpreting them to infer the lateral position of the source.\nThe ILD is largely caused by the acoustic shadowing effect of the head. For higher frequencies (with wavelengths shorter than the head’s diameter), the sound cannot diffract around the head, leading to more pronounced differences in intensity between the ears, depending on the angle of incidence.\nThese binaural cues work together most effectively between 800 Hz and 1600 Hz. Below 800 Hz, ITD is the dominant cue; above 1600 Hz, ILD becomes more effective. The distance between the listener and the sound source also affects these cues, with ILD being particularly sensitive to distance changes, while ITD remains relatively stable.\nBinaural localization does not require prior knowledge of the sound source. In contrast, monaural localization—when only one ear is used—often depends on recognizing the sound in advance. One key monaural cue is the Spectral Cue (also called the Monaural Spectral Cue), which arises from changes in the sound spectrum caused by interactions with the outer ear (the pinna).\nEven slight modifications in the signals reaching the auditory system can lead to significant shifts in the perceived spatial image. From an acoustic perspective, the pinna acts as a directional filter, primarily affecting high frequencies. It introduces spectral and temporal modifications to the signal depending on its angle of incidence and distance. These effects help the listener distinguish between sounds originating in front versus behind, and also assist in determining the elevation of the source.\nHistorically, the role of the pinnae in spatial hearing was underestimated, often seen merely as protective structures. Contemporary research, however, confirms their essential role in spatial perception and in reducing environmental noise, such as wind.\nWhen it comes to estimating source distance, familiarity with the sound plays a significant role. For example, spoken voice at a normal volume provides a relatively accurate cue for distance perception. Like light intensity or visual size, sound intensity decreases with distance, following the inverse square law. But intensity is not the only factor: air acts as a low-pass filter, attenuating high frequencies over distance. This effect is clearly noticeable in scenarios like an approaching airplane, where the sound not only grows louder but also becomes brighter and richer in frequency content.\nIn sum, sound localization is a complex perceptual process informed by a combination of binaural and monaural cues, spectral filtering by the outer ear, and familiarity with the sound source. These mechanisms interact continuously to allow listeners to orient themselves in a dynamic acoustic environment, whether real or simulated.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Space</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#hearing-in-enclosed-spaces",
    "href": "chapters/space.html#hearing-in-enclosed-spaces",
    "title": "6  Space",
    "section": "6.3 Hearing in Enclosed Spaces",
    "text": "6.3 Hearing in Enclosed Spaces\nWhen we experience sound in an enclosed environment, the first auditory cue that reaches our ears is known as the direct sound—the signal that travels straight from the source without interacting with any obstacles. This is followed by the first-order reflections, which occur when the sound bounces off a single surface—such as a wall—before reaching the listener. These are then succeeded by second-order reflections, involving two bounces, and progressively higher orders of reflection. Eventually, this cascade of reflections gives rise to a diffuse auditory sensation known as reverberation.\nIn a typical empty room—consider one with four walls, a ceiling, and a floor—the six primary surfaces contribute to the first-order reflections. These early reflections are particularly significant when the sound has a sharp or impulsive attack, as they assist the auditory system in identifying the spatial position of the sound source. Reverberation, by contrast, carries information about the acoustic properties and dimensions of the room. It plays a key role in shaping the overall perception of the space’s material and geometry.\nA critical metric used in room acoustics is the reverberation time, commonly denoted as T₆₀. This refers to the time it takes for the reverberant sound energy to decay by 60 decibels after the original sound source has ceased. A widely used formula for estimating this value is the Sabine equation: \\[T_{60} = 0.16 \\times \\frac{V}{A}\\]\nHere, V represents the volume of the room in cubic meters, and A is the total equivalent absorption area, calculated as the sum of the products of each surface’s area and its corresponding absorption coefficient. Importantly, absorption coefficients are frequency-dependent, meaning that the acoustic behavior of a room varies with different sound frequencies.\nTo account for this, acousticians often use a metric known as the Noise Reduction Coefficient (NRC), which averages absorption values across key frequencies—specifically 250 Hz, 500 Hz, 1000 Hz, and 2000 Hz. This provides a more practical estimate of how a room absorbs sound across the human auditory spectrum.\nA graphical representation of room acoustics typically illustrates the temporal distribution of reflections produced by an impulse response within the space. In such plots, the height of each line corresponds to the relative amplitude of each reflection. These visualizations help researchers and designers understand how sound energy evolves over time in an architectural space, offering insight into both clarity and spatial impression.\nUnderstanding how direct sound, early reflections, and reverberation interact is essential for both technical and artistic applications in sound design. Whether composing for multichannel electroacoustic environments or designing spatial sound installations, these acoustic principles are fundamental to crafting immersive and intelligible auditory experiences.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Space</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#virtual-source-simulation-via-stereophonic-techniques",
    "href": "chapters/space.html#virtual-source-simulation-via-stereophonic-techniques",
    "title": "6  Space",
    "section": "6.4 Virtual Source Simulation via Stereophonic Techniques",
    "text": "6.4 Virtual Source Simulation via Stereophonic Techniques\nStereophonic systems enable the simulation of virtual sound source localization using just two loudspeakers positioned to the left and right of the listener. By manipulating the amplitude of each signal sent to the speakers, we can generate the perceptual illusion of a sound source moving laterally across the auditory field. Additional transformations also allow us to simulate depth and even create the sensation of a source emanating from behind the speakers, constructing an illusory auditory space.\nThe perception of a virtual source positioned between two speakers arises from the system’s ability to artificially generate an Interaural Level Difference (ILD). If the signal emitted from the right speaker is louder than that from the left, the right ear perceives a higher intensity, prompting the auditory system to localize the source toward the right. The degree of perceived lateral displacement is directly proportional to the intensity difference.\nIn typical stereo setups, the speakers are arranged with a 60° separation. However, for purposes of spatial expansion and compatibility with quadraphonic systems (involving four speakers surrounding the listener), we assume a 90° separation. We define 0° as the location of the left speaker and 90° as the right. The objective is to calculate the appropriate amplitude for each speaker to position the virtual source at a desired angle θ within this range.\nTo ensure the virtual source appears equidistant from the listener during its movement, the total sound intensity emitted by both speakers must remain constant. That is:\n\\[\nI_L + I_R = k\n\\]\nAssuming intensity is normalized between 0 and 1, we can simplify this to:\n\\[\nI_L + I_R = 1\n\\]\nHere, \\(I_L\\) and \\(I_R\\) represent the intensities emitted by the left and right speakers, respectively. When the source is fully localized at the left, \\(I_L = 1\\) and \\(I_R = 0\\); when at the right, \\(I_L = 0\\) and \\(I_R = 1\\); and when centered, both are \\(0.5\\). However, in most digital audio systems we operate on amplitude, not intensity. Since intensity is proportional to the square of amplitude, we use the relation:\n\\[\nI \\propto A^2\n\\]\nThus, the condition of constant intensity becomes:\n\\[\nA_L^2 + A_R^2 = 1\n\\]\nA common misconception is to linearly scale amplitudes (e.g., \\(A_L = A_R = 0.5\\) when centered), but this would yield:\n\\[\n0.5^2 + 0.5^2 = 0.25 + 0.25 = 0.5\n\\]\n—only half the intended total intensity, which could be mistakenly perceived as an increase in distance. To preserve consistent intensity across the stereo field, we rely on a trigonometric identity:\n\\[\n\\sin^2(\\theta) + \\cos^2(\\theta) = 1\n\\]\nUsing this, we can define the amplitude assignments as:\n\\[\nA_R = \\sin(\\theta) \\\\\nA_L = \\cos(\\theta)\n\\]\nWhere θ ranges from 0° (left) to 90° (right). For example, at 45°, both \\(sin(45^\\circ)\\) and \\(cos(45^\\circ)\\) equal approximately 0.707. Squaring and summing these values confirms that the total intensity remains 1:\n\\[\nA_R^2 + A_L^2 = \\sin^2(45^\\circ) + \\cos^2(45^\\circ) = 0.5 + 0.5 = 1\n\\]\nThus, this method preserves the perceptual coherence of the virtual sound source’s position while maintaining a constant energy output, a critical aspect of spatial audio realism.\nIn summary, for any virtual angle \\(\\theta \\in [0^\\circ, 90^\\circ]\\), we can accurately simulate spatial position with consistent intensity using:\n\\[\nA_R = \\sin(\\theta) \\\\\nA_L = \\cos(\\theta)\n\\]\nThis technique forms the basis for further expansion into quadraphonic and multichannel spatialization systems.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Space</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#implementing-a-stereo-spatialization-in-pure-data",
    "href": "chapters/space.html#implementing-a-stereo-spatialization-in-pure-data",
    "title": "6  Space",
    "section": "6.5 Implementing a Stereo Spatialization in Pure Data",
    "text": "6.5 Implementing a Stereo Spatialization in Pure Data\nBefore diving into the implementation of a stereophonic spatialization system in Pure Data (Pd), it is essential to note that the objects used to compute trigonometric functions such as sine and cosine require the input angle to be expressed in radians, not degrees. Recall that one radian is the angle formed when the length of the arc of a circle equals the radius. From this definition, we derive the following equivalences:\n\n360° = \\(2\\pi\\) radians\n\n180° = \\(\\pi\\) radians\n\n90° = \\(\\frac{\\pi}{2}\\) radians\n\nTo convert degrees to radians, we multiply the angle in degrees by the ratio (\\(\\frac{2\\pi}{360}\\)). To optimize performance and avoid redundant calculations, we can precompute this factor:\n\\[\n\\text{Angle}_{\\text{rad}} = \\text{Angle}_{\\text{deg}} \\times \\frac{2\\pi}{360} = \\text{Angle}_{\\text{deg}} \\times 0.0174533\n\\]\n\n\n\nFig.\n\n\nThe patch illustrated below simulates a virtual sound source using stereophonic panning. This technique, commonly referred to as intensity panning, adjusts the balance between two loudspeakers. The source signa is split into two channels, each multiplied by the cosine and sine of the specified angle (in radians) to distribute the amplitude between the left and right speakers. As we vary the angle between 0° and 90°, the source appears to move along an arc, emulating the listener’s perception of spatial position.\n\n6.5.1 Simulating Distance\nTo simulate distance, we must account for how amplitude diminishes as the source moves away from the listener. Unlike intensity, which decreases with the square of the distance, amplitude is inversely proportional to the distance:\n\\[\nA \\propto \\frac{1}{d}\n\\]\nThus, the amplitude perceived by the listener is:\n\\[\nA_{\\text{perceived}} = \\frac{A_{\\text{source}}}{d}\n\\]\nTo manage both the angle of lateralization and the distance to the source simultaneously, we use the slider2d object from the ELSE library. This object provides a two-dimensional matrix that captures mouse pointer movement and returns the corresponding (x, y) coordinates.\n\n\n\nFig.\n\n\nWe map the horizontal axis (x) to the angle (0°–90°) and the vertical axis (y) to the distance (1-10). These values are then routed via [send] and [receive] objects for use throughout the patch. It’s crucial to prevent division by zero in distance calculations, so we define the minimum distance as 1—equivalent to the baseline distance between the listener and speakers. If speakers are placed 2 meters away, then a distance of \\(d = 1\\) equals 2 meters in real space, \\(d = 2\\) would correspond to 4 meters, and so on. In this system, distance is relative to the speaker-listener spacing.\n\n\n6.5.2 Simulating Air Absorption\nTo further enhance the realism of our spatialization, we simulate air absorption, which—although minimal at short distances—contributes to the perceptual illusion of depth. As distance increases, high frequencies are attenuated, which we simulate using a low-pass filter object[lop~].\n\n\n\nFig.\n\n\nWe control the cutoff frequency of this filter using a [expr] object ([expr (1 - $f1) * 7000 + 1000]). For example, we can scale a distance range of 1–10 to a cutoff frequency range of 8000–1000 Hz. As the source moves farther away, the filter reduces the brightness of the sound, mimicking atmospheric filtering.\n\n\n6.5.3 Simulating Reverberant Spaces\nConsider a large environment with significant reverberation—such as a church or garage. When someone speaks nearby, the direct sound dominates, and reverberation is minimal. Conversely, at greater distances, the direct signal weakens while reverberation remains relatively constant. This is due to the fact that reverberation is a function of the environment and not as sensitive to the source’s location.\n\n\n\nFig.\n\n\nTo simulate this phenomenon, we introduce a reverberation chamber using [freeverb~]. The reverb level is controlled via a rotary dial ([knob] from the ELSE library). By configuring the knob for a low initial setting, we simulate the acoustic behavior where reverberation becomes more prominent as the source moves farther from the listener. As the source approaches, the increased amplitude of the direct signal masks the reverberation, completing our auditory illusion.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Space</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#simulation-through-quadraphonic-spatialization-the-chowning-model",
    "href": "chapters/space.html#simulation-through-quadraphonic-spatialization-the-chowning-model",
    "title": "6  Space",
    "section": "6.6 Simulation through Quadraphonic Spatialization: The Chowning Model",
    "text": "6.6 Simulation through Quadraphonic Spatialization: The Chowning Model\nTo extend spatial simulation beyond the limitations of stereophony—typically constrained to a 90° field—or to achieve finer localization resolution, the intensity panning technique used with two loudspeakers can be generalized to multi-channel systems.\n\n\n\nFig.\n\n\nBy placing four loudspeakers at the corners of a square, one can simulate a 360° horizontal sound field, allowing for a full circular distribution of a virtual source. Arranging six speakers in a hexagonal layout also provides 360° coverage, with improved localization accuracy. As a general rule, more channels yield higher resolution and spatial definition, though this comes at the cost of increased system complexity. From a practical standpoint, quadraphonic configurations have proven to offer a compelling balance between implementation feasibility and perceptual quality, and have thus gained wide acceptance in spatial sound simulation.\nOne of the most well-known models in this domain is the John Chowning quadraphonic model. It features:\n\nDirect sound simulation from a virtual source\nLocal reverberation, individually tailored for each speaker based on the source’s angle\nGlobal reverberation shared across all four speakers\n\n\n6.6.1 Amplitude and Distance Scaling\nThe audio input signal is first attenuated according to the inverse of the distance:\n\\[\nA_{\\text{direct}} = \\frac{1}{D}\n\\]\nThen, the signal is scaled by a coefficient specific to each speaker, depending on the virtual source’s position:\n\\[\nA_i = \\frac{1}{D} \\times C_i(\\theta)\n\\]\nwhere:\n\n\\(A_i\\) is the amplitude for speaker \\(i\\),\n\\(D\\) is the distance between the source and the listener,\n\\(C_i(\\theta)\\) is the coefficient for speaker \\(i\\) as a function of the source angle \\(\\theta \\in [0^\\circ, 360^\\circ]\\),\nand the speaker coefficients are labeled LRA, LFA, RFA, RRA (Left Rear, Left Front, Right Front, Right Rear).\n\n\n\n6.6.2 Gain Function and Phase Offsets\nTo avoid complex calculations of gain for each speaker, Chowning implements a predefined function, stored as a lookup table of 512 samples. The table’s function ranges between 0 and 1 and is designed to generate continuous 360° source motion through intensity panning.\nEach speaker reads the same function with a phase offset:\n\nThe first quarter of the function represents a sine wave from \\(0^\\circ\\) to \\(90^\\circ\\)\nThe second quarter is a cosine wave over the same angular range\nThe remaining half is silent (zero amplitude)\n\nAs the virtual source rotates, only two speakers are active at a time, crossfading between them. For instance, as the source moves from \\(theta = 90^\\circ\\) to \\(\\theta = 0^\\circ\\), the cosine-driven speaker fades out while the sine-driven speaker fades in, mimicking a continuous spatial trajectory.\n\n\n6.6.3 Reverberation Modeling\nIn the lower part of Chowning’s model (see again Figure G.8.9), reverberation is processed separately: - The reverberation signal is attenuated by the square root of the inverse of the distance:\n\\[\nA_{\\text{reverb}} = \\sqrt{\\frac{1}{D}} \\times \\text{PRV}\n\\]\nwhere PRV is an empirical parameter representing the reverberation percentage.\n\nA global reverb signal is scaled again by \\(\\frac{1}{D}\\)\nLocal reverberation is further attenuated by \\(1 - \\frac{1}{D}\\), then scaled using the same angle-dependent coefficients (LRA, LFA, RFA, RRA)\n\nTo increase realism, four independent reverberation units are used—one for each loudspeaker—ensuring spatial decorrelation and more convincing immersion.\nThis model not only exemplifies how mathematical reasoning and spatial design converge in sound synthesis, but also stands as a landmark in algorithmic composition and immersive audio. Through relatively simple mathematical operations and efficient data structures (such as phase-offset lookup tables), it offers a powerful foundation for dynamic spatial control in real-time systems such as Pure Data.\nHere’s how to think about the solution:\n\n\n6.6.4 Coordinate System\nThe circle object gives you Cartesian coordinates (x, y) in range [-1, 1], but you need:\n\nPolar coordinates (angle, radius)\nScaled values (0-359° for azimuth, 1-10 for distance)\n\n\n\n6.6.5 Mathematical Conversion\n\n6.6.5.1 Azimuth (Angle) Calculation\nWe need to convert (x, y) to angle using arctangent using:\n[expr atan2($f2, $f1) * 180 / 3.14159]\nKey points:\n\natan2(y, x) gives angle in radians (-π to π)\nMultiply by 180/π to convert to degrees (-180° to 180°)\nAdd 360° if negative to get 0-360° range:\n\n[expr (atan2($f2, $f1) * 180 / 3.14159 + 360) % 360]\n\n\n6.6.5.2 Distance Calculation\nConvert (x, y) to radius using Pythagorean theorem, then scale:\n[expr sqrt($f1*$f1 + $f2*$f2) * 9 + 1]\nLogic:\n\nsqrt(x² + y²) gives radius in range [0, 1]\nMultiply by 9 and add 1 to get range [1, 10]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Space</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#references",
    "href": "chapters/space.html#references",
    "title": "6  Space",
    "section": "References",
    "text": "References\n\n\n\n\nAbregú, E. L., E. R. Calcagno, and R. O. Vergara. 2012. “La Distancia Como Factor Estructural de La Música.” Revista Argentina de Musicología, no. 12-13: 379–400.\n\n\nBasso, Gustavo, Oscar Pablo Di Liscia, and Juan Pampin, eds. 2009. Música y Espacio: Ciencia, Tecnología y Estética. Buenos Aires: Editorial de la Universidad Nacional de Quilmes.\n\n\nKnudsen, V. O. 1932. Architectural Acoustics. J. Wiley y Sons.\n\n\nRisett, J. C. 1969. “An Introductory Catalogue of Computer Synthesized Sounds.” Nueva Jersey: Bell Telephone Laboratories.\n\n\nSchaeffer, Pierre. 2003. Tratado de Los Objetos Musicales. Translated by Araceli Cabezón de Diego. Alianza Editorial.\n\n\nStockhausen, Karlheinz. 1959. “Musik Im Raum.” Die Reihe, no. 5.\n\n\nWishart, Trevor. 1996. On Sonic Art. Amsterdam: Harwood Academic Publishers.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Space</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html",
    "href": "chapters/conclusion.html",
    "title": "7  Conclusion",
    "section": "",
    "text": "7.1 Key Takeaways\nIn this chapter, we summarize the key points discussed throughout the book. We reflect on the main topics introduced in the first chapter and the detailed discussions in the second chapter.\nThank you for reading! We hope this book has provided valuable insights and knowledge.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html#key-takeaways",
    "href": "chapters/conclusion.html#key-takeaways",
    "title": "7  Conclusion",
    "section": "",
    "text": "Highlight the main objectives of the book.\nDiscuss the significance of the topics covered.\nEncourage further exploration and learning in the subject area.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "References",
    "section": "",
    "text": "Abregú, E. L., E. R. Calcagno, and R. O. Vergara. 2012. “La\nDistancia Como Factor Estructural de La Música.” Revista\nArgentina de Musicología, no. 12-13: 379–400.\n\n\nBasso, Gustavo, Oscar Pablo Di Liscia, and Juan Pampin, eds. 2009.\nMúsica y Espacio: Ciencia, Tecnología y Estética. Buenos Aires:\nEditorial de la Universidad Nacional de Quilmes.\n\n\nBrown, Andrew. 2023. “Live Coding Patterns and a Toolkit for Pure\nData.” Organised Sound 28: 1–12. https://doi.org/10.1017/S1355771823000365.\n\n\nBrummitt, Charles D., and Eric Rowland. 2012. “Boundary Growth in\nOne-Dimensional Cellular Automata.” Complex Systems 21:\n85–116. https://doi.org/10.48550/arXiv.1204.2172.\n\n\nCanossa, Alessandro, Luis Laris Pardo, Michael Tran, and Alexis Lozano\nAngulo. 2022. “From Data Humanism to Metaphorical Visualization–an\nEducational Game Case Study.” In International Conference on\nHuman-Computer Interaction, 109–17. Cham: Springer.\n\n\nEpstein, Paul. 1986. “Pattern Structure and Process in Steve\nReich’s Piano Phase.” The Musical Quarterly 72 (4):\n494–502.\n\n\nGardner, Martin. 1970. “MATHEMATICAL GAMES - the Fantastic\nCombinations of John Conway’s New Solitaire Game Life.”\nScientific American 223 (4): 120–23. http://www.jstor.org/stable/24927642.\n\n\nGresham-Lancaster, Scot. 2012. “Relationships of Sonification to\nMusic and Sound Art.” AI and Society 27 (2): 207–12.\n\n\nHallnäs, Lars, and Johan Redström. 2001. “Slow\nTechnology–Designing for Reflection.” Personal and Ubiquitous\nComputing 5: 201–12.\n\n\nHasse, Sarah. 2012. “I Am Sitting in a Room.” Body,\nSpace & Technology 11. https://doi.org/10.16995/bst.71.\n\n\nHelmholtz, Hermann. 1954. On the Sensations of Tone as a\nPhysiological Basis for the Theory of Music. 2nd ed. New York:\nDover Publications.\n\n\nKalbande, Swaraj. 2022. “Conway’s Game of Life – Life on\nComputer.” Medium. https://medium.com/@swarajkalbande123/conways-game-of-life-life-on-computer-b7edfc85d21a.\n\n\nKim, Nam Wook, Hyejin Im, Nathalie Henry Riche, Alicia Wang, Krzysztof\nGajos, and Hanspeter Pfister. 2019. “Dataselfie: Empowering People\nto Design Personalized Visuals to Represent Their Data.” In\nProceedings of the 2019 CHI Conference on Human Factors in Computing\nSystems, 1–12. New York, NY, USA: Association for Computing\nMachinery.\n\n\nKnudsen, V. O. 1932. Architectural Acoustics. J. Wiley y Sons.\n\n\nLévi-Strauss, Claude. 1964. Le Cru Et Le Cuit. Vol. 1.\nMythologiques. Paris: Plon.\n\n\nLupi, Giorgia. 2017. “Data Humanism: The Revolutionary Future of\nData Visualization.” PrintMag. https://www.printmag.com/article/data-humanism-future-of-data-visualization/.\n\n\nReich, Steve. 2002. “Music as a Gradual Process.” In\nWritings on Music 1965–2000, 34–36. Oxford; New York: Oxford\nUniversity Press.\n\n\nRisett, J. C. 1969. “An Introductory Catalogue of Computer\nSynthesized Sounds.” Nueva Jersey: Bell Telephone Laboratories.\n\n\nSchaeffer, Pierre. 2003. Tratado de Los Objetos Musicales.\nTranslated by Araceli Cabezón de Diego. Alianza Editorial.\n\n\nSterne, Jonathan, and Tara Rodgers. 2011. “Poetics of Signal\nProcessing.” Differences 22 (2-3): 31–53. https://doi.org/10.1215/10407391-1428834.\n\n\nStockhausen, Karlheinz. 1959. “Musik Im Raum.” Die\nReihe, no. 5.\n\n\nThéberge, Paul. 1997. Any Sound You Can Imagine: Making\nMusic/Consuming Technology. Hanover & London: Wesleyan\nUniversity Press.\n\n\nToussaint, Godfried. 2005. “The Euclidean Algorithm Generates\nTraditional Musical Rhythms.” In, 47–56.\n\n\nWishart, Trevor. 1996. On Sonic Art. Amsterdam: Harwood\nAcademic Publishers.\n\n\nWolfram, Stephen. 1983. “Statistical Mechanics of Cellular\nAutomata.” Reviews of Modern Physics 55 (3): 601–44. https://doi.org/10.1103/RevModPhys.55.601.",
    "crumbs": [
      "References"
    ]
  }
]