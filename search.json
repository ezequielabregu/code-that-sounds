[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Code that Sounds",
    "section": "",
    "text": "Preface\nThe advent of computers has granted an unprecedented degree of precision and creative freedom in the manipulation of sound—freedom that remains unattainable with traditional acoustic instruments. In the digital realm, sound is no longer bound by the physical constraints of its sources; it becomes a malleable entity, capable of being shaped, transformed, and reimagined in virtually limitless ways. One could argue that while the traditional composer writes with sounds, the digital artist writes the sounds themselves, constructing sonic material from the ground up rather than selecting from preexisting timbres. This shift marks a fundamental transformation in the imaginary process, where the manipulation of sound at its structural level becomes a form of creative authorship. Over time, a common thread emerged: the desire to bridge artistic expression and technical skill, to write code not just as a means to an end, but as a space of exploration, experimentation, and play. These developments are made possible by the transition from analog to digital representation. In the digital domain, sounds are translated into numerical data, which can be systematically analyzed, modified, and recombined. This process—known as digitization—converts sensory or textual information (whether a sound, image, or written word) into discrete numerical sequences, enabling computers to process and transform them with extraordinary speed and flexibility.\nIn these pages, you’ll find the distilled insights, exercises, and creative strategies that have shaped countless workshops and academic programs. The goal has always been twofold: to equip readers with the tools to build interactive digital systems, and to nurture a mindset where sound, code, and structure can be explored as artistic materials. Whether it’s a generative soundscape, a data-driven artwork, or a custom tool built from scratch, the projects in this book are designed to foster technical growth while encouraging individual expression.\nThis is not a manual in the traditional sense, nor is it a fixed curriculum. Instead, this book invites you to engage on your own terms, navigating its chapters in whatever order best suits your curiosity. The modular structure is intentional: it supports creative detours, sudden insights, and unexpected connections between ideas. You are encouraged to experiment, remix, and stretch the boundaries of the examples presented. Treat the book as both a guide and a sandbox—one where art, sound, code, and interactivity come alive through your engagement. Whether you’re an artist exploring new tools, a programmer seeking creative outlets, or a student diving into the world of interactive media, I hope this book helps you discover how code can become a language for imagination.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-am-i",
    "href": "index.html#who-am-i",
    "title": "Code that Sounds",
    "section": "Who am I?",
    "text": "Who am I?\nMy name is Ezequiel Abregú, and I am a sound artist, composer, multi-instrumentalist, and researcher originally from Buenos Aires, Argentina. My artistic practice encompasses sound recordings, audio installations, performances, sound sculptures, sound design, and compositions for chamber music, choreography, and theater. I am particularly interested in the interplay between music, performance, sound art, live electronics, auditory and visual perception, interactive media, and the application of technology in art.\nI hold a Ph.D. focusing on the relationship between visual and auditory perception in sound art, and a degree in Composition with Electroacoustic Media from the National University of Quilmes (UNQ). My academic career includes teaching positions at several institutions1: I am a professor at the University of Quilmes (Computing Applied to Music area), the National University of Arts (Multimedia Arts area), and the University of Tres de Febrero (Electronic Arts area).\nMy passion for programming and digital audio applications has led me to explore various programming languages and tools over the past two decades, including C, C++, Python, and Pure Data. I am an advocate of the open-source philosophy, regularly working with Linux and sharing my projects in publicly accessible repositories. My technical expertise extends to hardware development using microcontrollers and single-board computers, enabling me to adopt a hands-on approach in both my artistic and research endeavors.\n\n\n\n\nEzequiel Abregú\n\n\n\n\n\n\n\n\n\nMore information about my work can be found on my personal website ezequielabregu.net",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "Code that Sounds",
    "section": "What is this book about?",
    "text": "What is this book about?\nThe goal of this book is to support readers in exploring the intersection of creativity and technology in collaboration with peers from diverse backgrounds. It encourages the development of a creative mindset alongside programming skills to design original tools, algorithms, and artistic works. Through this synthesis, the book invites readers to engage with sound, interactivity, and control protocols—addressing both their technical implementation and expressive potential.\nRather than offering a fixed, linear progression, the structure of this book is deliberately open and modular. Readers are encouraged to navigate the content according to their interests, needs, or curiosity. This flexibility supports an experimental approach to learning, where exploration and play are not only welcome but essential.\nBy approaching programming as a creative practice, this book invites you to think, make, and reflect through code. Whether you’re building an interactive sound installation, prototyping a digital instrument, or simply experimenting with new ideas, the goal is to empower you with the tools and concepts to express yourself in the digital domain—and to enjoy the process along the way.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-code-that-sounds",
    "href": "index.html#why-code-that-sounds",
    "title": "Code that Sounds",
    "section": "Why Code That Sounds?",
    "text": "Why Code That Sounds?\nAt first glance, the title Code That Sounds may seem literal—code that produces audio. But it also opens a conceptual field that reflects the spirit of this book: the convergence of computation, sound, and creative expression. We treat code not only as a functional tool for generating and processing sound, but as a medium for composition, experimentation, and discovery.\nThe title emphasizes the act of coding as a means to produce sound—transforming logical structures into audible forms, from simple sine waves to complex interactive systems. But it also gestures toward a deeper, more poetic reading. In English, “sounds” doesn’t only refer to audio—it also implies resonance, meaning, and sensibility. A line of code can sound right, not just technically but aesthetically. This expressive quality reveals code as a language with tone, rhythm, and intention.\nThrough this lens, Code That Sounds positions programming as a creative act—part design, part composition. It invites us to explore how code becomes an audible thought, a structure that lives and reacts, especially in interactive systems. The code doesn’t just produce sound—it performs it.\nThis book is shaped by that ethos. We approach creative coding not simply as a technical challenge, but as a practice of listening, of composing with systems, of learning through making. Code That Sounds is both our method and our invitation: to write, to hear, and to imagine what code can become.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#from-problem-to-algorithm",
    "href": "index.html#from-problem-to-algorithm",
    "title": "Code that Sounds",
    "section": "From Problem to Algorithm",
    "text": "From Problem to Algorithm\nOne of the central ideas guiding this book is that programming begins not with a tool, but with a question. Before we concern ourselves with implementation, we start by identifying a problem—something that needs to be understood, shaped, or transformed. This problem might be technical (how to generate a rhythm evenly distributed over time?), perceptual (how to spatialize a sound to suggest motion?), or poetic (how to evoke a sense of disorientation through modulation?). Each chapter begins with such a prompt, inviting us to investigate the mechanics and metaphors behind the systems we aim to build.\nFrom this point, the process becomes analytical and compositional. We break down the problem into its smallest components, identify the parameters and constraints at play, and explore the structures—logical, mathematical, sonic—that underpin it. This leads us to design an algorithm: a set of steps, a conceptual model, or a generative rule that connects the problem to a potential solution. The algorithm is not just a procedure—it is a form of reasoning, a translation of intuition into system.\nOnly once this conceptual groundwork has been laid do we turn to implementation. The choice of environment becomes relevant, but always in service of the larger question. It is not simply learning how to use a tool; you are using the tool to articulate a thought. This means thinking through code, not just writing it. It means seeing your patch not as an end, but as a hypothesis, a sketch, an evolving structure open to revision.\nThroughout the book, I invite you to cultivate this mindset. When studying an example patch, don’t ask only what it does—ask how it solves a problem. What are the assumptions embedded in its logic? Could the same result be achieved differently? What happens if you modify the rules, invert the process, or apply the algorithm in another context?\nBy working in this way, programming becomes more than an exercise in execution. It becomes a space of inquiry and invention—an extension of your thinking, your listening, your compositional practice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Code that Sounds",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book is for anyone drawn to the idea that code can become an expressive, sonic, and artistic material. It is designed for students, artists, educators, and curious minds—regardless of their technical background—who wish to explore programming as a medium for crafting sound, interactivity, and performance. Whether you’re new to creative coding or already familiar with digital media, this book will guide you through approaches that treat code as more than just a set of instructions. In this context, programming becomes a mode of listening, composing, and experimentation. While the programming language provides the technical environment, our primary focus is on developing a creative and critical practice centered on sound and code.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-you-going-to-learn",
    "href": "index.html#what-are-you-going-to-learn",
    "title": "Code that Sounds",
    "section": "What are you going to learn?",
    "text": "What are you going to learn?\nThroughout this book, you will learn how to develop and shape sonic experiences using code. You will build interactive systems that respond, transform, and sound—systems that move beyond the screen and into space, time, and perception. Along the way, you will engage with fundamental techniques of sound synthesis, algorithmic thinking, and generative design, all within a context that values exploration, composition, and artistic intent. Beyond tools and syntax, you will learn to listen to your code, to let it surprise you, and to use it as a means for creative discovery and auditory expression.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-you-not-going-to-learn",
    "href": "index.html#what-are-you-not-going-to-learn",
    "title": "Code that Sounds",
    "section": "What are you not going to learn?",
    "text": "What are you not going to learn?\nThis book is not a manual or reference guide. Rather than aiming for completeness, we prioritize depth over breadth: we focus on specific concepts, practices, and case studies that support an artistic and experimental approach. You won’t find here every object or external library documented, nor step-by-step instructions for GUI design or audio engineering. Instead, you will find a flexible, hands-on framework to begin crafting systems that sound, and from there, make them your own. This book works best in dialogue with other resources and invites you to learn by making, remixing, and listening.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#a-work-in-progress",
    "href": "index.html#a-work-in-progress",
    "title": "Code that Sounds",
    "section": "A Work in Progress",
    "text": "A Work in Progress\nBefore we delve deeper into the subject, let it be clear from the outset:\n\n\nThis book is, by design, a work in progress.\n\n\nThe intention is not to present a definitive compendium or a closed collection of recipes. Instead, what I offer here is a growing body of materials, case studies, and conceptual tools that evolve in parallel with the creative and technical challenges faced by artists and researchers working at the intersection of code, sound, and interactivity. The tools used throughout this book are chosen for their openness, adaptability, and ability to support rapid prototyping with conceptual transparency. More importantly, this book aspires to offer a framework for thinking—an approach to creative problem-solving that extends beyond the act of coding itself.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#backend-first-leaving-the-gui-for-later",
    "href": "index.html#backend-first-leaving-the-gui-for-later",
    "title": "Code that Sounds",
    "section": "Backend First: Leaving the GUI for Later",
    "text": "Backend First: Leaving the GUI for Later\nAnother important methodological decision in this book is our emphasis on the backend. We begin by focusing on signal flow, algorithmic thinking, and control structures—those components that shape the inner workings of a patch. In many cases, the graphical interface (GUI) can be a distraction from the deeper mechanics at play.\nBy concentrating first on backend logic, we build a strong foundation that can later support more refined user interactions. GUI design will certainly be addressed, but in future chapters, when we are equipped with a clearer understanding of how our systems behave and how we want them to evolve. In other words, we treat the visual layer as a representation of logic, not a replacement for it.\nThis book is an invitation to approach creative coding not as a set of shortcuts or pre-made solutions, but as a process of inquiry. We will get our hands dirty, make mistakes, and revise along the way. In doing so, we learn not just how to build systems, but how to think with and through them. That is the deeper promise of creative code.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#book-structure",
    "href": "index.html#book-structure",
    "title": "Code that Sounds",
    "section": "Book Structure",
    "text": "Book Structure\n\n\n\n\n\nflowchart LR\n    A[Code that Sounds] --&gt; B[Preface]\n    B --&gt; B1[Who am I?]\n    B --&gt; B2[What is this book about?]\n    B --&gt; B3[Why Code That Sounds?]\n    B --&gt; B4[From Problem to Algorithm]\n\n    \n    A --&gt; C[1 Introduction]\n    C --&gt; C1[What is Pure Data?]\n    C --&gt; C2[Why Pd?]\n    C --&gt; C3[What is Creative Coding?]\n    C --&gt; C4[Getting Started]\n    \n    A --&gt; D[2 Sequencing]\n    D --&gt; D1[Arrays and Sequencers]\n    D --&gt; D2[8-Step Sequencer Implementation]\n    D --&gt; D3[Random Step Sequencer]\n    D --&gt; D4[Piano Phase - Steve Reich]\n    D --&gt; D5[Random Melody Generator]\n    D --&gt; D6[Euclidean Algorithm]\n    D --&gt; D7[Cellular Automata]\n    \n    A --&gt; E[3 Rec & Play]\n    E --&gt; E1[I'm Sitting in a Room - Alvin Lucier]\n    E --&gt; E2[I am sitting in a freeverb~]\n    E --&gt; E3[Simple Audio Player]\n    E --&gt; E4[Audio Sampler with phasor~]\n    \n    A --&gt; F[4 Sound Synthesis]\n    F --&gt; F1[Introduction & Oscillatory Movements]\n    F --&gt; F2[Sound Sources]\n    F --&gt; F3[Additive Synthesis]\n    F --&gt; F4[Amplitude & Ring Modulation]\n    F --&gt; F5[Frequency Modulation]\n    F --&gt; F6[Subtractive Synthesis]\n    \n    A --&gt; G[5 Sonification]\n    G --&gt; G1[Introduction & Data Types]\n    G --&gt; G2[Sonification as Creative Framework]\n    G --&gt; G3[Data Sonification Artworks Database]\n    G --&gt; G4[Data Humanism - Giorgia Lupi]\n    G --&gt; G5[Electrocardiogram Data]\n    G --&gt; G6[ECG-Controlled Step Sequencer]\n    G --&gt; G7[Image Sonification with RGBA Data]\n    G --&gt; G8[Image Scanner]\n    G --&gt; G9[Camera Scanner]\n    \n    A --&gt; H[6 Spatial Audio]\n    H --&gt; H1[The Music Sound Space]\n    H --&gt; H2[Spatial Localization of Sound]\n    H --&gt; H3[Hearing in Enclosed Spaces]\n    H --&gt; H4[Virtual Source Simulation]\n    H --&gt; H5[Stereo Spatialization Implementation]\n    H --&gt; H6[Simulating Distance]\n    H --&gt; H7[Enhanced Distance Modeling]\n    H --&gt; H8[Quadraphonic Spatialization: Chowning Model]\n    \n    A --&gt; I[7 Conclusion]\n    I --&gt; I1[A Synthesis of Themes]\n    I --&gt; I2[Sound, Code, and Interaction]\n    I --&gt; I3[Open Tools, Open Futures]\n    I --&gt; I4[Toward What Comes Next]\n    \n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#fff3e0\n    style D fill:#e8f5e8\n    style E fill:#fce4ec\n    style F fill:#f1f8e9\n    style G fill:#e0f2f1\n    style H fill:#fafafa\n    style I fill:#ffebee",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Code that Sounds",
    "section": "Contributing",
    "text": "Contributing\nIf you would like to contribute to this book, please feel free to fork the repository and submit a pull request. I welcome any suggestions, corrections, or improvements to the content. You can also report issues or request features by opening an issue in the repository. You can find the source code for this book on GitHub.\n\n\n\n\n\n\nNote\n\n\n\nPlease, do not write me emails with questions!\nIf you have questions, post them on the Pure Data forum, the Pd mailing lists, or the Facebook Pd Group.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Code that Sounds",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to express my gratitude to the following individuals and organizations for their support, feedback and contributions to this book:\nMartín Matus Lerner, Pablo Freiberg.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Code that Sounds",
    "section": "License",
    "text": "License\nThis book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. You are free to share and adapt the material, provided you give appropriate credit, do not use it for commercial purposes, and distribute your contributions under the same license.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Code that Sounds",
    "section": "",
    "text": "Multimedia Arts UNA, Electronic Arts UNTREF, Bachelor of Music and Technology UNQ, Master’s Degree in Sound Art UNQ, Doctorate in Arts UNA↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Pure Data?\nTraditionally, software development has relied on text-based programming languages, where code is written line by line and later executed to reveal its results. While efficient and powerful for those with formal training, this approach often poses a significant barrier to artists, musicians, and creative practitioners who are less familiar with symbolic logic or syntactic precision. The abstract nature of traditional code can be daunting—especially for those who are more at home in visual, embodied, or sensory modes of making.\nAn alternative paradigm has emerged through graphical and modular environments, where functions are represented as visual elements and assembled through spatial composition rather than text. In this model, creative practitioners can construct interactive systems by literally drawing connections between components—building programs as if sketching ideas in space. Such environments privilege immediacy and iteration, allowing users to hear, see, and modify the behavior of systems in real time. This shift not only enhances accessibility, but also aligns with tactile and intuitive forms of creative inquiry.\nThe design principles behind these tools often resonate with the traditions of analog modular synthesis, where sonic processes were shaped by routing signals through a network of interconnected modules. The visual and spatial logic of such systems enables a fluid interplay between structure and experimentation. Translating this model into a digital environment supports an approach to coding that feels more like composing, sculpting, or improvising—where signal flow and behavior are not predefined but discovered through interaction.\nWorking within such environments encourages a rethinking of programming itself—not merely as an engineering task, but as a medium for composing with systems. Whether developing real-time sound processors, generative structures, or interactive instruments, the act of coding becomes a form of sonic thinking. Algorithms are not just functional mechanisms, but artistic gestures; control flows become expressive tools. In this way, creative coding fosters a hybrid mindset that embraces both analytical precision and artistic openness, offering a powerful foundation for experimentation at the intersection of sound, interaction, and computation.\nPure Data (or Pd) is a real-time graphical programming environment for audio, video, and graphical processing. Pd is commonly used for live music performance, VeeJaying, sound effects, composition, audio analysis, interfacing with sensors, using cameras, controlling robots or even interacting with websites. Because all of these various media are handled as digital data within the program, many fascinating opportunities for cross-synthesis between them exist. Sound can be used to manipulate video, which could then be streamed over the internet to another computer which might analyze that video and use it to control a motor-driven installation.\nProgramming with Pd is a unique interaction that is much closer to the experience of manipulating things in the physical world. The most basic unit of functionality is a box, and the program is formed by connecting these boxes together into diagrams that both represent the flow of data while actually performing the operations mapped out in the diagram. The program itself is always running, there is no separation between writing the program and running the program, and each action takes effect the moment it is completed.\nThe community of users and programmers around Pd have created additional functions (called “externals” or “external libraries”) which are used for a wide variety of other purposes, such as video processing, the playback and streaming of MP3s or Quicktime video, the manipulation and display of 3-dimensional objects and the modeling of virtual physical objects. There is a wide range of external libraries available which give Pd additional features. Just about any kind of programming is feasible using Pd as long as there are externals libraries which provide the most basic units of functionality required.\nThe core of Pd written and maintained by Miller S. Puckette and includes the work of many developers, making the whole package very much a community effort.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#what-is-pure-data",
    "href": "chapters/introduction.html#what-is-pure-data",
    "title": "1  Introduction",
    "section": "",
    "text": "A basic “Hello World” example in Pd",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#why-pd",
    "href": "chapters/introduction.html#why-pd",
    "title": "1  Introduction",
    "section": "1.2 Why Pd?",
    "text": "1.2 Why Pd?\nPd is a powerful, open-source environment for creative coding, offering a uniquely visual approach to programming that is especially suited for artists, musicians, and interactive media designers. Its intuitive interface and modular structure make it a flexible and accessible tool for developing real-time audio and visual projects—from live performances to experimental installations.\nOne of Pd’s standout features is its graphical programming interface, which replaces traditional lines of code with visual objects and patch cords. This allows users to construct complex behaviors by connecting elements on screen, making it easier to prototype and refine creative ideas. Real-time processing capabilities mean that audio and visual data can be generated, modified, and responded to instantly—ideal for performances, generative art, or interactive systems that react to sensors or user input.\nBeyond its creative potential, Pd offers practical advantages: it is cross-platform, running on Windows, macOS, and Linux, and integrates easily with tools like Arduino, Raspberry Pi, and Max/MSP, enabling hybrid systems that combine digital and physical components. A rich ecosystem of external libraries supports advanced functions like synthesis, visualization, and computer vision. Its open-source nature encourages exploration and collaboration, with a supportive community, extensive documentation, and countless tutorials available. Because it’s free and widely used in education, Pd is not only an effective tool for artistic expression but also a valuable learning resource for developing a strong foundation in programming and multimedia design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#what-is-creative-coding",
    "href": "chapters/introduction.html#what-is-creative-coding",
    "title": "1  Introduction",
    "section": "1.3 What is Creative Coding?",
    "text": "1.3 What is Creative Coding?\nCreative coding is the practice of using programming as a tool for artistic expression. It transforms code from a purely functional medium into a creative one, enabling the development of visual art, music, interactive experiences, and experimental media. This approach encourages artists, designers, and technologists to explore beyond the limits of traditional art forms, embracing code as a flexible and dynamic means of invention and communication.\nThe applications of creative coding are diverse, ranging from generative visuals and algorithmic design to responsive installations and live audiovisual performances. Creators often use languages and environments specifically designed to support creative work, such as Processing, OpenFrameworks, Max/MSP, and Pd. These platforms make it easier to manipulate data, control media in real time, and experiment with unconventional interfaces and outputs.\nImportantly, creative coding transcends the boundaries of specific disciplines. It can intersect with visual art, music, dance, theater, architecture, and even narrative writing. What unites these practices is the use of code as an expressive tool—one that invites innovation, play, and conceptual exploration. Closely tied to the values of the open-source movement, creative coding thrives in a culture of sharing, where artists and developers freely exchange code, tutorials, and ideas. This collaborative ecosystem fosters continuous learning and reinvention, empowering creators to expand what is possible through digital technology.\nFor an in-depth exploration of creative coding, consider checking out these resources:\n\nAwesome Creative coding - A curated list of resources, libraries, and tools for creative coding.\nCreative Code Berlin - A collection of creative coding resources, tutorials, and projects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#getting-started",
    "href": "chapters/introduction.html#getting-started",
    "title": "1  Introduction",
    "section": "1.4 Getting Started",
    "text": "1.4 Getting Started\nThis section will guide you through the installation of Pd Vanilla, the official and minimal distribution of Pd, on Windows, macOS, and Linux systems. The installation process is straightforward and will have you up and running in no time.\nTo get the most out of this book, you should have a basic understanding of programming and some familiarity with the Pd language.\n\n1.4.1 Tools, Manuals, and Resources\nIf you are new to Pd or programming, there are many online resources that can help you get started. Pd may initially appear intimidating—its blank, minimalistic interface offers no hints or prompts, and users are often left wondering where to begin. What objects are available? How do they function? This curated list of resources is intended to guide newcomers and experienced users alike by offering clear pathways into the Pd ecosystem.\n\n\n\nResource\nAuthor/Source\nDescription\n\n\n\n\nPd Tutorials Browser\nBuilt-in\nBeginner tutorials within Pd (Help → Browser → Manuals) covering audio, visuals, networking\n\n\nObject Help Patches\nBuilt-in\nContextual help for any object via right-click → Help, with embedded examples\n\n\nPd FLOSS Manual\nCommunity\nBeginner-friendly comprehensive guide covering installation to advanced techniques\n\n\nOfficial Pd Documentation\nMiller Puckette\nDefinitive reference by Pd’s creator, accessible via Help → HTML Manual\n\n\nTheory and Techniques of Electronic Music\nMiller Puckette\nCanonical text accompanying Pd’s audio examples, focuses on DSP concepts\n\n\npuredata.org\nOfficial\nMain website with news, documentation, FAQs, and community resources\n\n\nYouTube/Vimeo Tutorials\nVarious\nVideo tutorials with step-by-step demonstrations\n\n\nPatchstorage\nCommunity\nPlatform for sharing and discovering Pd patches from beginner to advanced\n\n\nPd Workshop Files\nMax Neupert, Frank Barknecht\nWorkshop materials including motion detection examples\n\n\nDesigning Sound\nAndy Farnell\nPatches and tutorials focused on procedural audio and sound design\n\n\nLoadbang\nJohannes Kreidler\nOnline tutorial and book with practical creative coding projects\n\n\nKunstuniversität Graz\nIEM Graz\nExtensive collection of tutorials and example patches\n\n\nMultimedia Programming with Pd\nBryan Chung\nBook covering digital media authoring: images, animations, audio, videos\n\n\nElectronic Music and Sound Design Vol. 1\nBianchi, Cipriani, Giri\nTheory and practice overview with glossary and student evaluation tests\n\n\nLive Electronics Tutorial\nAlexandre Torres Porres\nTheory and practice of live electronics and computer music synthesis/DSP\n\n\nProgramming Sound with Pure Data\nTony Hillerson\nIntroduces and explores Pure Data, building understanding of sound design concepts along the way\n\n\nEkran.org Gem Tutorials\nEkran.org\nTutorials on Gem for audiovisual composition and\n\n\n\n\n\n1.4.2 Recommended Pd distributions\nThis book is based on Pd Vanilla distribution, which is the most widely used version. You can download it from the Pd official website.\n\n\n\nPd Vanilla distribution\n\n\nThis version is maintained by the original author, Miller Puckette1, and is the most stable supported version of Pd. It includes all the core features and libraries needed for most projects, making it a great starting point for beginners and experienced users alike. Pd Vanilla is a free, open-source and available for Windows, macOS, and Linux operating systems.\nIn addition to Pd Vanilla, there are several other distributions and versions of Pd that you may find useful:\n\nPurr Data – A fork of Pd-l2ork that focuses on usability and accessibility, with a more polished interface and additional features. Purr Data serves the same purpose, but offers a new and much improved graphical user interface and includes many 3rd party plug-ins. Like Pd, it runs on Linux, macOS and Windows, and is open-source throughout.\n\n\n\n\nPurr Data distribution\n\n\n\nPlugdata – plugdata is a plugin wrapper designed for Miller Puckette’s Pd, featuring an enhanced graphical user interface (GUI) created using JUCE, headed by Timothy Schoen. this project is still a work in progress, and may still have some bugs. By default, plugdata comes with the cyclone and ELSE collections of externals and abstractions.\n\n\n\n\nPlugdata distribution\n\n\n\nPd Ceammc - a general purpose Pd distribution and library that is used for performance, sound-design and education purposes in Centre of electroacoustic music of Moscow Conservatory (CEAM).\n\n\n\n\nPd Ceammc distribution\n\n\n\n\n1.4.3 Recommended libraries and externals\nThe following libraries are recommended to be used with Pd. They are not included in the Pd Vanilla distribution, but they can be easily installed and used with it:\n\n\n\n\n\n\n\nLibrary\nDescription & Usefulness\n\n\n\n\ncyclone\nA large collection of Max/MSP-compatible objects. Useful for porting Max patches and advanced DSP tasks.\n\n\nELSE\nExtensive library for audio, control, math, and UI\n\n\niemlib\nCollection of general purpose objects and filters for Pd\n\n\nlist-abs\nTools for advanced list manipulation and data processing\n\n\nzexy\nEssential utilities for math, signal, and control operations. Adds many missing core features.\n\n\nGem\nGraphics Environment for Multimedia. Enables real-time visuals and video.\n\n\nceammc\nlibrary used for work and education purposes in Centre of electroacoustic music of Moscow Conservatory (CEAMMC) and ZIL-electro studio\n\n\ncomport\nSerial port communication. Useful for connecting Pd to Arduino, sensors, and hardware\n\n\nmrpeach\nNetworking and OSC support. Essential for interactive and networked Pd projects\n\n\nfreeverb~\nHigh-quality reverb effect. Simple way to add spatialization to audio patches\n\n\njmmmp\nCollection of GUI and utility objects. Enhances user interface design in Pd patches\n\n\nmapping\nTools for mapping data (e.g., sensors to sound). Useful for interactive installations and performances\n\n\n\n\nThese libraries provide additional objects and features that can enhance your projects and make it easier to work with sound synthesis, data visualization, and interactive installations.\n\n\n1.4.4 Installing Pd Vanilla\nThe following is a step-by-step installation of Pd Vanilla, the official and minimal distribution of Pd, on Windows, macOS, and Linux systems.\n\n\n1.4.5 Installing on Windows\n\n1.4.5.1 Download the Installer\n\nVisit the official Pd website: https://puredata.info/downloads/pure-data\nScroll down to the Windows section.\nClick the latest version link (e.g., pd-0.55-2.windows-installer.exe).\n\n\n\n1.4.5.2 Run the Installer\n\nLocate the downloaded .exe file in your Downloads folder.\nDouble-click to run the installer.\nFollow the installation wizard:\n\nChoose the installation location (default is fine).\nAllow the program to create a Start Menu shortcut.\n\n\n\n\n1.4.5.3 Verify Installation\n\nAfter installation, open Pd from the Start Menu or desktop shortcut.\nThe main Pd window should appear with a blank patch ready.\n\n\n\n\n1.4.6 Installing on macOS\n\n1.4.6.1 Download the Disk Image\n\nVisit: https://msp.ucsd.edu/software.html\nScroll to the macOS section.\nDownload the .dmg file (e.g., Pd-0.54-1.dmg).\n\n\n\n1.4.6.2 Install the Application\n\nOpen the downloaded .dmg file.\nDrag the Pd icon into the Applications folder.\n\n\n\n1.4.6.3 Open Pd\n\nOpen your Applications folder.\nRight-click (or Ctrl + click) the Pd icon and select Open.\n\nThe first time, macOS may warn that the app is from an unidentified developer.\nConfirm to proceed.\n\n\n\n\n1.4.6.4 Optional: Enable Audio Permissions\n\nIf prompted, allow Pd to access the microphone.\nOpen System Preferences &gt; Security & Privacy &gt; Microphone and ensure Pd is enabled.\n\n\n\n\n1.4.7 Installing on Linux\nPd is available in the package repositories of most major Linux distributions. Below are instructions for popular systems.\n\n1.4.7.1 Debian/Ubuntu-based Systems\nsudo apt update\nsudo apt install puredata\n\n\n1.4.7.2 Fedora\nsudo dnf install puredata\n\n\n1.4.7.3 Arch Linux\nsudo pacman -S puredata\n\n\n1.4.7.4 Verify Installation\n\nOpen a terminal and type pd.\nThe Pd GUI should launch, displaying a blank patch.\nIf you encounter issues, check your package manager or consult the Pd community for troubleshooting.\n\n\n\n\n1.4.8 Installing Externals\nPd’s functionality can be extended through the use of externals—additional libraries that provide new objects and features. These externals are created by the Pd community and can add everything from new audio effects to advanced data processing tools. Installing externals is straightforward thanks to Pd’s built-in package manager.\n\n1.4.8.1 1. Open Pd\n\nLaunch Pd on your computer.\nMake sure you are using the Vanilla version for best compatibility with the package manager.\n\n\n\n1.4.8.2 2. Access the “Find Externals” Tool\n\nIn the Pd menu, go to Help → Find Externals…\nThis opens the Deken package manager, which allows you to search for and install externals directly from within Pd.\n\n\n\n1.4.8.3 3. Search for an External\n\nIn the search bar, type the name of the external or library you want to install (for example, zexy, cyclone, or iemlib).\nPress Enter or click the Search button.\nA list of matching externals will appear, showing available versions for your platform.\n\n\n\n1.4.8.4 4. Install the External\n\nClick on the desired external in the list.\nChoose the version that matches your operating system and Pd version.\nClick Install.\nThe external will be downloaded and placed in your Pd externals folder (typically ~/Documents/Pd/externals on macOS and Linux, or C:\\Users\\&lt;YourName&gt;\\Documents\\Pd\\externals on Windows).\n\n\n\n1.4.8.5 5. Add the External to Your Pd Path (if needed)\n\nMost externals are automatically available after installation.\nIf Pd cannot find the external, you may need to add its folder to Pd’s search path:\n\nGo to Preferences → Path…\nClick New and add the path to the external’s folder (for example, ~/Documents/Pd/externals/cyclone).\nClick OK and restart Pd.\n\n\n\n\n1.4.8.6 6. Use the External in Your Patch\n\nIn your patch, create an object with the name of the external’s library, followed by the object you want to use.\nFor example, to use the counter object from the cyclone library:\n[cyclone/counter]\nSome libraries require you to load them with a special object, such as [declare -lib cyclone] at the top of your patch.\n\n\n\n1.4.8.7 7. Verify Installation\n\nIf the object appears without errors (no red box), the external is installed correctly.\nIf you see errors, double-check the installation path and that you are using the correct object/library name.\n\n\n\n1.4.8.8 Manual Installation (Advanced)\nIf you need to install an external manually:\n\nDownload the external from https://deken.puredata.info/ or the developer’s website.\nExtract the files to your Pd externals folder.\nAdd the folder to Pd’s path as described above.\n\n\nTip: Always check the documentation for each external, as installation steps or requirements may vary.\n\nFor more details, see the official Pd documentation on externals.\n\n\n\n1.4.9 Setting Up a Virtual MIDI Bus\nEstablishing a virtual MIDI bus allows MIDI messages to be routed between different applications on the same computer, or to create intricate internal MIDI configurations within applications. This setup is particularly useful for integrating software instruments, sequencers, or custom-built Pd patches into a single ecosystem, enabling real-time interaction across programs.\n\n1.4.9.1 macOS\nmacOS provides a native solution for virtual MIDI routing through the IAC Bus (Inter-Application Communication Bus), a built-in MIDI driver that allows you to create multiple virtual MIDI ports.\n\n\n\nImage of IAC Driver in MIDI Studio\n\n\nTo enable and configure the IAC Bus:\n\nOpen the Audio MIDI Setup application (found in /Applications/Utilities), then select Window → Show MIDI Studio from the top menu.\nIn the MIDI Studio window, double-click the IAC Driver icon to open its configuration panel.\nEnable the virtual bus by checking Device is online. This activates the driver.\nYou may rename the default port if desired, for better identification in your DAW or software environment.\nTo add additional virtual MIDI ports, click the + button. Each new bus functions as a separate virtual cable and can be independently addressed.\n\nOnce activated, these ports will appear as MIDI input and output options within your software (e.g., Pd, Live, Max), supporting the full range of MIDI messages including Note, Control Change, and MIDI Sync.\n\n\n1.4.9.2 Windows\nUnlike macOS, Windows does not include a built-in virtual MIDI bus. However, several reliable third-party tools are freely available:\n\nloopMIDI, developed by Tobias Erichsen, provides a robust solution for creating virtual MIDI ports and supports multiple concurrent buses. It is compatible with both 32-bit and 64-bit systems and is widely recommended for use with modern DAWs and programming environments.\n\n\n\n\nloopMIDI interface\n\n\n\nMIDI Yoke is a legacy solution bundled with MIDI-OX, offering basic virtual MIDI routing. However, MIDI Yoke is 32-bit only, and therefore not compatible with 64-bit software, including Live 10 and later. Use with caution and only in legacy setups.\n\nWhen setting up virtual MIDI buses in Windows, ensure that your audio software and any MIDI-routing utilities are operating with compatible bit-depths and permissions. Proper driver installation and restart may be necessary to expose the new virtual ports in your system’s MIDI routing options.\nVirtual MIDI buses are essential tools for building interactive sound systems, enabling communication between modular audio software, DAWs, and real-time instruments. Whether you are sending MIDI from Pd to a DAW, or routing control signals between applications, setting up a virtual MIDI infrastructure is a crucial foundational step.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Miller Puckette is a computer music researcher and the creator of Pd. He has been involved in the development of various software tools for music and multimedia, including Max/MSP and Pd. His work has had a significant impact on the field of computer music and interactive media. More information about Miller Puckette can be found on his website.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/sequencing.html",
    "href": "chapters/sequencing.html",
    "title": "2  Sequencing",
    "section": "",
    "text": "2.1 Arrays and Sequencers\nIn this chapter, we will explore the concept of sequencers and how they can be implemented in Pd. We will cover various types of sequencers, including step sequencers, random sequencers, and algorithmic composition techniques, with practical examples and implementations.\nSequencer is a tool for organizing and controlling the playback of events in a temporally ordered sequence. This sequence consists of a series of discrete steps, where each step represents a regular time interval and can contain information about event activation or deactivation. In addition to controlling musical events such as notes, chords, and percussions, a step sequencer can also manage a variety of other events. This includes parameter changes in virtual instruments or audio/video synthesizers, real-time effect automation, lighting control in live performances or multimedia installations, triggering of samples to create patterns and effect sequences, as well as firing MIDI control events to operate external hardware or software. The following table illustrates the basic structure of a step sequencer:\nWhere:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/sequencing.html#arrays-and-sequencers",
    "href": "chapters/sequencing.html#arrays-and-sequencers",
    "title": "2  Sequencing",
    "section": "",
    "text": "Step\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nValue\nx\nx\nx\nx\nx\nx\nx\nx\n\n\nIndex\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n\nEach cell represents a step in the sequencer.\n1 to 8 indicate the steps.\nEmpty cells represent steps without events or notes.\nYou can fill each cell with notes or events to represent the desired sequence.\n\n\n2.1.1 Formulation (pseudo code)\nData Structures:\n- Define a data structure to represent each step of the sequence (e.g., array, list).\n\nVariables:\n- Define an array to store the MIDI note values for each step.\n- Initialize the sequencer parameters:\n  - CurrentStep = 0\n  - Tempo\n  - NumberOfSteps = 8\n\nAlgorithm:\n1. Initialization:\n   a. Set CurrentStep to 0.\n   b. Ask the user to input the Tempo.\n   c. Set NumberOfSteps to 8.\n\n2. Loop:\n   a. While true:\n      i. Calculate the time duration for each step based on the Tempo.\n      ii. Play the MIDI note corresponding to the CurrentStep.\n      iii. Increment CurrentStep.\n      iv. If CurrentStep exceeds NumberOfSteps, reset it to 0.\n      v. Wait the calculated duration before moving to the next step.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/sequencing.html#implementation-of-the-8-step-sequencer",
    "href": "chapters/sequencing.html#implementation-of-the-8-step-sequencer",
    "title": "2  Sequencing",
    "section": "2.2 Implementation of the 8-Step Sequencer",
    "text": "2.2 Implementation of the 8-Step Sequencer\n\n\n\nFig.Implementation of an 8-step MIDI sequencer showing the complete signal flow from tempo control through step counting, array lookup, and MIDI output generation\n\n\nThis implementation demonstrates how to create a basic 8-step MIDI sequencer that cycles through a predefined sequence of notes at a controllable tempo. The patch showcases essential concepts including timing control, array-based data storage, step counting with modulo arithmetic, and MIDI output generation.\n\n2.2.1 Patch Overview\nThe 8-step sequencer patch implements a classic step-based sequencing paradigm where musical events are organized into discrete time slots. The patch operates by maintaining an internal counter that advances through eight sequential positions, with each position corresponding to a MIDI note value stored in an array. The sequencer provides real-time tempo control through a BPM slider interface and uses Pd’s built-in MIDI capabilities to output note events that can be routed to software synthesizers or external MIDI devices.\nThe patch architecture follows a linear signal flow pattern that begins with user control inputs and culminates in MIDI note output. The timing engine uses Pd’s metro object as the primary clock source, generating regular pulses that drive the step advancement mechanism. Each pulse triggers a counter increment operation, which is processed through modulo arithmetic to ensure the sequence loops seamlessly after reaching the eighth step. The current step index is then used to retrieve the corresponding MIDI note value from a pre-populated array, which is subsequently formatted into proper MIDI messages and transmitted through the system’s MIDI output infrastructure.\nThe visual interface provides immediate feedback on the sequencer’s operation through numeric displays that show both the current step position and the MIDI note value being played. The tempo control utilizes a mathematical conversion from beats per minute to milliseconds, allowing users to adjust the playback speed in musically meaningful units while maintaining precise timing accuracy.\n\n\n2.2.2 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Start/Stop Toggle] --&gt; B[Metro Object]\n    C[BPM Slider] --&gt; D[BPM to ms Conversion]\n    D --&gt; B\n    B --&gt; E[Step Counter]\n    E --&gt; F[+1 Increment]\n    F --&gt; G[Modulo 8]\n    G --&gt; E\n    G --&gt; H[Step Index Display]\n    H --&gt; I[Array Lookup]\n    I --&gt; J[MIDI Note Display]\n    J --&gt; K[Makenote Object]\n    K --&gt; L[MIDI Output]\n    M[Loadbang] --&gt; N[Array Initialization]\n    N --&gt; I\n\n\n\n\n\n\nThe sequencer’s operation begins when the user activates the start/stop toggle, which enables the metro object to begin generating timing pulses. The tempo of these pulses is determined by the BPM slider value, which is converted from beats per minute to milliseconds using the expression 60000/$f1. This conversion ensures that the metro object receives timing intervals in the appropriate units for accurate tempo control. Each timing pulse from the metro object triggers the step counter mechanism, implemented using a combination of f (float storage), +1 (increment), and mod 8 (modulo) objects. The float object stores the current step index, which is incremented by one each time a pulse is received. The incremented value is then processed through the modulo operation to ensure it wraps back to zero after reaching seven, creating the characteristic eight-step cycle.\nThe current step index serves dual purposes within the patch. First, it provides visual feedback to the user through a numeric atom display, allowing real-time monitoring of the sequencer’s position. Second, and more importantly, it functions as an index for retrieving MIDI note values from the midi_note array using the tabread object. The MIDI note array is initialized at patch startup through a loadbang object that sends a message containing the sequence 60 62 64 65 67 69 71 72, representing a C major scale starting from middle C. This initialization ensures that the sequencer has meaningful musical content available immediately upon loading the patch.\nOnce a MIDI note value is retrieved from the array, it is displayed numerically and then processed through the makenote object, which creates properly formatted MIDI note-on and note-off messages. The makenote object is configured with a fixed velocity of 100 and a duration of 250 milliseconds, providing consistent note characteristics across all steps. The resulting MIDI messages are then transmitted through the noteout object to MIDI channel 1, where they can be received by synthesizers or recording software.\n\n\n2.2.3 Key Objects and Their Roles\nThe following table summarizes the essential objects used in the step sequencer implementation and their specific functions within the patch:\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nmetro\nTiming engine that generates regular pulses\n\n\nexpr 60000/$f1\nConverts BPM to milliseconds for metro timing\n\n\nf\nStores current step index value\n\n\n+ 1\nIncrements step counter on each pulse\n\n\nmod 8\nWraps step counter back to 0 after reaching 7\n\n\ntabread midi_note\nReads MIDI note values from the array\n\n\nmakenote\nCreates MIDI note-on/off messages with velocity and duration\n\n\nunpack f f\nSeparates MIDI note and velocity values\n\n\nnoteout\nTransmits MIDI messages to specified channel\n\n\narray midi_note\nStores the sequence of MIDI note values\n\n\n\n The patch demonstrates several fundamental programming concepts, including the use of timing objects for rhythmic control, array manipulation for data storage and retrieval, mathematical operations for counter management, and MIDI message formatting for external communication. The modular design allows for easy modification of the note sequence, tempo range, and MIDI output parameters, making it a versatile foundation for more complex sequencing applications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/sequencing.html#random-step-sequencer-implementation",
    "href": "chapters/sequencing.html#random-step-sequencer-implementation",
    "title": "2  Sequencing",
    "section": "2.3 Random Step Sequencer Implementation",
    "text": "2.3 Random Step Sequencer Implementation\n\n\n\nFig. Random step sequencer patch showing the integration of random number generation with array-based note storage and MIDI output, creating unpredictable yet musically coherent patterns from predefined harmonic material.\n\n\nThe random step sequencer represents an evolution of the basic sequential approach to rhythm generation. Instead of following a predictable linear progression through stored note values, this implementation introduces stochastic elements that create unpredictable yet musically coherent patterns. This patch demonstrates how randomization can be integrated into sequencing systems to generate non-repetitive musical content while maintaining the fundamental structure of step-based sequencing.\n\n2.3.1 Patch Overview\nThe random step sequencer patch builds upon the foundational concepts of the basic 8-step sequencer while introducing a crucial modification: the step advancement mechanism is replaced with a random selection process. Rather than incrementing through array indices sequentially, the system generates random index values that point to different positions within the stored note sequence. This approach maintains the same timing control, array-based storage, and MIDI output capabilities as the sequential version, but produces entirely different musical results due to the non-linear access pattern.\nThe architecture preserves the linear signal flow from user controls to MIDI output, but introduces a random number generator at the critical junction where step advancement occurs. The timing engine continues to use Pd’s metro object as the primary clock source, but each timing pulse now triggers a random selection rather than a predictable increment. This modification transforms the sequencer from a deterministic system into a probabilistic one, where the same stored sequence can produce infinite variations in playback order.\nThe interface maintains the same visual feedback mechanisms as the sequential version, displaying both the randomly selected step index and the corresponding MIDI note value. The tempo control system remains unchanged, allowing users to adjust playback speed while experiencing the unpredictable note selection patterns that emerge from the randomization process.\n\n\n2.3.2 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Start/Stop Toggle] --&gt; B[Metro Object]\n    C[BPM Slider] --&gt; D[BPM to ms Conversion]\n    D --&gt; B\n    B --&gt; E[Random Generator]\n    E --&gt; F[Random Index 0-7]\n    F --&gt; G[Step Index Display]\n    G --&gt; H[Array Lookup]\n    H --&gt; I[MIDI Note Display]\n    I --&gt; J[Makenote Object]\n    J --&gt; K[MIDI Output]\n    L[Loadbang] --&gt; M[Array Initialization]\n    M --&gt; H\n\n\n\n\n\n\nThe operational sequence begins identically to the sequential version, with the user activating the start/stop toggle to enable metro object timing generation. The BPM slider continues to control tempo through the same mathematical conversion formula 60000/$f1, ensuring consistent timing accuracy regardless of the randomization occurring downstream.\nThe fundamental difference emerges at the step generation stage. Instead of using a counter that increments predictably, each timing pulse from the metro object triggers a random number generator implemented with the expr random(0, 8) object. This expression generates random integers between 0 and 7 (inclusive), providing equal probability access to any position within the 8-element note array. The random selection occurs independently for each timing pulse, meaning the same index can be selected multiple times in succession, or the system might skip certain indices entirely during any given playback session.\nThe randomly generated index serves the same dual purpose as in the sequential version: providing immediate visual feedback through a numeric display and functioning as the lookup key for array access. The tabread midi_note object retrieves the MIDI note value corresponding to the randomly selected index, creating an unpredictable sequence of pitches from the stored scale pattern.\nThe subsequent processing stages remain unchanged from the sequential implementation. The retrieved MIDI note value is displayed numerically, then processed through the makenote object with the same fixed velocity (100) and duration (250 milliseconds) parameters. The resulting MIDI messages are transmitted through the noteout object to MIDI channel 1, maintaining full compatibility with external synthesizers and recording systems.\nThe array initialization process follows the identical pattern established in the sequential version, using a loadbang object to populate the midi_note array with the C major scale sequence 60 62 64 65 67 69 71 72 at patch startup. This ensures immediate availability of musical content regardless of the randomization method employed.\n\n\n2.3.3 Key Objects and Their Roles\nThe following table summarizes the essential objects used in the random step sequencer implementation, highlighting the specific changes from the sequential version:\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nmetro\nTiming engine that generates regular pulses\n\n\nexpr 60000/$f1\nConverts BPM to milliseconds for metro timing\n\n\nexpr random(0, 8)\nGenerates random indices from 0 to 7\n\n\ntabread midi_note\nReads MIDI note values from the array\n\n\nmakenote\nCreates MIDI note-on/off messages with velocity and duration\n\n\nunpack f f\nSeparates MIDI note and velocity values\n\n\nnoteout\nTransmits MIDI messages to specified channel\n\n\narray midi_note\nStores the sequence of MIDI note values\n\n\n\n\nThis implementation demonstrates how simple modifications to algorithmic components can produce dramatically different results. The random step sequencer maintains all the practical advantages of the sequential version—including real-time tempo control, visual feedback, and MIDI compatibility—while introducing the creative possibilities inherent in stochastic processes. The resulting system serves as an excellent foundation for exploring probability-based composition techniques and can be extended with weighted random selection, controlled randomness, or hybrid deterministic-stochastic approaches.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/sequencing.html#piano-phase-steve-reich",
    "href": "chapters/sequencing.html#piano-phase-steve-reich",
    "title": "2  Sequencing",
    "section": "2.4 Piano Phase (Steve Reich)",
    "text": "2.4 Piano Phase (Steve Reich)\n\nView Piano Phase Score (S. Reich)\nDesign patterns are abstractions that capture idiomatic tendencies of practices and processes. These abstractions are embedded in live coding languages as functions and syntax, allowing for the reuse of these practices in new projects (Brown 2023).\nPiano Phase is an example of “music as a gradual process,” as Reich stated in his essay from 1968.(Reich 2002). In it, Reich described his interest in using processes to generate music, particularly noting how the process is perceived by the listener. Processes are deterministic: a description of the process can describe an entire whole composition. In other words, once the basic pattern and the phase process have been defined, the music consists itself. Reich called the unexpected ways change occurred via the process “by-products”, formed by the superimposition of patterns. The superimpositions form sub-melodies, often spontaneously due to echo, resonance, dynamics, and tempo, and the general perception of the listener(Epstein 1986). Piano Phase led to several breakthroughs that would mark Reich’s future compositions. The first is the discovery of using simple but flexible harmonic material, which produces remarkable musical results when phasing occurs. The use of 12-note or 12-division patterns in Piano Phase proved to be successful, and Reich would re-use it in Clapping Music and Music for 18 Musicians. Another novelty is the appearance of rhythmic ambiguity during phasing of a basic pattern. The rhythmic perception during phasing can vary considerably, from being very simple (in-phase), to complex and intricate.\nThe first section of Piano Phase has been the section studied most by musicologists. A property of the first section of phase cycle is that it is symmetric, which results in identical patterns half-way through the phase cycle (Epstein 1986). Piano Phase can be conceived as an algorithm. Starting with two pianos playing the same sequence of notes at the same speed, one of the pianists begins to gradually accelerate their tempo.\nWhen the separation between the notes played by both pianos reaches a fraction of a note’s duration, the phase is shifted and the cycle repeats. This process continues endlessly, creating a hypnotic effect of rhythmic phasing that evolves over time as an infinite sequence of iterations.\n\n\n\nPhase Difference\n\n\n\n2.4.1 Algorithm Formulation (pseudo code)\n1. Initialize two pianists with the same note sequence.\n2. Set an initial tempo for both pianists.\n3. Repeat until the desired phase is reached:\n     a. Gradually increase the second pianist's tempo.\n     b. Compare note positions between both pianists.\n     c. When the separation between the notes reaches a fraction of a note’s duration, invert the phase.\n     d. Continue playback.\n4. Repeat the cycle indefinitely to create a continuous and evolving rhythmic phasing effect.\n\n\n2.4.2 Piano Phase Implementation in Pd\n\n\n\nFig. Piano Phase patch demonstrating phasing process through two sequencers with different metro intervals, visualized with hradio step indicators and MIDI output for each voice.\n\n\n\n\n2.4.3 Patch Overview\nThe piano-phase.pd patch implements a minimalist phasing process inspired by Steve Reich’s Piano Phase. It uses two parallel sequencers, each reading from the same melodic sequence but advancing at slightly different rates. This gradual tempo difference causes the two sequences to drift out of phase, creating evolving rhythmic and melodic patterns.\n\n2.4.3.1 Sequence Data\n[table sequence 12]\nThe sequence is stored in a table named sequence with 12 MIDI note values:\n64 66 71 73 74 66 64 73 71 66 74 73\nThis pattern is initialized at patch load.\n\n\n\n2.4.4 Data Flow\n\n\n\n\n\nflowchart TD\n    Start([Start/Stop]) --&gt; M1[Clock 1 410ms]\n    Start --&gt; M2[Clock 2 406ms]\n    M1 --&gt; C1[Counter 1]\n    M2 --&gt; C2[Counter 2]\n    C1 --&gt; T1[read sequence]\n    C2 --&gt; T2[read sequence]\n    T1 --&gt; MK1[MIDI noteout]\n    T2 --&gt; MK2[MIDI noteout]\n\n\n\n\n\n\nThe operation of the Piano Phase patch begins with an initialization stage, triggered by a loadbang object when the patch is opened. This setup phase populates the shared sequence table with its 12 predefined MIDI note values. Simultaneously, the two metro objects, which serve as the independent clocks for each of the two parallel “pianists” or sequencers, are configured with their distinct time intervals: the left sequencer’s metro is set to 410 milliseconds, and the right sequencer’s metro to a slightly faster 406 milliseconds. A single toggle control allows the user to start or stop both metro objects concurrently, initiating or halting the phasing process.\nOnce activated, each metro object begins to output a stream of bangs at its specified interval. These bangs drive separate but structurally identical sequencing logics. For each sequencer, its respective metro pulse triggers a counter. This counter, typically implemented with a combination of f (float atom), + 1 (increment), and mod 12 (modulo) objects, advances its step index from 0 to 11. Upon reaching 11, the modulo operation ensures it wraps back to 0, allowing the 12-step sequence to loop continuously.\nThe current step index generated by each counter is then used to read a MIDI note value from the shared sequence table via a tabread sequence object. Thus, each sequencer independently fetches notes from the same melodic pattern according to its own current position. The retrieved MIDI note from each sequencer is subsequently processed by its own makenote object, which formats it into standard MIDI note-on and note-off messages, typically with a predefined velocity and duration. These MIDI messages are then sent to separate noteout objects, allowing the two voices to be routed to different MIDI channels or instruments if desired, or simply to play concurrently. Visual feedback for each sequencer’s current step is provided by a dedicated hradio object, which updates in sync with its counter.\nThe core phasing effect emerges from the slight discrepancy in the metro intervals. Because the right sequencer’s clock (406 ms) is marginally faster than the left’s (410 ms), its counter advances more rapidly. This causes the right sequencer’s melodic line to gradually “pull ahead” of the left. Over time, this desynchronization means the two identical melodic patterns are heard starting at different points relative to each other, creating complex, shifting rhythmic and harmonic interactions. This process continues cyclically: the faster sequence eventually “laps” the slower one, bringing them back into unison before they begin to drift apart again, perpetuating the mesmerizing phasing phenomenon central to Reich’s composition.\nPhasing Effect\n\nThe right sequencer is slightly faster, so its step index gradually shifts ahead of the left sequencer.\nOver time, the two sequences drift out of phase, producing new rhythmic and melodic combinations.\nEventually, the faster sequencer laps the slower one, and the process repeats.\n\n\n\n2.4.5 Key Objects and Their Roles\n\n\n\nObject\nPurpose\n\n\n\n\nmetro\nSets the timing for each sequencer\n\n\nexpr int(60000/$f1)\nConverts BPM to milliseconds\n\n\nf, + 1, mod 12\nAdvances and wraps the step index\n\n\ntabread sequence\nReads the current note from the sequence\n\n\nmakenote\nGenerates MIDI note-on/off with duration\n\n\nnoteout\nSends MIDI notes to the output device",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/sequencing.html#random-melody-generator",
    "href": "chapters/sequencing.html#random-melody-generator",
    "title": "2  Sequencing",
    "section": "2.5 Random Melody Generator",
    "text": "2.5 Random Melody Generator\nThe patch demonstrates how to generate a melody using a musical scale, store it in an array, and play it back using MIDI.\n\n\n\nFig. Random melody generator patch showing scale definition, algorithmic note selection, array storage, and MIDI playback system for creating melodic sequences from predefined harmonic material.\n\n\n\n2.5.1 Patch Overview\nThe Random Melody Generator patch demonstrates a comprehensive approach to algorithmic composition. This implementation showcases how to generate melodic content using musical scales, store the generated material in arrays for manipulation, and create a playback system with MIDI output capabilities.\nThe patch architecture is built around several interconnected functional blocks that work together to create a complete generative music system. The first component handles scale definition and melody generation, where musical scales are defined as interval patterns and used as the foundation for creating new melodic material. The melody storage system utilizes array objects to hold the generated sequences, providing both data persistence and visual feedback. The playback control mechanism employs metronome objects and indexing systems to sequence through the stored melodies at controllable tempos. Finally, the MIDI output section transforms the stored note data into playable MIDI messages that can be sent to external synthesizers or software instruments.\nThis modular design approach allows for easy modification and extension of the system. Each functional block can be adjusted independently, enabling composers and sound designers to experiment with different scales, generation algorithms, storage methods, and playback configurations without affecting the overall system stability.\n\n\n2.5.2 Data Flow\n\n\n\n\n\nflowchart TD\n    S[Scale Message] --&gt; L[list Append]\n    L --&gt; R[random + Offset]\n    R --&gt; M[Modulo/Indexing]\n    M --&gt; A[Melody Array]\n    A --&gt; P[Playback - metro, tabread]\n    P --&gt; MN[makenote]\n    MN --&gt; NO[noteout]\n\n\n\n\n\n\nThe scale definition process begins with a message box containing the fundamental building blocks of Western harmony represented as semitone intervals. The example scale 0 2 4 5 7 9 11 represents a major scale pattern, where each number corresponds to a specific semitone distance from a root note. This intervallic approach provides flexibility, as the same pattern can be transposed to any key by simply changing the root note offset. The scale intervals are processed through list manipulation objects, which append the values to an internal list structure and calculate the scale’s length for use in subsequent random selection processes.\nThe following table illustrates the mapping of musical notes to their corresponding intervals in semitones, which is crucial for understanding how the scale is constructed and how notes are generated from it.\n\n\n\n\nNote\nInterval\n\n\n\n\nC\n0\n\n\nC# / Db\n1\n\n\nD\n2\n\n\nD# / Eb\n3\n\n\nE\n4\n\n\nF\n5\n\n\nF# / Gb\n6\n\n\nG\n7\n\n\nG# / Ab\n8\n\n\nA\n9\n\n\nA# / Bb\n10\n\n\nB\n11\n\n\n\n\n The random note selection mechanism forms the core of the melodic generation process. A random number generator produces values between 0 and 47, which are then shifted upward by 60 to place the resulting MIDI notes in a musically appropriate range around middle C. This range selection ensures that generated melodies will fall within a playable range for most instruments and remain audible in typical monitoring setups. The random values are then mapped to the predefined scale using modulo operations and list indexing, ensuring that all generated notes conform to the harmonic framework established by the scale definition.\nThe melody construction process utilizes looping mechanisms, specifically the until, i, and + 1 objects, to generate sequences of predetermined length. Each iteration of the loop calculates a new note based on the scale mapping process and stores the result in a growing list structure. This approach allows for the creation of melodic phrases of any desired length, from short motifs to extended melodic lines. The deterministic nature of the loop structure ensures predictable behavior while the random element within each iteration provides the variability necessary for interesting melodic content.\nFor melody storage, the patch employs array system, specifically an array named melody that serves as both a data repository and a visual display element. The generated melodic sequences are written to this array using the tabwrite object, which provides indexed storage capabilities. The array’s visual representation appears as a graph within the patch interface, allowing users to see the melodic contour and note relationships in real-time. This visual feedback proves invaluable for understanding the characteristics of generated material and making informed decisions about parameter adjustments.\nThe playback control system centers around a metro object that functions as a digital metronome, generating regular timing pulses that drive the sequencing process. The tempo of these pulses is controlled by a horizontal slider interface element, providing real-time tempo adjustment capabilities. Each pulse from the metronome advances an index counter that points to the next position in the melody array. This indexing system automatically wraps around to the beginning when it reaches the end of the stored sequence, creating seamless looping playback that continues indefinitely until stopped by the user.\nThe MIDI output stage transforms the stored melodic data into standard MIDI protocol messages that can interface with external hardware and software. The note values retrieved from the array are processed through a makenote object, which creates properly formatted MIDI note-on and note-off messages with specified velocity and duration parameters. These parameters can be adjusted to affect the musical expression of the playback, with velocity controlling the apparent loudness or intensity of notes and duration determining their length. The final noteout object handles the actual transmission of MIDI data to the system’s MIDI routing infrastructure, where it can be directed to synthesizers, samplers, or recording software.\n\n\n2.5.3 Key Objects and Their Roles\n\n\n\n\nObject\nPurpose\n\n\n\n\nrandom 48\nGenerates random note indices\n\n\n+ 60\nShifts notes to a higher MIDI octave\n\n\nmod 12\nMaps notes to scale degrees\n\n\nlist-idx\nRetrieves scale degree from the list\n\n\ntabwrite\nWrites notes to the melody array\n\n\nmetro\nControls playback timing\n\n\ntabread\nReads notes from the melody array\n\n\nmakenote\nCreates MIDI notes with velocity/duration\n\n\nnoteout\nSends MIDI notes to output\n\n\n\n\n The integration of these components creates a flexible and powerful tool for algorithmic composition. The system can generate new melodic material on demand, store multiple variations for comparison, and play back the results in real-time with full MIDI compatibility. This makes it suitable for both experimental composition work and practical music production applications, where generated material can serve as inspiration, accompaniment, or the foundation for further development.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/sequencing.html#the-euclidean-algorithm-generates-traditional-musical-rhythms",
    "href": "chapters/sequencing.html#the-euclidean-algorithm-generates-traditional-musical-rhythms",
    "title": "2  Sequencing",
    "section": "2.6 The Euclidean Algorithm Generates Traditional Musical Rhythms",
    "text": "2.6 The Euclidean Algorithm Generates Traditional Musical Rhythms\n\n\n\nEucliden image\n\n\nSee original paper\nThe Euclidean rhythm in music was discovered by Godfried Toussaint in 2004 and is described in a 2005 paper “The Euclidean Algorithm Generates Traditional Musical Rhythms” (Toussaint 2005). The paper presents a method for generating rhythms that are evenly distributed over a given time span, using the Euclidean algorithm. This method is particularly relevant in the context of world music, where such rhythms are often found. The Euclidean algorithm (as presented in Euclid’s Elements) calculates the greatest common divisor of two given integers. It is shown here that the structure of the Euclidean algorithm can be used to efficiently generate a wide variety of rhythms used as timelines (ostinatos) in sub-Saharan African music in particular, and world music in general. These rhythms, here called Euclidean rhythms, have the property that their onset patterns are distributed as evenly as possible. Euclidean rhythms also find application in nuclear physics accelerators and computer science and are closely related to several families of words and sequences of interest in the combinatorics of words, such as Euclidean strings, with which the rhythms are compared (Toussaint 2005).\n\n2.6.1 Hypothesis\nSeveral researchers have observed that rhythms in traditional world music tend to exhibit patterns distributed as regularly or evenly as possible. The hypothesis is that the Euclidean algorithm can be used to generate these rhythms. According to the hypothesis, the Euclidean algorithm can be used to generate rhythms that are evenly distributed over a given time span. This is particularly relevant in the context of world music, where such rhythms are often found. The following is a summary of the main points:\n\nPatterns of maximal evenness can be described using the Euclidean algorithm on the greatest common divisor of two integers.\n\n\n\n2.6.2 Patterns of Maximal Evenness\nThe Pattern of Maximal Evenness is a concept used in music theory to create Euclidean rhythms. Euclidean rhythms are rhythmic patterns that evenly distribute beats over a time cycle.\nIn essence, the Pattern of Maximal Evenness seeks to distribute a specific number of beats evenly within a given time span. This is achieved by dividing the time into equal parts and assigning beats to these divisions uniformly.\nExample:\nx = beat\n· = rest\n[×· ×· ×· ×· ] → [1 0 1 0 1 0 1 0]\n(8,4) = 4 beats evenly distributed over 8 pulses\n[×· ·×· ·×·] → [1 0 0 1 0 0 1 0]\n(8,3) = 3 beats evenly distributed over 8 pulses\n\n\n2.6.3 Euclidean Algorithm\nOne of the oldest known algorithms, described in Euclid’s Elements (around 300 BCE) in Proposition 2 of Book VII, now known as the Euclidean algorithm, calculates the greatest common divisor of two given integers.\nThe idea is very simple. The smaller number is repeatedly subtracted from the larger until the larger becomes zero or smaller than the smaller one, in which case it becomes the remainder. This remainder is then repeatedly subtracted from the smaller number to obtain a new remainder. This process continues until the remainder is zero.\nTo be more precise, consider the numbers 5 and 8 as an example:\n\n\n\nStep\nOperation\nEquation\nQuotient\nRemainder\n\n\n\n\n1\nDivide 8 by 5\n\\[8 = 1 \\cdot 5 + 3\\]\n1\n3\n\n\n2\nDivide 5 by 3\n\\[5 = 1 \\cdot 3 + 2\\]\n1\n2\n\n\n3\nDivide 3 by 2\n\\[3 = 1 \\cdot 2 + 1\\]\n1\n1\n\n\n4\nDivide 2 by 1\n\\[2 = 2 \\cdot 1 + 0\\]\n2\n0\n\n\n\n The idea is to keep dividing the previous divisor by the remainder obtained in each step until the remainder is 0. When we reach a remainder of 0, the previous divisor is the greatest common divisor of the two numbers.\nIn short, the process can be seen as a sequence of equations:\n\\[\\begin{align*}\n8 &= (1)\\cdot(5) + 3 \\\\\n5 &= (1)\\cdot(3) + 2 \\\\\n3 &= (1)\\cdot(2) + 1 \\\\\n2 &= (1)\\cdot(2) + 0\n\\end{align*}\\]\n\nNote: 8 = (1) . (5) + 3 means that 8 is divided by 5 once, yielding a quotient of 1 and a remainder of 3.\n\nIn essence, it involves successive divisions to find the greatest common divisor of two positive numbers (GCD from now on).\nThe GCD of two numbers a and b, assuming a &gt; b, is found by first dividing a by b, and obtaining the remainder r.\nThe GCD of a and b is the same as that of b and r. When we divide a by b, we obtain a quotient c and a remainder r such that:\n\\(a = c \\cdot b + r\\)\nExamples:\nLet’s compute the GCD of 17 and 7.\nSince 17 = 7 · 2 + 3, then GCD(17, 7) is equal to GCD(7, 3). Again, since 7 = 3 · 2 + 1, then GCD(7, 3) is equal to GCD(3, 1). Here, it is clear that the GCD between 3 and 1 is simply 1. Therefore, the GCD between 17 and 7 is also 1.\n\\[\\text{GCD}(17,7) = 1\\] \\[17 = 7 \\cdot 2 + 3\\] \\[7 = 3 \\cdot 2 + 1\\] \\[3 = 1 \\cdot 3 + 0\\]\nAnother example:\n\\[\\text{GCD}(8,3) = 1\\] \\[8 = 3 \\cdot 2 + 2\\] \\[3 = 2 \\cdot 1 + 1\\] \\[2 = 1 \\cdot 2 + 0\\]\n\n\n2.6.4 How does computing the GCD turn into maximally even distributed patterns?\nRepresent a binary sequence of k ones [1] and n − k zeros [0], where each [0] bit represents a time interval and the ones [1] indicate signal triggers.\nThe problem then reduces to:\n\nConstruct a binary sequence of n bits with k ones such that the ones are distributed as evenly as possible among the zeros.\n\nA simple case is when k evenly divides n (with no remainder), in which case we should place ones every n/k bits. For example, if n = 16 and k = 4, then the solution is:\n\\[[1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0]\\]\nThis case corresponds to n and k having a common divisor of k (in this case 4).\nMore generally, if the greatest common divisor between n and k is g, we would expect the solution to decompose into g repetitions of a sequence of n/g bits. This connection with greatest common divisors suggests that we could compute a maximally even rhythm using an algorithm like Euclid’s.\n \n\n2.6.4.1 Example (13, 5)\nLet’s consider a sequence with n = 13 and k = 5.\nSince 13 − 5 = 8, we start with a sequence consisting of 5 ones, followed by 8 zeros, which can be thought of as 13 one-bit sequences:\n\\[[1 1 1 1 1 0 0 0 0 0 0 0 0]\\]\nWe begin moving zeros by placing one zero after each one, creating five 2-bit sequences, with three remaining zeros:\n\\[[10] [10] [10] [10] [10] [0] [0] [0]\\]\n\\[13 = 5 · 2 + 3\\]\nThen we distribute the three remaining zeros similarly, placing a [0] after each [10] sequence:\n\\[[100] [100] [100] [10] [10]\\]\n\\[5 = 3 · 1 + 2\\]\nWe now have three 3-bit sequences, and a remainder of two 2-bit sequences. So we continue the same way, placing one [10] after each [100]:\n\\[[10010] [10010] [100]\\]\n\\[3 = 2 · 1 + 1\\]\nThe process stops when the remainder consists of a single sequence (here, [100]), or we run out of zeros.\nThe final sequence is, therefore, the concatenation of [10010], [10010], and [100]:\n\\[[1 0 0 1 0 1 0 0 1 0 1 0 0]\\]\n\\[2 = 1 · 2 + 0\\]\n \n\n\n2.6.4.2 Example (17, 7)\nSuppose we have 17 pulses and want to evenly distribute 7 beats over them.\n1. We align the number of beats and silences (7 ones and 10 zeros):\n\\[[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\\]\n\\[[1 1 1 1 1 1 1]  [0 0 0 0 0 0 0 0 0 0]\\]\n2. We form 7 groups, corresponding to the division of 17 by 7; we get 7 groups of [1 0] and 3 remaining zeros [000], which means the next step forms 3 groups until only one or zero groups remain.\n\\[ [1 0] [1 0] [1 0] [1 0] [1 0] [1 0] [1 0] [0] [0] [0]\\]\n\\[ 17 = 7 \\cdot 2 + 3\\]\n\n\n\n1\n1\n1\n1\n1\n1\n1\n0 0 0\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n \n3. Again, this corresponds to dividing 7 by 3. In our case, we are left with only one group and we are done.\n \n\\[ [1 0 0] [1 0 0] [1 0 0] [1 0] [1 0] [1 0] [1 0]\\]\n\n\n\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\\[ [1 0 0 1 0] [1 0 1 0 0] [1 0 0 1 0] [1 0]\\]\n\\[ 7 = 3 · 2 + 1\\]\n\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n1\n1\n1\n\n\n\n\n\n\n0\n0\n0\n\n\n\n\n\n\n\n \n4. Finally, the rhythm is obtained by reading the grouping column by column, from left to right, step by step.\n\\[ [1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0]\\]\n\\[ 3 = 1 · 3 + 0\\]\n \n\n\n\n2.6.5 Implementation in Pd - Euclidean sequencer\nThe following code implements the Euclidean algorithm in Pd to generate a Euclidean rhythm. The algorithm is based on the mathematical formula:\n\\[(\\text{index} \\cdot \\text{hits}) \\%  \\text{ steps} &lt; \\text{notes}\\]\nwhere:\n\n\\(\\text{index}\\) = index of the Euclidean series (array)\n\\(\\text{hits}\\) = number of notes to be played\n\n\\(\\text{steps}\\) = array size\n\\(\\text{notes}\\) = threshold for comparison\n\nYoy can check this Euclidean rhythm demo in order to interact and see how the algorithm works.\n\n2.6.5.1 Understanding the Mathematical Formula\nThe formula (index * hits) % steps &lt; hits provides a simple and efficient way to approximate the distribution of pulses (or beats) over a sequence of discrete steps. It works by multiplying the current position (represented by index) by the total number of hits (pulses) desired. This result is then taken modulo the total number of steps to ensure it wraps around properly in a cyclical pattern. Finally, the result of this modulo operation is compared to the number of hits. If the condition is true, we place a beat at that position; otherwise, we place a rest.\nThis method produces a rhythm by relying on modular arithmetic. The result is a mathematically regular distribution of beats that often approximates what we expect from an even rhythmic distribution. However, it does so without any recursive logic or iteration—it simply applies a consistent rule to each step in isolation. This makes the formula extremely efficient: it runs in constant time for any position, and it does not require any memory to store the pattern.\n\n\n2.6.5.2 The Bjorklund Algorithm Explained\nIn contrast, the Bjorklund algorithm is a more sophisticated procedure rooted in the Euclidean algorithm for computing the greatest common divisor (GCD). This algorithm begins with two values: the number of pulses (beats) and the number of rests (steps minus beats). It then recursively groups these elements in a way that maximizes the evenness of their distribution.\nThe method proceeds by repeatedly pairing elements—first grouping pulses with rests, then regrouping the leftovers, and so on—until the sequence cannot be subdivided further. The final pattern emerges from this recursive grouping, and it is typically rotated so that it starts with a pulse. The result is a rhythm that is maximally even, meaning that the beats are spaced as equally as possible given the constraints.\nThis process, while more computationally demanding and conceptually complex, produces the canonical Euclidean rhythms often cited in academic and musical literature.\n\n\n2.6.5.3 Comparing the Formula and the Algorithm\nThe core difference between the two approaches lies in how they arrive at the rhythmic pattern. The mathematical formula provides a direct, position-based method for determining beat placement. It does not take into account the context of previous or future beats—it treats each step in isolation. This is why it is so fast and well-suited for real-time applications, such as live audio processing in Pd or other creative coding environments.\nThe Bjorklund algorithm, on the other hand, is concerned with the global structure of the pattern. It ensures that beats are distributed with maximal evenness and follows a well-defined sequence of operations that are both recursive and stateful. This means it needs to store and manipulate arrays of data to arrive at the final rhythm. The computational complexity of this algorithm is higher, and it is not as straightforward to implement, but the results are musically and mathematically robust.\nOne major distinction is that the formula often produces a rotated version of the Bjorklund rhythm. That is, while the number and spacing of beats may be similar, the starting position may differ. The Bjorklund rhythm always starts with a pulse, ensuring that it adheres to a particular musical convention, while the formula does not guarantee this.\nAnother important distinction lies in the distribution logic. The formula uses a fixed mathematical rule to space out the beats, leading to regular but not necessarily optimal placement. The Bjorklund algorithm, however, iteratively rearranges beats and rests to achieve the best possible balance.\n\n\n2.6.5.4 A Concrete Example: (13, 5)\nLet’s consider the case of 13 steps with 5 beats.\nUsing the mathematical formula, we apply (index * 5) % 13 &lt; 5 for each position:\nPattern: 1 0 0 1 0 0 1 0 1 0 0 1 0\nIn contrast, the Bjorklund algorithm produces:\nPattern: 1 0 1 0 0 1 0 1 0 0 1 0 0\nBoth patterns contain five beats. Both distribute them fairly evenly. But the Bjorklund version achieves a more perceptually even spacing and aligns with theoretical expectations. The formula result is essentially a rotated variant.\nIn summary, the formula (index * hits) % steps &lt; hits offers a pragmatic and computationally efficient way to generate rhythm patterns that resemble Euclidean rhythms. It is well-suited for real-time use cases and environments where simplicity and speed are more important than strict accuracy. In contrast, the Bjorklund algorithm provides a mathematically rigorous method for generating rhythms with maximal evenness. It aligns with canonical definitions and is favored in theoretical and compositional contexts.The choice between these methods depends on your priorities: use the formula for lightweight, real-time approximation, and use Bjorklund when you need precision and adherence to the canonical Euclidean model.\n\n\n\n2.6.6 Euclidean Rhythm Generator\nThis section provides a detailed explanation of the Euclidean-basic-Serie.pd patch. This Pd patch demonstrates how to generate Euclidean rhythms using a straightforward mathematical formula.\n\n\n\nEuclidean rhythm generator patch showing step and hit parameter controls, mathematical formula calculation, and pattern output display for generating evenly distributed rhythmic sequences.\n\n\n\n2.6.6.1 Patch Overview\nThe patch is structured into several functional blocks. First, there are controls for setting the number of steps and the number of hits (pulses). Next, the patch calculates the Euclidean pattern using a mathematical formula.\n(index * hits ) % steps\n↓\n[&lt; notes]\nFinally, the resulting pattern is output and visualized as a list of pulses and rests.\n\n\n2.6.6.2 Data Flow\n\n\n\n\n\nflowchart TD\n    S[Steps Slider] --&gt; A[Step Counter]\n    H[Hits Slider] --&gt; B[Hits Value]\n    A --&gt; C[Euclidean Formula: index * hits % steps]\n    B --&gt; C\n    C --&gt; D[Compare &lt; hits]\n    D --&gt; E[Pattern List]\n    E --&gt; F[Output/Display]\n\n\n\n\n\n\nThe process begins with the user setting the number of steps in the sequence using a horizontal slider labeled “Steps.” This determines the total length of the rhythmic cycle. The user also sets the number of hits, or pulses, using another slider labeled “HITS.” These two values are stored and sent to the rest of the patch, where they are used to calculate the rhythmic pattern.\nOnce the parameters are set, the patch uses a loop, implemented with the until object, to iterate through each step index from 0 up to one less than the total number of steps. For each index, the patch calculates a value using the formula (index * hits) % steps. This formula determines the position of each pulse within the cycle by multiplying the current index by the number of hits and taking the result modulo the total number of steps. The result of this calculation is then compared to the number of hits. If the result is less than the number of hits, a pulse (represented by a 1) is added to the pattern. Otherwise, a rest (represented by a 0) is added. This process is repeated for every step in the sequence, gradually building up the complete Euclidean rhythm as a list of ones and zeros.\nAfter the pattern has been calculated, it is displayed as a list, allowing you to see the sequence of pulses and rests. The patch also includes a reset mechanism, so that the pattern can be recalculated and updated whenever the user changes the number of steps or hits.\n\n\n2.6.6.3 Example: (Steps = 13, Hits = 5)\nTo illustrate how the patch works, consider the case where the number of steps is 13 and the number of hits is 5. The following table shows the calculation for each step:\n\n\n\nIndex\nCalculation\nResult\n&lt; Hits?\nOutput\n\n\n\n\n0\n(0 * 5) % 13 = 0\n0\nYes\n1\n\n\n1\n(1 * 5) % 13 = 5\n5\nYes\n1\n\n\n2\n(2 * 5) % 13 = 10\n10\nNo\n0\n\n\n3\n(3 * 5) % 13 = 2\n2\nYes\n1\n\n\n4\n(4 * 5) % 13 = 7\n7\nNo\n0\n\n\n5\n(5 * 5) % 13 = 12\n12\nNo\n0\n\n\n6\n(6 * 5) % 13 = 4\n4\nYes\n1\n\n\n7\n(7 * 5) % 13 = 9\n9\nNo\n0\n\n\n8\n(8 * 5) % 13 = 1\n1\nYes\n1\n\n\n9\n(9 * 5) % 13 = 6\n6\nNo\n0\n\n\n10\n(10 * 5) % 13 = 11\n11\nNo\n0\n\n\n11\n(11 * 5) % 13 = 3\n3\nYes\n1\n\n\n12\n(12 * 5) % 13 = 8\n8\nNo\n0\n\n\n\n The resulting pattern is:\n\\[1 1 0 1 0 0 1 0 1 0 0 1 0\\]\nThis sequence distributes five pulses as evenly as possible across thirteen steps.\n\n\n2.6.6.4 Key Objects and Their Roles\nThe following table summarizes the key objects used in the patch and their roles:\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nuntil\nLoops through each step index\n\n\nexpr\nCalculates (index * hits) % steps\n\n\n&lt;\nCompares result to hits, outputs 1 or 0\n\n\nadd2\nAppends each result to the output pattern list\n\n\nmod\nEnsures index wraps around for correct calculation\n\n\n\n\n\n\n2.6.7 Euclidean Rotation Generator\nThis section explains the Euclidean-Rotation.pd patch. This Pd patch builds on the basic Euclidean rhythm generator by introducing a rotation parameter, allowing you to shift the starting point of the rhythm pattern. This feature is useful for exploring different phase relationships and rhythmic variations without changing the underlying distribution of pulses.\n\n\n\nFig. Euclidean rotation generator patch showing enhanced rhythm control with rotation parameter for cyclical pattern shifting and phase exploration.\n\n\n\n2.6.7.1 Patch Overview\nThe patch allows you to set the number of steps, the number of hits (pulses), and the rotation amount. The rotation parameter shifts the pattern cyclically, so the rhythm can start at any point in the sequence. The patch calculates the Euclidean pattern using a formula that incorporates the rotation, and then outputs the resulting pattern for visualization and further use.\nThe following diagram illustrates the data flow for the rhythm and rotation:\n\n\n\n\n\nflowchart TD\n    Steps[Steps Slider] --&gt; Counter[Step Counter]\n    Hits[Hits Slider] --&gt; HitsVal[Hits Value]\n    Rot[Rotation Slider] --&gt; RotVal[Rotation Value]\n    Counter --&gt; Formula[Euclidean Formula: index_plus_rotation_times_hits_mod_steps]\n    RotVal --&gt; Formula\n    HitsVal --&gt; Formula\n    Formula --&gt; Compare[Compare to Hits]\n    Compare --&gt; Pattern[Pattern List]\n    Pattern --&gt; Output[Output/Display]\n\n\n\n\n\n\n\n\n2.6.7.2 Data Flow\nThe process begins with the user setting the number of steps, hits, and rotation using horizontal sliders. The steps slider determines the total length of the rhythmic cycle, the hits slider sets the number of pulses to be distributed, and the rotation slider specifies how many positions to shift the pattern.\nA loop, implemented with the until object and a counter, iterates through each step index. For each index, the patch calculates the value using the formula ((index + rotation) * hits) % steps. This formula determines the position of each pulse within the cycle, taking into account the rotation. The result is then compared to the number of hits: if it is less than the number of hits, a pulse (represented by a 1) is added to the pattern; otherwise, a rest (represented by a 0) is added. This process is repeated for every step, building up the complete rotated Euclidean rhythm.\nThe resulting pattern is displayed as a list, showing the sequence of pulses and rests after rotation. The patch also provides a reset mechanism to clear and recalculate the pattern when parameters change.\n\n\n2.6.7.3 Example: Rotating a Euclidean Rhythm\nSuppose you set 8 steps, 3 hits, and a rotation of 2. The main rhythm without rotation might be:\n\\[1 0 0 1 0 0 1 0\\]\nApplying a rotation of 2 shifts the pattern two steps to the right, resulting in:\n\\[0 1 0 0 1 0 0 1\\]\nThis allows you to experiment with different phase offsets and rhythmic feels.\n\n\n2.6.7.4 Key Objects and Their Roles\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nuntil\nLoops through each step index\n\n\ncounter\nIncrements the step index for each iteration\n\n\nexpr\nCalculates the Euclidean formula with rotation\n\n\n&lt;\nCompares the formula result to hits, outputs 1 or 0\n\n\nadd2\nAppends each result to the output pattern list\n\n\nmod\nEnsures index wraps around for correct calculation\n\n\ntabwrite\nWrites the final pattern to an array for visualization\n\n\n\n\n\n\n2.6.8 Euclidean Accents Generator\nThis section explains the Euclidean-Accents.pd patch. This Pd patch extends the basic Euclidean rhythm generator by adding a second, independent Euclidean pattern to control accents. The result is a two-layer sequencer: one layer for the main rhythm (hits) and another for accentuation, allowing for more expressive and dynamic rhythmic patterns.\n\n\n\nEuclidean accents generator showing two parallel Euclidean rhythm generators combined to create accented beat patterns with independent control over main rhythm and accent placement.\n\n\n\n2.6.8.1 Patch Overview\nThe patch is organized into two parallel sections. The first section generates the main Euclidean rhythm, while the second section generates an accent pattern using the same mathematical approach. Both sections allow for independent control of steps, hits, rotation, and accents. The outputs of both patterns are then combined to produce a final sequence where steps can be silent, regular, or accented.\nThe following diagram illustrates the data flow for both rhythm and accent layers:\n\n\n2.6.8.2 Data Flow\n\n\n\n\n\nflowchart TD\n    S[Steps Slider] --&gt; A[Step Counter]\n    H[Hits Slider] --&gt; B[Hits Value]\n    R[Rotation Slider] --&gt; C[Rotation Value]\n    A --&gt; D[Euclidean Formula: index+rotation*hits%steps]\n    B --&gt; D\n    C --&gt; D\n    D --&gt; E[Compare &lt; hits]\n    E --&gt; F[Main Pattern List]\n    ACC[Accents Slider] --&gt; ACCV[Accents Value]\n    ACCROT[Accents Rotation Slider] --&gt; ACCROTVAL[Accents Rotation Value]\n    A2[Step Counter] --&gt; D2[Euclidean Formula: index+acc_rotation*accents %steps]\n    ACCV --&gt; D2\n    ACCROTVAL --&gt; D2\n    D2 --&gt; E2[Compare &lt; accents]\n    E2 --&gt; F2[Accent Pattern List]\n    F & F2 --&gt; G[Combine Patterns]\n    G --&gt; H[Output/Display]\n\n\n\n\n\n\nThe process begins with the user setting the number of steps, hits, and rotation for the main rhythm using horizontal sliders. These parameters determine the length of the sequence, the number of pulses, and the rotation (offset) of the pattern. The patch uses a loop (until) and a counter to iterate through each step index. For each step, it calculates the value using the formula ((index + rotation) * hits) % steps. This value is compared to the number of hits; if it is less, a pulse (1) is added to the main pattern, otherwise a rest (0).\nIn parallel, the user can set the number of accents and their rotation using additional sliders. The accent pattern is generated in the same way as the main rhythm, but with its own independent parameters. The formula ((index + acc_rotation) * accents) % steps is used to determine the placement of accents. The result is a second pattern of 1s (accent) and 0s (no accent).\nAfter both patterns are generated, they are combined step by step. If both the main pattern and the accent pattern have a 1 at the same step, the output for that step is set to 2 (indicating an accented hit). If only the main pattern has a 1, the output is 1 (regular hit). If neither has a 1, the output is 0 (rest). The final combined pattern is written to an array and displayed, allowing for visualization and further use in sequencing.\n\n\n2.6.8.3 Example: Combining Rhythm and Accents\nSuppose you set 8 steps, 3 hits, and 2 accents. The main rhythm might produce a pattern like:\n\\[1 0 0 1 0 0 1 0\\]\nThe accent pattern, with its own rotation, might produce:\n\\[1 0 1 0 0 1 0 0\\]\nCombining these, the output would be:\n\\[2 0 1 1 0 1 1 0\\]\nHere, “2” indicates an accented hit, “1” a regular hit, and “0” a rest.\n\n\n2.6.8.4 Key Objects and Their Roles\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nuntil\nLoops through each step index\n\n\ncounter\nIncrements the step index for each iteration\n\n\nexpr\nCalculates the Euclidean formula for both rhythm and accent patterns\n\n\n&lt;\nCompares the formula result to hits or accents, outputs 1 or 0\n\n\nadd2\nAppends each result to the output pattern list\n\n\nlist-idx\nRetrieves values from the generated lists for combination\n\n\nexpr if($f1+$f2==2, 2, $f1)\nCombines main and accent patterns into a single output\n\n\ntabwrite\nWrites the final pattern to an array for visualization",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/sequencing.html#cellular-automata",
    "href": "chapters/sequencing.html#cellular-automata",
    "title": "2  Sequencing",
    "section": "2.7 Cellular Automata",
    "text": "2.7 Cellular Automata\n\nCellular automata are simple machines consisting of cells that update in parallel at discrete time steps. In general, the state of a cell depends on the state of its local neighborhood at the previous time step. The earliest known examples were engineered for specific purposes, such as the two-dimensional cellular automaton constructed by von Neumann in 1951 to model biological self-replication (Brummitt and Rowland 2012)\nA cellular automaton is a mathematical and computational model for a dynamic system that evolves in discrete steps. It is suitable for modeling natural systems that can be described as a massive collection of simple objects interacting locally with each other.\nA cellular automaton is a collection of “colored” cells on a grid of specified shape that evolves through a series of discrete time steps according to a set of rules based on the states of neighboring cells. The rules are then applied iteratively over as many time steps as desired.\nCellular automata come in a variety of forms and types. One of the most fundamental properties of a cellular automaton is the type of grid on which it is computed. The simplest such grid is a one-dimensional line. In two dimensions, square, triangular, and hexagonal grids can be considered.\nOne must also specify the number of colors (or distinct states) k that a cellular automaton can assume. This number is typically an integer, with k=2 (binary, [1] or [0]) being the simplest choice. For a binary automaton, color 0 is commonly referred to as “white” and color 1 as “black”. However, cellular automata with a continuous range of possible values can also be considered.\nIn addition to the grid on which a cellular automaton resides and the colors its cells can assume, one must also specify the neighborhood over which the cells influence each other.\n\n2.7.1 One-Dimensional Cellular Automata (1DCA)\nThe simplest option is \"nearest neighbors\", where only the cells directly adjacent to a given cell can influence it at each time step. Two common neighborhoods in the case of a two-dimensional cellular automaton on a square grid are the so-called Moore neighborhood (a square neighborhood) and the von Neumann neighborhood (a diamond-shaped neighborhood).\nCellular automata are considered as a vector. Each component of the vector is called a cell. Each cell is assumed to take only two states:\n[0] –&gt; white\n[1] –&gt; black\nThis type of automaton is known as an elementary one-dimensional cellular automaton (1DCA). A dynamic process is performed, starting from an initial configuration C(0) of each of the cells (stage 0), and at each new stage, the state of each cell is calculated based on the state of the neighboring cells and the cell itself in the previous stage.\nThe cellular automata that we study in this section are one-dimensional. A one-dimensional cellular automaton consists of:\n\nan alphabet Σ of size k,\na positive integer d,\na function i from the set of integers to Σ, and\na function f from Σd (d-tuples of elements in Σ) to S.\n\nThe function i is called the initial condition, and the function f is called the rule. We think of the initial condition as an infinite row of discrete cells, each assigned one of k colors. To evolve the cellular automaton, we update all cells in parallel, where each cell updates according to f, a function of d cells in its vicinity on the previous step. I adopt the usual convention of naming a cellular automaton’s rule by the number whose base-k digits consist of the outputs of the rule under the kd possible inputs of d cells, ordered reverse-lexicographically. For example, the two-color rule depending on three cells that maps the eight possible inputs according to the rule 00011110 = 30 in this numbering. Here we have identified 0 = [ ] and 1 = [x].\n\n\n2.7.2 The Case of Rule 30\nRule 30 is a binary one-dimensional cellular automaton introduced by Stephen Wolfram (Wolfram 1983). It considers an infinite one-dimensional array of cellular automaton cells with only two states, with each cell in some initial state.\nAt discrete time intervals, each cell changes state spontaneously based on its current state and the state of its two neighbors.\nWhat it consists of:\n\nEach cell can be in one of two states: alive or dead.\nThe next generation of a cell is determined by the current state of the cell and the state of its two neighboring cells.\nThere are 8 possible configurations of neighboring states (3^2), and for each, Rule 30 defines whether the cell lives or dies in the next generation.\n\n\nIt is called Rule 30 because in binary, 000111102 = 30\n\nFor Rule 30, the set of rules that governs the automaton’s next state is:\n\n\n\n\n\n\n\n\n\nPattern (decimal)\nPattern (binary)\nNext State(center cell)\nNew state(center cell)\n\n\n\n\n0\n000\nDead\n000\n\n\n1\n001\nAlive\n011\n\n\n2\n010\nDead\n000\n\n\n3\n011\nAlive\n011\n\n\n4\n100\nAlive\n011\n\n\n5\n101\nDead\n000\n\n\n6\n110\nAlive\n011\n\n\n7\n111\nDead\n000\n\n\n\n\nIn the following diagram, the top row shows the state of the central cell (cell i) and its two neighboring cells at a given stage, and the bottom row shows the state of the central cell in the next stage.\nFor example, in the first case of the figure:\n\nif the state of a cell at a given stage is [1] (black) and\nits two neighbors at that stage are also [1] (black),\nthen the cell’s state in the next stage will be [0] (white).\n\n\n\n\nRule 30 cellular automaton evolution displaying binary state transitions and emergent pattern generation from simple neighborhood-based rules.\n\n\nLet’s break down the procedure for determining the next state in the 8 combinations:\nInput configurations: Consider the 8 possible input combinations of the three cells (a central cell and its left and right neighbors). Since each cell can be in one of two possible states (0 or 1), the combinations are 000, 001, 010, 011, 100, 101, 110, and 111.\nBinary representation of the rule: Rule 30 is represented by the number 30 in binary, which is 00011110. This binary representation determines the rules for the next state of the central cell for each of the 8 possible combinations.\nBit correspondence: The 8 bits of the binary representation (00011110) correspond to the 8 input combinations in order. From right to left, the bits represent the next state of the central cell for each input combination.\nFor example, the least significant bit of 00011110 (the rightmost bit) is 0. This means that when the input combination is 000, the next state of the central cell will be 0.\nNext state determination: For each input combination (e.g., 000, 001, 010, 011, 100, 101, 110, 111), the corresponding bit in the binary representation of Rule 30 [00011110] indicates the next state of the central cell.\n\n\n\nInput Configuration\nNext State\n\n\n\n\n000\n0\n\n\n001\n1\n\n\n010\n1\n\n\n011\n1\n\n\n100\n1\n\n\n101\n0\n\n\n110\n0\n\n\n111\n0\n\n\n\n\nFor example, if the input combination is 000 and the corresponding bit in Rule 30 is 0, then the next state of the central cell will be 0.\nTherefore, the rules are not arbitrary but are determined by the binary representation of Rule 30, which specifies the next state of the central cell for each input combination of its neighbors.\n\n\n\nRule 30 transition table illustrating the eight neighborhood patterns and their resulting center cell states for the next generation.\n\n\n\n\n2.7.3 One-Dimensional Cellular Automata Implementation\nhis implementation demonstrates how to construct a one-dimensional cellular automaton system that can generate intricate sequences based on elementary rule sets. The patch showcases advanced concepts including rule-based pattern generation, boundary condition handling, state evolution tracking, and data persistence mechanisms.\n\n\n\nOne-dimensional cellular automaton patch implementing Rule 30 with iteration history display showing the evolution from a single seed cell into complex binary patterns.\n\n\n\n\n\n\nIteration\nBinary Output\n\n\n\n\n0\n0 0 0 0 0 0 1 0 0 0 0 0\n\n\n1\n0 0 0 0 0 1 1 1 0 0 0 0\n\n\n2\n0 0 0 0 1 1 0 0 1 0 0 0\n\n\n3\n0 0 0 1 1 0 1 1 1 1 0 0\n\n\n4\n0 0 1 1 0 0 1 0 0 0 1 0\n\n\n5\n0 1 1 0 1 1 1 1 0 1 1 1\n\n\n6\n0 1 0 0 1 0 0 0 0 1 0 0\n\n\n7\n1 1 1 1 1 1 0 0 1 1 1 0\n\n\n8\n1 0 0 0 0 0 1 1 1 0 0 0\n\n\n9\n1 1 0 0 0 1 1 0 0 1 0 1\n\n\n10\n0 0 1 0 1 1 0 1 1 1 0 1\n\n\n11\n1 1 1 0 1 0 0 1 0 0 0 1\n\n\n12\n0 0 0 0 1 1 1 1 1 0 1 1\n\n\n13\n1 0 0 1 1 0 0 0 0 0 1 0\n\n\n14\n1 1 1 1 0 1 0 0 0 1 1 0\n\n\n15\n1 0 0 0 0 1 1 0 1 1 0 0\n\n\n16\n1 1 0 0 1 1 0 0 1 0 1 1\n\n\n17\n0 0 1 1 1 0 1 1 1 0 1 0\n\n\n18\n0 1 1 0 0 0 1 0 0 0 1 1\n\n\n19\n0 1 0 1 0 1 1 1 0 1 1 0\n\n\n20\n1 1 0 1 0 1 0 0 0 1 0 1\n\n\n21\n0 0 0 1 0 1 1 0 1 1 0 1\n\n\n22\n1 0 1 1 0 1 0 0 1 0 0 1\n\n\n23\n0 0 1 0 0 1 1 1 1 1 1 1\n\n\n\n\n\n2.7.3.1 Patch Overview\nThe one-dimensional cellular automaton patch implements a comprehensive system for simulating elementary cellular automata, particularly focusing on Stephen Wolfram’s classification of 256 possible rules. The system operates by maintaining a linear array of cells, each containing a binary state (0 or 1), and applying transformation rules based on local neighborhoods of three consecutive cells. The patch provides sophisticated control over initial conditions (axioms), boundary behaviors, rule selection, and iteration management, making it suitable for both educational exploration and practical generative applications.\nThe architecture follows a modular design pattern with distinct functional subsystems that handle different aspects of the cellular automaton simulation. The rule management subsystem converts numerical rule codes into binary lookup tables that define state transitions for all possible three-cell neighborhoods. The boundary condition handler manages edge cases when the automaton reaches the limits of its defined space, supporting both loop-around and reflection modes. The iteration engine coordinates the step-by-step evolution of the cellular array, maintaining state history and providing real-time feedback on the generation process.\nThe patch interface accommodates various input methods for configuring the automaton parameters, including direct rule code entry (0-255), symbolic mode specification (loop, reflect, or fixed), and custom axiom definition through list messages. The system provides multiple output streams for accessing generated data, including real-time state information, complete iteration histories, and formatted data suitable for external analysis or visualization tools.\n\n\n2.7.3.2 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Rule Code Input] --&gt; B[Binary Rule Generation]\n    C[Axiom Definition] --&gt; D[Initial State Setup]\n    E[Mode Selection] --&gt; F[Boundary Configuration]\n    B --&gt; G[Rule Lookup Table]\n    D --&gt; H[Current State Array]\n    F --&gt; I[Edge Handler]\n    J[Iterate Bang] --&gt; K[State Evolution Engine]\n    H --&gt; K\n    G --&gt; K\n    I --&gt; K\n    K --&gt; L[Next Generation]\n    L --&gt; M[State History]\n    L --&gt; N[Data Output]\n    L --&gt; H\n\n\n\n\n\n\nThe cellular automaton’s operation begins with initialization phase where the rule code, boundary mode, and initial axiom are processed and stored in the system’s internal data structures. The rule code undergoes binary conversion through a specialized subpatch that transforms decimal values (0-255) into their eight-bit binary representations, which correspond to the output states for all possible three-cell neighborhood configurations (000, 001, 010, 011, 100, 101, 110, 111).\nThe axiom processing system accepts list-formatted initial conditions that define the starting configuration of the cellular array. This initial state is stored in a text object that serves as both the current generation reference and the foundation for subsequent iterations. The boundary mode configuration determines how the system handles edge conditions when calculating neighborhoods for cells at the array boundaries, with options for periodic (loop), reflective, or fixed boundary conditions.\nWhen an iteration is triggered through the iterate bang, the system enters its core evolution cycle. The current state array is processed through a neighborhood analysis routine that examines each cell in conjunction with its immediate neighbors. For each three-cell neighborhood, the system constructs a lookup key by treating the binary states as a three-bit number, which is then used to index into the previously generated rule table to determine the next state for the center cell.\nThe boundary condition handler plays a crucial role during this process, providing appropriate neighbor values for cells at the array edges based on the selected boundary mode. In loop mode, the array is treated as circular, with the leftmost and rightmost cells considered adjacent. Reflect mode mirrors the edge values inward, while fixed mode uses predetermined constant values for out-of-bounds references.\nThe resulting next generation is simultaneously output for immediate access and stored in the iteration history system. This dual-path approach allows for real-time monitoring of the automaton’s evolution while maintaining a complete record of all generated states. The history management system can accommodate both memory-efficient storage (retaining only the current state) and comprehensive archival (preserving all iterations) depending on the intended application.\n\n\n2.7.3.3 Key Objects and Their Roles\nThe following table summarizes the essential objects and subsystems used in the cellular automaton implementation:\n\n\n\n\n\n\n\n\nObject/Subsystem\nPurpose\n\n\n\n\nmake.code subpatch\nConverts rule codes to binary lookup tables and manages initialization\n\n\nfloat.to.binary subpatch\nTransforms decimal rule codes into 8-bit binary representations\n\n\ntext define $0-rules\nStores rule lookup table and configuration parameters\n\n\ntext define $0-outcome\nMaintains current generation state and iteration history\n\n\nlist-drip\nProcesses cell arrays sequentially for neighborhood analysis\n\n\nlist split 3\nExtracts three-cell neighborhoods from the current state\n\n\ntext search\nPerforms rule lookups based on neighborhood patterns\n\n\ntext get\nRetrieves rule outcomes and state information\n\n\nboundary.mode subpatch\nHandles edge conditions and neighbor calculation\n\n\nlist append/prepend\nManages array concatenation and state assembly\n\n\nroute\nDirects messages to appropriate processing subsystems\n\n\nv $0-iteration.n\nTracks current iteration number and generation count\n\n\nsave subpatch\nProvides data export functionality for generated sequences\n\n\n\n\nThe patch demonstrates several advanced programming techniques, including dynamic text object manipulation for rule storage, sophisticated list processing for array operations, and modular subpatch design for complex algorithmic implementations. The use of local variables ($0- prefix) ensures proper encapsulation when multiple instances of the patch are used simultaneously, while the declare objects manage external file dependencies for rule and outcome storage.\nThe cellular automaton implementation showcases how it can be used to create sophisticated algorithmic systems that bridge computational theory and practical creative applications. The modular architecture allows for easy experimentation with different rule sets, boundary conditions, and initial configurations, making it an excellent tool for exploring the rich behavioral space of elementary cellular automata and their potential applications in generative music, visual art, and algorithmic composition.\n\n\n\n2.7.4 Two-Dimensional Cellular Automata (2DCA)\nTwo-dimensional cellular automata are an extension of one-dimensional cellular automata, where cells not only have neighbors to the left and right, but also above and below. This allows modeling of more complex and structurally rich systems, such as two-dimensional phenomena like wave propagation, growth patterns in biology, fire spread, fluid simulation, among others.\nTwo-dimensional cellular automata (2DCA) are computational models that simulate dynamic systems on a two-dimensional grid.\nThey consist of:\n\nCells: Each cell in the grid can have a finite state, such as alive or dead, and can change state according to the automaton’s rules.\nNeighborhood: Each cell has a neighborhood, which is a set of adjacent cells that influence its state. The neighborhood can be rectangular, hexagonal, circular, or of any other shape.\nTransition rule: The transition rule defines how the state of a cell changes based on its current state and the state of the cells in its neighborhood. The rule can be deterministic or probabilistic.\nEvolution: The cellular automaton evolves through iterations. In each iteration, the state of each cell is updated according to the transition rule.\n\n\n\n2.7.5 Case Study: Conway’s Game of Life\n\nThe most well known cellular automaton is the Game of Life, a two-dimensional cellular automaton which has been invented by John Horton Conway in 1970. It is a simulation that describes the evolution of a population of cells across a two-dimensional grid of squares by three (or four, depending on the wording) simple rules. Every cell has one of the two states ”dead” and ”alive”, and the state changes of the cells depend on the number of living neighbor cells. When it was first published by Martin Gardner in the journal Scientific American in 1970, it was praised to have ”fantastic combinations” (Gardner 1970). Since its invention, thousands of patterns have been found by many people, and there seems to be no end in sight. Even after more than 50 years of history, interesting new patterns are still discovered.\nThe “game” is actually a zero-player game, meaning its evolution is determined by its initial state, requiring no further input from human players. One interacts with the Game of Life by creating an initial configuration and observing how it evolves.\nView Original Article - MATHEMATICAL GAMES - The fantastic combinations of John Conway’s new solitaire game life (Martin Gardner)\n\nGOL (Game of Life) and CGOL (Conway’s Game of Life) are commonly used acronyms.\n\n\n2.7.5.1 Rules\nThe universe of the Game of Life is an infinite two-dimensional orthogonal grid of square cells, each of which (at any given time) is in one of two possible states, alive (alternatively “on”) or dead (alternatively “off”). At each time step, the following transitions occur:\nThe four rules of Conway’s Game of Life:\n1. Overpopulation: Any live cell with more than three live neighbors dies due to overpopulation.\n2. Underpopulation: Any live cell with fewer than two live neighbors dies due to underpopulation.\n3. Stability: Any live cell with two or three live neighbors survives to the next generation.\n4. Reproduction: Any dead cell with exactly three live neighbors becomes a live cell.\n \nWe can also summarize the rules in a table:\n\n\n\n\n\n\n\n\n\n\nRule\nDescription\nCurrent State\nLive Neighbors\nNext State\n\n\n\n\nOverpopulation\nDeath by overcrowding\nAlive\nMore than 3\nDead\n\n\nUnderpopulation\nDeath by isolation\nAlive\nFewer than 2\nDead\n\n\nStability\nSurvival\nAlive\n2 or 3\nAlive\n\n\nReproduction\nBirth\nDead\n3\nAlive\n\n\n\n\n\n\n\n\n\n\nOr summarized as\n\n\n\n0 → 3 live neighbors → 1\n1 → &lt; 2 or &gt; 3 live neighbors → 0\nWhere 0 represents a dead cell and 1 a live cell.\n\n\n\n\n\n\nGame of Life rules diagram illustrating cell state transitions based on the number of living neighbors in the Moore neighborhood.\n\n\nJohn Conway’s Game of Life is made of an infinite grid of square cells, each of which is alive or dead. The rules below govern the evolution of the system. Every cell has eight neighbors, and all cells update simultaneously as time advances.\n\n\n\nConway’s Game of Life rule visualization showing the four fundamental rules governing cell evolution: underpopulation, survival, reproduction, and overpopulation based on neighbor count.\n\n\nThe initial pattern constitutes the system’s ‘seed’. The first generation is created by applying the above rules simultaneously to each cell in the seed; births and deaths occur simultaneously, and the discrete moment in which this occurs is sometimes called a step. (In other words, each generation is a pure function of the previous one.) The rules continue to be applied repeatedly to create more generations.\n\n\n\n2.7.6 Origins\nConway was interested in a problem presented in the 1940s by renowned mathematician John von Neumann, who was trying to find a hypothetical machine that could build copies of itself and succeeded when he found a mathematical model for such a machine with very complicated rules on a rectangular grid. The Game of Life emerged as Conway’s successful attempt to simplify von Neumann’s ideas.\nThe game made its first public appearance in the October 1970 issue of Scientific American, in Martin Gardner’s “Mathematical Games” column, under the title “The fantastic combinations of John Conway’s new solitaire game ‘Life’”.\nSince its publication, Conway’s Game of Life has attracted significant interest due to the surprising ways patterns can evolve.\nLife is an example of emergence and self-organization. It is of interest to physicists, biologists, economists, mathematicians, philosophers, generative scientists, and others, as it shows how complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counterintuitive notion that “design” and “organization” can spontaneously arise in the absence of a designer.\nConway carefully selected the rules, after considerable experimentation, to meet three criteria:\n\nThere should be no initial pattern for which a simple proof exists that the population can grow without limit.\nThere must be initial patterns that appear to grow indefinitely.\nThere should be simple initial patterns that evolve and change for a considerable period before ending in one of the following ways:\n\ndying out completely (due to overcrowding or becoming too sparse); or\nsettling into a stable configuration that remains unchanged thereafter, or entering an oscillating phase in which they repeat a cycle endlessly of two or more periods.\n\n\n\n\n2.7.7 Patterns\nMany different types of patterns occur in the Game of Life, including static patterns (“still lifes”), repeating patterns (“oscillators” – a superset of still lifes), and patterns that move across the board (“spaceships”). Common examples of these three classes are shown below, with live cells in black and dead cells in white.\n\n\n\nGosper glider gun\n\n\nUsing the provided rules, you can investigate the evolution of simple patterns:\n\n\n\n3 cells Source\n\n\n\n\n\n4 cells Source\n\n\nPatterns that evolve for long periods before stabilizing are called Methuselahs, the first of which discovered was the R-pentomino.\n\n\n\nR-pendomino source\n\n\nDiehard is a pattern that eventually disappears, rather than stabilizing, after 130 generations, which is believed to be the maximum for initial patterns with seven or fewer cells.\n\n\n\nDiehard source\n\n\nAcorn takes 5,206 generations to produce 633 cells, including 13 escaping gliders.\n\n\n\nAcorn source\n\n\nConway originally conjectured that no pattern could grow indefinitely; that is, for any initial configuration with a finite number of live cells, the population could not grow beyond some finite upper bound. The Gosper glider gun pattern produces its first glider in the 15th generation, and another every 30 generations thereafter.\n\n\n\nGosper’s glider gun source\n\n\n\n\n\nGosper glider gun\n\n\nFor many years, this pattern was the smallest known. In 2015, a gun called Simkin glider gun was discovered, which emits a glider every 120 generations, and has fewer live cells but is spread across a larger bounding box at its ends.\n\n\n\nSimkin glider gun\n\n\nFor a more detailed overview of gliders and other patterns, you can refer and extensive post by Swaraj Kalbande Conway’s Game of Life-Life on Computer (Kalbande 2022).\n\n\n2.7.8 Python Implementation of Game of Life\nimport time\nimport pygame\nimport numpy as np\n\nCOLOR_BG = (10, 10, 10,)  # Color de fondo\nCOLOR_GRID = (40, 40, 40)  # Color de la cuadrícula\nCOLOR_DIE_NEXT = (170, 170, 170)  # Color de las células que mueren en la siguiente generación\nCOLOR_ALIVE_NEXT = (255, 255, 255)  # Color de las células que siguen vivas en la siguiente generación\n\npygame.init()\npygame.display.set_caption(\"conway's game of life\")  # Título de la ventana del juego\n\n# Función para actualizar la pantalla con las células\ndef update(screen, cells, size, with_progress=False):\n    updated_cells = np.zeros((cells.shape[0], cells.shape[1]))  # Matriz para almacenar las células actualizadas\n\n    for row, col in np.ndindex(cells.shape):\n        alive = np.sum(cells[row-1:row+2, col-1:col+2]) - cells[row, col]  # Cálculo de células vecinas vivas\n        color = COLOR_BG if cells[row, col] == 0 else COLOR_ALIVE_NEXT  # Color de la célula actual\n\n        if cells[row, col] == 1:  # Si la célula actual está viva\n            if alive &lt; 2 or alive &gt; 3:  # Si tiene menos de 2 o más de 3 vecinos vivos, muere\n                if with_progress:\n                    color = COLOR_DIE_NEXT\n            elif 2 &lt;= alive &lt;= 3:  # Si tiene 2 o 3 vecinos vivos, sigue viva\n                updated_cells[row, col] = 1\n                if with_progress:\n                    color = COLOR_ALIVE_NEXT\n        else:  # Si la célula actual está muerta\n            if alive == 3:  # Si tiene exactamente 3 vecinos vivos, revive\n                updated_cells[row, col] = 1\n                if with_progress:\n                    color = COLOR_ALIVE_NEXT\n\n        pygame.draw.rect(screen, color, (col * size, row * size, size - 1, size - 1))  # Dibuja la célula en la pantalla\n\n    return updated_cells  # Devuelve las células actualizadas\n\n# Función principal del programa\ndef main():\n    pygame.init()\n    screen = pygame.display.set_mode((800, 600))  # Crea la ventana del juego\n\n    cells = np.zeros((60, 80))  # Crea una matriz de células muertas\n    screen.fill(COLOR_GRID)  # Rellena la pantalla con el color de la cuadrícula\n    update(screen, cells, 10)  # Actualiza la pantalla con las células\n\n    pygame.display.flip()\n    pygame.display.update()\n\n    running = False  # Variable para controlar si el juego está en ejecución\n\n    while True:\n        for Q in pygame.event.get():\n            if Q.type == pygame.QUIT:  # Si se cierra la ventana, termina el programa\n                pygame.quit()\n                return\n            elif Q.type == pygame.KEYDOWN:\n                if Q.key == pygame.K_SPACE:  # Si se presiona la tecla espacio, se inicia o pausa el juego\n                    running = not running\n                    update(screen, cells, 10)\n                    pygame.display.update()\n            if pygame.mouse.get_pressed()[0]:  # Si se presiona el botón izquierdo del ratón\n                pos = pygame.mouse.get_pos()  # Obtiene la posición del ratón\n                cells[pos[1] // 10, pos[0] // 10] = 1  # Marca la célula correspondiente como viva\n                update(screen, cells, 10)\n                pygame.display.update()\n\n        screen.fill(COLOR_GRID)  # Rellena la pantalla con el color de la cuadrícula\n\n        if running:  # Si el juego está en ejecución\n            cells = update(screen, cells, 10, with_progress=True)  # Actualiza las células con progreso\n            pygame.display.update()\n\n        time.sleep(0.001)  # Espera un breve tiempo para controlar la velocidad del juego\n\nif __name__ == \"__main__\":\n    main()\n\nThis Python script implements Conway’s Game of Life, a cellular automaton devised by British mathematician John Horton Conway. It is a zero-player game, meaning its evolution is determined entirely by its initial state, with no further input required.\nThe script begins by importing the necessary modules: time, pygame for the graphical interface, and numpy to handle the game grid as a 2D matrix. It then defines color constants used for visualizing the game.\nThe pygame.init() function is called to initialize all imported Pygame modules. The window title is set to “Conway’s Game of Life” using pygame.display.set_caption().\nThe update() function updates the game state and redraws the grid. It takes four arguments: screen (the Pygame surface to draw on), cells (the current game state as a 2D numpy array), size (the pixel size of each cell), and with_progress (a boolean indicating whether to display cells that will change in the next generation).\nThe function creates a new 2D matrix updated_cells filled with zeros, matching the shape of cells. It then iterates over each cell, calculates the number of live neighbors, and applies the game rules to determine whether the cell will be alive in the next generation. The function draws each cell on the screen using the appropriate color and returns updated_cells.\nThe main() function initializes Pygame, creates the game window, and initializes the game state as a 2D numpy array of zeros (representing dead cells). It then enters the main loop, which handles Pygame events (such as closing the window or key presses), updates the game state if it is running, and redraws the grid. The game can be started or paused by pressing the spacebar, and cells can be toggled manually by clicking on them.\nFinally, the script calls main() to launch the game. Press the spacebar to begin.\n\n\n2.7.9 Further Reading\n\nThe Game Of Life – Emergence In Generative Art (2020)\nConway’s Game of Life – Life on Computer by Swaraj Kalbande\nWikipedia – Conway’s Game of Life\n\n\n\n2.7.10 Interactive Websites\n\nJohn Conway’s Game of Life – An Introduction to Cellular Automata\nconwaylife.com\nCellular Automata Megathread",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/sequencing.html#references",
    "href": "chapters/sequencing.html#references",
    "title": "2  Sequencing",
    "section": "References",
    "text": "References\n\n\n\n\nBrown, Andrew. 2023. “Live Coding Patterns and a Toolkit for Pure Data.” Organised Sound 28: 1–12. https://doi.org/10.1017/S1355771823000365.\n\n\nBrummitt, Charles D., and Eric Rowland. 2012. “Boundary Growth in One-Dimensional Cellular Automata.” Complex Systems 21: 85–116. https://doi.org/10.48550/arXiv.1204.2172.\n\n\nEpstein, Paul. 1986. “Pattern Structure and Process in Steve Reich’s Piano Phase.” The Musical Quarterly 72 (4): 494–502.\n\n\nGardner, Martin. 1970. “MATHEMATICAL GAMES - the Fantastic Combinations of John Conway’s New Solitaire Game Life.” Scientific American 223 (4): 120–23. http://www.jstor.org/stable/24927642.\n\n\nKalbande, Swaraj. 2022. “Conway’s Game of Life – Life on Computer.” Medium. https://medium.com/@swarajkalbande123/conways-game-of-life-life-on-computer-b7edfc85d21a.\n\n\nReich, Steve. 2002. “Music as a Gradual Process.” In Writings on Music 1965–2000, 34–36. Oxford; New York: Oxford University Press.\n\n\nToussaint, Godfried. 2005. “The Euclidean Algorithm Generates Traditional Musical Rhythms.” In, 47–56.\n\n\nWolfram, Stephen. 1983. “Statistical Mechanics of Cellular Automata.” Reviews of Modern Physics 55 (3): 601–44. https://doi.org/10.1103/RevModPhys.55.601.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Sequencing</span>"
    ]
  },
  {
    "objectID": "chapters/recplay.html",
    "href": "chapters/recplay.html",
    "title": "3  Rec & Play",
    "section": "",
    "text": "3.1 Simple Audio Player\nIn this chapter, we will explore the relationship between recording and playback. We will look at how these two processes can be used to create new sounds and compositions. We will also discuss the technical aspects of recording and playback, including the equipment and techniques used in the process. We will also consider the artistic implications of recording and playback, including how these processes can be used to create new forms of expression and communication.\nThe simple audio player represents a streamlined approach to audio file playback that emphasizes ease of use while providing essential control over playback parameters. This Pd implementation demonstrates how to create an intuitive audio player interface that combines file loading, visual feedback, and precise playback control in a single cohesive system. The patch showcases fundamental concepts including automatic file loading, visual waveform display, start/stop position control, playback speed manipulation, and smooth parameter interpolation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/recplay.html#simple-audio-player",
    "href": "chapters/recplay.html#simple-audio-player",
    "title": "3  Rec & Play",
    "section": "",
    "text": "Simple audio player with waveform display, position control sliders, speed adjustment, and visual feedback.\n\n\n\n3.1.1 Patch Overview\nThe simple audio player patch implements a user-friendly playback system that automatically loads an audio file into a visual array and provides comprehensive control over playback parameters through an intuitive slider-based interface. The patch operates by calculating playback positions based on normalized start and stop points, which are then converted to sample-accurate indices for precise audio retrieval. The architecture emphasizes visual feedback through multiple display elements that show both the raw sample data and the calculated playback parameters in real-time.\nThe design centers around immediate usability, where a single \"play!\" button triggers playback of a predefined segment with user-controlled speed and timing parameters. The visual interface provides dual representation of the audio content through both a large waveform display that shows the entire audio file and smaller numeric displays that provide precise feedback on calculated values. The playback engine employs interpolated table reading to ensure high-quality audio reproduction across all speed settings, while the parameter calculation system converts user-friendly interface values into the sample-accurate indices required for proper audio playback.\nThe patch incorporates automatic initialization through a loadbang that ensures the audio file is loaded immediately when the patch opens, eliminating the need for manual file loading procedures. The control system provides independent manipulation of start position, stop position, playback speed, and interpolation timing, with all parameters working together to create a flexible and responsive playback environment suitable for both educational exploration and practical audio playback applications.\n\n\n3.1.2 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Loadbang] --&gt; B[File Loading]\n    B --&gt; C[Soundfiler]\n    C --&gt; D[Array Storage]\n    D --&gt; E[Sample Count]\n    F[Start Slider] --&gt; G[Position Calculation]\n    H[Stop Slider] --&gt; G\n    I[Speed Slider] --&gt; J[Duration Calculation]\n    E --&gt; K[Sample Index Conversion]\n    G --&gt; K\n    J --&gt; L[Interpolation Parameters]\n    K --&gt; L\n    M[Play Button] --&gt; N[Line~ Object]\n    L --&gt; N\n    N --&gt; O[Tabread4~]\n    D --&gt; O\n    O --&gt; P[Audio Output]\n\n\n\n\n\n\nThe audio player system begins its operation with an automatic initialization sequence triggered by the loadbang object at patch startup. This initialization sends a message to the soundfiler object instructing it to load the file “speech.wav” into the “audio” array with automatic resizing enabled. The soundfiler responds by reading the entire audio file into the array and returning the total number of samples, which becomes the fundamental reference value for all subsequent position and timing calculations. This sample count is immediately displayed in a numeric box and distributed throughout the system to serve as the basis for converting normalized slider positions into actual sample indices.\nThe position control system operates through two horizontal sliders that represent the start and stop positions as normalized values between 0 and 1. These normalized positions are processed through expression objects that multiply them by the total sample count, converting the user-friendly slider positions into precise sample indices that correspond to actual locations within the audio file. The start position calculation uses the expression int($f1*$f2) where the first input is the normalized slider value and the second is the total sample count, ensuring that the resulting index points to an exact sample location within the array.\nThe timing and speed control mechanism calculates the duration and interpolation parameters necessary for smooth playback transitions. The duration calculation subtracts the start sample index from the stop sample index using the expression $f2 - $f1, providing the total number of samples that will be played during the current playback segment. This duration value is then converted from samples to milliseconds using the expression abs(int(($f1 / 44100) * 1000)), which divides by the standard sample rate and multiplies by 1000 to produce a time value suitable for the interpolation system.\nThe speed control system modifies the calculated duration by multiplying it with the speed slider value through the expression int($f1*$f2). This calculation allows users to control playback speed, where values less than 1 produce slower playback and values greater than 1 produce faster playback. The resulting modified duration becomes the interpolation time parameter that determines how quickly the playback position moves from the start index to the stop index.\nWhen the \"play!\" button is triggered, all calculated parameters are combined into a message that drives the line~ object. The pack f f f object consolidates the start position, stop position, and interpolation time into a single list, which is then formatted by a message object into the proper syntax for the line~ object. This formatted message instructs the line~ to smoothly interpolate from the start sample index to the stop sample index over the calculated time duration, creating a continuously changing index value that scans through the selected portion of the audio file.\nThe final audio generation stage employs the tabread4~ object to retrieve audio samples from the array using the continuously changing index values provided by the line~ object. The fourth-order interpolation ensures smooth audio reproduction even when the playback speed produces non-integer index values or when rapid parameter changes occur. The interpolated audio output is then routed to the output~ object for final amplification and speaker routing, completing the signal path from stored audio data to audible output.\n\n\n3.1.3 Key Objects and Their Roles\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nsoundfiler\nLoads audio files into arrays and reports sample count\n\n\narray audio\nStores audio waveform data with visual display\n\n\ntabread4~\nPerforms interpolated audio reading from the array\n\n\nline~\nGenerates smooth ramp signals for position scanning\n\n\npack f f f\nCombines start, stop, and time parameters\n\n\nexpr\nPerforms mathematical calculations for position and timing\n\n\noutput~\nProvides stereo audio output with level control\n\n\n\nThe patch demonstrates several key Pd programming concepts, including automatic patch initialization through loadbang, visual array management for waveform display, mathematical expression evaluation for parameter conversion, and smooth audio-rate interpolation for high-quality playback. The modular design separates user interface elements from audio processing components, making the system easy to understand and modify. This implementation serves as an excellent foundation for understanding basic audio playback principles and can be easily extended with additional features such as looping, multiple file support, or external control integration. The visual feedback provided by the waveform display and numeric readouts makes it particularly suitable for educational applications where understanding the relationship between interface controls and audio parameters is important.\n\n\n3.1.4 Creative Applications\n\nExtreme time stretching: Push the speed control to very low values (0.1 or lower) to create drone-like textures from short audio snippets\nRhythmic patterns: Use rapid speed changes and phase jumps to generate percussive sequences from sustained tones\nReverse archaeology: Load full songs and use reverse playback to discover hidden melodic content and create backwards reveals\nGranular-style effects: Combine fast phase position changes with short loop lengths to simulate granular synthesis behaviors\nLive performance tools: Map MIDI controllers to speed and phase parameters for real-time manipulation during performances\nMicro-sampling: Load very short sounds (vocal consonants, instrument attacks) and stretch them to reveal internal acoustic details\nRhythmic displacement: Sync multiple samplers to the same clock but with different phase offsets to create polyrhythmic patterns",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/recplay.html#audio-sampler-with-phasor",
    "href": "chapters/recplay.html#audio-sampler-with-phasor",
    "title": "3  Rec & Play",
    "section": "3.2 Audio Sampler with [phasor~]",
    "text": "3.2 Audio Sampler with [phasor~]\nThe audio sampler represents a fundamental tool in digital audio processing and musical performance, enabling the capture, manipulation, and playback of recorded audio material. This Pd implementation demonstrates how to create a comprehensive sampling system that provides real-time control over playback speed, direction, position, and interpolation. The patch showcases essential concepts including array-based audio storage, variable-speed playback, phase control, and temporal interpolation for smooth parameter transitions.\n\n\n\nPhasor-based audio sampler demonstrating speed control, phase manipulation, and interpolated table reading with visual waveform display.\n\n\n\n3.2.1 Patch Overview\nThe audio sampler patch implements a playback engine that reads audio data from a table array and provides extensive real-time control over the playback parameters. The system operates by using a phasor oscillator to generate index values that scan through the stored audio samples, with multiplication factors applied to control playback speed and direction. The architecture supports both forward and reverse playback, variable speed control ranging from extreme slow motion to high-speed acceleration, and precise positioning within the audio material.\nThe design follows a modular approach where audio loading, parameter control, and playback generation are handled by distinct subsystems that communicate through Pd’s send and receive mechanism. The audio storage system utilizes a resizable array that automatically adjusts to accommodate different audio file lengths, while the playback control provides independent manipulation of speed, direction, interpolation time, and starting position. The output stage employs interpolated table reading to ensure smooth audio reproduction even during rapid parameter changes or extreme speed variations.\nThe interface provides immediate visual feedback through horizontal sliders for speed control, interpolation time adjustment, and phase positioning, along with numeric displays that show the current playback parameters and array position. This design enables both precise manual control and potential automation through external control sources, making it suitable for live performance, sound design, and experimental audio manipulation applications.\n\n\n3.2.2 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Audio File Loading] --&gt; B[Soundfiler Object]\n    B --&gt; C[Array Resize]\n    C --&gt; D[Sample Count Storage]\n    E[Speed Control] --&gt; F[Direction Multiplier]\n    F --&gt; G[Interpolation System]\n    G --&gt; H[Phasor Generation]\n    I[Phase Position] --&gt; H\n    J[Sample Rate] --&gt; K[Rate Calculation]\n    D --&gt; K\n    K --&gt; L[Base Frequency]\n    L --&gt; H\n    H --&gt; M[Table Reading]\n    M --&gt; N[Audio Output]\n\n\n\n\n\n\nThe patch begins its operation with the audio loading process, initiated by a message that triggers the soundfiler object to read an audio file into the designated array. The soundfiler automatically resizes the array to match the audio file length and returns the total number of samples, which is stored and distributed to other components through the send/receive objects. This sample count becomes crucial for calculating appropriate playback rates and ensuring the phasor oscillator operates within the correct frequency range.\nThe speed and direction control processes user input from the horizontal slider, which provides values ranging from -5 to +5, representing both playback speed and direction. Negative values produce reverse playback, while positive values generate forward playback, with the magnitude determining the speed multiplier. This control value is processed through an interpolation that uses the line object to create smooth transitions between different speed settings, preventing audible artifacts that might occur from abrupt parameter changes.\nThe core playback engine centers around a phasor~ oscillator that generates a continuously increasing ramp signal from 0 to 1. The frequency of this phasor is calculated by dividing the current sample rate by the total number of samples in the array, then multiplied by the speed control factor. This calculation ensures that a speed setting of 1 produces normal playback rate, while other values create proportional speed changes. The phasor output is then scaled to match the array size, creating index values that span the entire audio content.\nThe phase position control provides an additional layer of playback manipulation by allowing users to offset the starting position within the audio material. This parameter is multiplied with the scaled phasor output, enabling jumps to specific locations within the audio file or creating loop-like behaviors when combined with appropriate speed settings. The position control operates independently of the speed system, allowing for complex playback patterns and non-linear audio exploration.\nThe final playback stage employs the tabread4~ object, which performs fourth-order interpolated reading from the audio array. This interpolation method ensures smooth audio reproduction even when the playback speed produces non-integer index values or when rapid parameter changes occur. The interpolated output maintains audio quality across the full range of speed and position manipulations, providing professional-grade sample playback capabilities.\nReal-time monitoring and feedback systems track the current playback position and convert it to visual representations through the horizontal slider display and numeric readouts. The unsig~ objects convert audio-rate signals to control-rate values suitable for interface updates, while the positioning system provides continuous feedback about the current location within the audio material.\n\n\n3.2.3 Key Objects and Their Roles\n\n\n\n\n\n\n\nObject\nPurpose\n\n\n\n\nsoundfiler\nLoads audio files into arrays and reports sample count\n\n\ntabread4~\nPerforms interpolated audio reading from the array\n\n\nphasor~\nGenerates scanning ramp signal for array indexing\n\n\narray sampler\nStores audio data with automatic resizing capability\n\n\nexpr $f2 / $f1\nCalculates base playback rate from sample count and sample rate\n\n\nline\nProvides smooth interpolation between parameter changes\n\n\npack f f\nCombines value and interpolation time for line object\n\n\nsamplerate~\nReports current system sample rate\n\n\nunsig~\nConverts audio-rate signals to control-rate values\n\n\ns / r\nSend and receive objects for parameter distribution\n\n\n*~\nMultiplies audio signals for speed and direction control\n\n\noutput~\nProvides stereo audio output with level control\n\n\n\n This implementation serves as a foundation for more complex sampling applications, including granular synthesis, multi-sample instruments, and advanced audio manipulation tools. The real-time control capabilities make it suitable for live performance contexts, while the precise parameter control enables detailed studio work and sound design applications.\n\n\n3.2.4 Creative Applications\n\nCreate rhythmic patterns: Use rapid speed changes and phase jumps to generate percussive sequences from sustained tones\nGranular-style effects: Combine fast phase position changes with short loop lengths to simulate granular synthesis behaviors\nLive performance tools: Map MIDI controllers to speed and phase parameters for real-time manipulation during performances\nSound design layers: Layer multiple instances with different source materials and speed relationships to create complex evolving textures\nMicro-sampling: Load very short sounds (vocal consonants, instrument attacks) and stretch them to reveal internal acoustic details\nRhythmic displacement: Sync multiple samplers to the same clock but with different phase offsets to create polyrhythmic patterns",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/recplay.html#im-sitting-in-a-room-alvin-lucier",
    "href": "chapters/recplay.html#im-sitting-in-a-room-alvin-lucier",
    "title": "3  Rec & Play",
    "section": "3.3 I’m Sitting in a Room – Alvin Lucier",
    "text": "3.3 I’m Sitting in a Room – Alvin Lucier\nI’m Sitting in a Room (Alvin Lucier) consists of a 15-minute and 23-second sound recording. The piece opens with Lucier’s voice as he declares he is sitting in a room different from ours. His voice trembles slightly as he delivers the text, describing what will unfold over the next 15 minutes:\n\n“…I am recording the sound of my speaking voice and I am going to play it back into the room again and again…”\n\n\n\n\nI am sitting in a room design diagram illustrating the iterative recording and playback process that gradually transforms speech into pure room resonance.\n\n\nAs listeners, we know what is going to happen, but we don’t know how it will happen (Hasse 2012). We listen, following Lucier’s recorded voice. Then, Lucier plays the recording into the room and re-records it. This time, we begin to hear more of the room’s acoustic characteristics. He continues to play back and re-record his voice—over and over—until his speech becomes softened, almost dissolved, into the sonic reflections of the room in which the piece was recorded and re-recorded.\nOne effective way to study a piece is to replicate its technical aspects and the devices involved. The aim of this activity is to recreate the technical setup of I’m Sitting in a Room, focusing on the processes of recording, playback, and automation.\nThere are many ways to implement this technical system. Just to mention two environments we’re currently working with: it’s relatively straightforward in Pd. In terms of hardware, you’ll need:\n\n\n\n\n\n\n\n\nHardware Component\nSpecification\nPurpose\n\n\n\n\nComputer\nAny computer capable of running Pd\nHost system for running the patch\n\n\nSpeaker\nMono audio output\nPlays back recorded audio into the room\n\n\nMicrophone\nMono audio input\nCaptures audio from the room\n\n\nRoom\nSemi-reverberant acoustic space\nProvides natural acoustic reflections and resonances\n\n\n\n The system should include at least two manual (non-automated) controls:\n\nStart recording\nStop recording\n\nChoose a room whose acoustic or musical qualities you’d like to evoke. Connect the microphone to the input of tape recorder #1. From the output of tape recorder #2, connect to an amplifier and speaker. Use the following text, or any other text of any length:\n\nI am sitting in a room different from the one you are in now. I am recording the sound of my speaking voice, and I am going to play it back into the room again and again until the resonant frequencies of the room reinforce themselves so that any semblance of my speech, with perhaps the exception of rhythm, is destroyed. What you will hear, then, are the natural resonant frequencies of the room articulated by speech. I regard this activity not so much as a demonstration of a physical fact, but more as a way to smooth out any irregularities my speech might have.\n\nThe following steps outline the process:\n\nRecord your voice through the microphone into tape recorder #1.\nRewind the tape, transfer it to tape recorder #2, and play it back into the room through the speaker. - Record a second generation of the statement via the microphone back into tape recorder #1.\nRewind this second generation to the beginning and splice it to the end of the original statement in tape recorder #2.\nPlayback only the second generation into the room and record a third generation into tape recorder #1.\nContinue this process through multiple generations.\n\nAll the recorded generations, presented in chronological order, create a tape composition whose duration is determined by the length of the original statement and the number of generations produced. Make versions in which a single statement is recycled through different rooms. Create versions using one or more speakers in different languages and spaces. Try versions where, for each generation, the microphone is moved to different parts of the room(s). You may also develop versions that can be performed in real time.\n\n3.3.1 Pd implementation of I’m sitting in a room\n\n\n\n\nPd implementation of I am sitting in a room with recursive audio recording with dual file storage, playback controls, and room acoustic processing.\n\n\nThis patch is inspired by Alvin Lucier’s iconic piece and demonstrates how to recursively record and play back sound to reveal the resonant frequencies of a room.\n\n\n3.3.2 Patch Overview\nThe Pd implementation of I’m Sitting in a Room creates a digital recreation of Alvin Lucier’s seminal work through a carefully orchestrated system of recording, playback, and re-recording processes. This patch demonstrates how computational tools can faithfully reproduce the acoustic phenomena that drive Lucier’s piece, where the gradual accumulation of room resonances transforms speech into pure acoustic space. The implementation leverages Pd’s audio file handling capabilities, signal routing architecture, and user interface elements to create an interactive environment that allows users to experience the recursive transformation process in real-time.\nThe system architecture revolves around a dual-recording mechanism that alternates between two separate audio files, mimicking the original tape recorder setup used in Lucier’s piece. This alternating approach ensures that each generation of the recording process builds upon the previous one, creating the cumulative acoustic effect that defines the work. The patch employs a signal routing system that uses Pd’s send and receive objects to distribute audio signals to multiple destinations simultaneously, enabling the concurrent processes of playback monitoring and re-recording that are essential to the piece’s realization.\n\n\n3.3.3 Data Flow\nThe following diagram illustrates the recursive nature of the recording and playback process, similar to how Lucier’s piece unfolds. Each generation of tape recorders captures the previous one, creating a feedback loop that emphasizes the room’s resonances.\n\n\n\n\n\nflowchart TB\n  %% Top level source\n  Input[Microphone]\n\n  subgraph PLAYBACK RECORDERS\n    direction LR\n    subgraph RecorderA [DEVICE A]\n      RecordA[Record A.wav]\n      PlayA[Play A.wav]\n      RecordA -.-&gt; PlayA\n    end\n    \n    subgraph RecorderB [DEVICE B]\n      RecordB[Record B.wav]\n      PlayB[Play B.wav]\n      RecordB -.-&gt;PlayB\n    end\n  end\n  \n  %% Output at the bottom  \n  Output((Speaker))\n  \n  %% Clear connections showing signal flow\n  start([1-start]) --&gt; RecordA\n  stop([2-stop]) --&gt; RecordA\n  stop([2-stop]) --&gt; RecordB\n  stop([2-stop]) --&gt; PlayA\n\n  Input ==&gt; RecordA\n  Input ==&gt; RecordB\n\n  PlayA ==&gt; Output\n  PlayB ==&gt; Output\n  \n  %% Feedback path showing recursion\n  PlayB -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordA\n  PlayA -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordB\n  Output ==&gt; room[\"ROOM'S&lt;br&gt;REFLECTIONS\"]:::roomStyle ==&gt; Input\n\n  classDef roomStyle fill:#f5f5f5,stroke:#333,stroke-dasharray:5 5 \n\n\n\n\n\n\n\nThe following sequence diagram illustrates the process of recording and playback in the patch. It shows how the audio input is captured, recorded, played back, and re-recorded in a loop, emphasizing the room’s resonances.\n\n\n\n\n\n\nsequenceDiagram\n    participant Start as Start\n    participant Stop as Stop\n    participant Input as Mic\n    participant RecA as Rec A.wav\n    participant PlayA as Play A.wav\n    participant RecB as Rec B.wav\n    participant PlayB as Play B.wav\n    participant Output as Speaker\n    participant Room as Room\n\n    %% Generation 1\n    activate Input\n    Start--&gt;&gt;RecA: Start button\n    activate RecA\n    Input-&gt;&gt;RecA: Mic input\n\n\n    Stop--xRecA: Stop Rec A  \n    deactivate RecA   \n    Stop--&gt;&gt;RecB: Start Rec B\n    activate RecB \n    Input-&gt;&gt;RecB: Mic input          \n    Stop--&gt;&gt;PlayA: Start Play A\n    activate PlayA\n\n    PlayA-&gt;&gt;Output: Playback A.wav\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n    PlayA--xRecB: Trigger Stop Rec B\n    deactivate RecB\n    deactivate PlayA\n\n    PlayA--&gt;&gt;RecA: Trigger Start Rec B\n    activate RecA\n    Input-&gt;&gt;RecA: Mic input \n    PlayA--&gt;&gt;PlayB: Trigger Start Play A\n    activate PlayB\n\n    %% Generation 2\n    PlayB-&gt;&gt;Output: Playback B.wav\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n\n    %% Generation 3 (process repeats)\n    PlayB--xRecA: Trigger Stop Rec A\n    deactivate RecA\n    deactivate PlayB\n    PlayB--&gt;&gt;RecB: Trigger Start Rec B\n    activate RecB \n    Input-&gt;&gt;RecB: Mic input\n    PlayB--&gt;&gt;PlayA: Trigger Start Play A\n    activate PlayA\n    PlayA-&gt;&gt;Output: Playback\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n\n    Note over Input,Room: The process continues, alternating between devices and reinforcing the room's resonances.\n    deactivate Input\n    deactivate PlayA\n    deactivate RecB\n\n\n\n\n\n\n\nThe I’m Sitting in a Room patch operates through a carefully orchestrated sequence of recording, playback, and re-recording phases that mirror Alvin Lucier’s original tape-based methodology. The system begins with audio input acquisition, where live microphone signals from the adc~ object are immediately routed through the s~ audio.input send object to distribute the audio signal to multiple destinations within the patch. This centralized routing approach ensures that the same input signal can be simultaneously monitored and recorded without creating feedback loops or signal conflicts.\nThe initial recording phase commences when the user triggers the “Start Recording” button, which sends a message to open and start recording to record-A.wav through the first writesf~ object. The audio content is captured from the r~ audio.input receive object, ensuring that whatever audio is currently being input to the system—whether live microphone input or processed audio from previous generations—is properly recorded to the designated file. The recording process continues until the user manually triggers the “Stop Recording” button, which sends a stop message to terminate the current recording session and prepare the system for the next phase of operation.\nUpon stopping the initial recording, the patch automatically initiates the first playback and re-recording cycle through a carefully timed sequence of operations. The stop message triggers a bang that, after a brief delay implemented through the del 5 object, opens and begins playing record-A.wav through the readsf~ object. Simultaneously, this same trigger initiates the recording process for the second generation by opening record-B.wav and starting the second writesf~ object. The audio from the playback is routed through the throw~ audio object for real-time monitoring while also being sent to the r~ audio.input pathway, where it becomes the source material for the next recording generation.\nThe alternating recording mechanism forms the core of the recursive process, where each generation of recorded material becomes the source for the subsequent recording. When record-A.wav is played back, its audio content is automatically captured and recorded to record-B.wav. Upon completion of this recording cycle, the process reverses: record-B.wav is played back while record-A.wav is re-recorded with the processed audio content. This alternating pattern continues indefinitely, with each generation accumulating more of the room’s acoustic characteristics and gradually transforming the original speech content into pure room resonance.\nThe timing and synchronization system ensures smooth transitions between recording and playback phases through a network of interconnected bang objects and delay objects. When a recording session ends, the resulting bang signal triggers multiple simultaneous actions: stopping the current recording, initiating the playback of the just-recorded file, and beginning the recording of the next generation. The del 5 objects provide brief delays that prevent timing conflicts and ensure that file operations complete properly before subsequent operations begin. This timing mechanism is crucial for maintaining the integrity of each generation and preventing data corruption during the file switching operations.\nThe monitoring and output system allows users to hear the transformation process in real-time through the throw~ and catch~ audio objects, which collect audio signals from whichever playback operation is currently active. The catch~ audio object serves as a central mixing point that combines all active audio signals and routes them to the dac~ object for speaker output. This real-time monitoring capability is essential for understanding how the acoustic transformation unfolds over successive generations, allowing users to hear the gradual dissolution of speech content and the emergence of pure room resonance patterns.\nThe patch initialization and control ensures proper startup conditions through a loadbang object that automatically enables DSP processing when the patch opens. The control interface provides clearly labeled buttons for starting and stopping recordings, with visual feedback through the bang objects that indicate when various operations are triggered. The send and receive network (s start.Rec.A, r start.Rec.A, s stop.Rec.A, r stop.Rec.A) provides centralized control over the recording processes, allowing multiple parts of the patch to coordinate their operations without creating complex direct connections that would make the patch difficult to understand and maintain.\n\n\n3.3.4 Key Objects and Their Roles\n\n\n\nObject\nPurpose\n\n\n\n\nadc~\nAudio input from microphone\n\n\nwritesf~\nRecord audio to a file\n\n\nreadsf~\nPlay audio from a file\n\n\nthrow~/catch~\nMix and route audio signals\n\n\ndac~\nAudio output to speakers\n\n\n\n\n\n3.3.5 How to Use the Patch\n\nStart Recording:\nClick the “Start Recording” button and speak or make a sound.\nStop Recording:\nClick the “Stop Recording” button to finish.\nPlayback and Re-record:\nUse the playback button to play your recording into the room and simultaneously re-record it.\nRepeat:\nRepeat the playback and re-recording process as many times as you like to hear the room’s resonances emerge.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/recplay.html#i-am-sitting-in-a-freeverb",
    "href": "chapters/recplay.html#i-am-sitting-in-a-freeverb",
    "title": "3  Rec & Play",
    "section": "3.4 I am sitting in a [freeverb~]",
    "text": "3.4 I am sitting in a [freeverb~]\nThe patch example extends the original I am sitting in a room concept by introducing a virtual acoustic environment using reverb and allowing for pre-recorded audio input instead of just live microphone input. This adaptation provides more creative flexibility and consistent results compared to using a physical room.\n\n\n\nImplementation of “I am sitting in a room” with freeverb showing virtual acoustic processing, dual file recording system, and pre-recorded audio input capabilities for controlled recursive audio transformation.\n\n\nThis patch is designed to simulate the recursive recording process of Lucier’s piece while allowing for more control over the sound environment. It uses the freeverb~ object to create a virtual room effect, enabling you to manipulate the acoustic characteristics of the sound without relying on a physical space.\n\n3.4.1 Patch Overview\nThe I am sitting in a [freeverb~] patch represents an digital adaptation of Alvin Lucier’s seminal work that replaces the physical acoustic environment with a controllable virtual room simulation. This implementation shows how creative coding can extend conceptual art practices by providing enhanced control over the recursive recording process while maintaining the essential transformative characteristics of the original piece. The patch architecture combines pre-recorded audio file playback capabilities with live input processing, allowing users to experiment with different source materials and acoustic environments without the constraints of physical room acoustics.\nThe system operates through a dual-recording mechanism that alternates between two audio files while applying cumulative reverb processing. This approach ensures that each generation builds upon the previous one’s acoustic characteristics, creating the progressive transformation that defines Lucier’s work. The patch employs a comprehensive signal routing system using Pd’s send and receive architecture to manage the complex interconnections between playback, processing, and recording components, enabling seamless transitions between generations without manual intervention.\n\n\n3.4.2 Unique Features of the [freeverb~] Version\n\n3.4.2.1 Audio File Input\nUnlike the original patch that only uses microphone input, this version can:\n\nPlay a pre-recorded audio file (speech.wav) as the initial source\nProcess this file through reverb before recording\nTrigger playback with a button (labeled “2. Playback audio”)\n\n\n\n\n\n\nflowchart LR\n    bng([bng]) --&gt; msg[\"msg open speech.wav, 1\"]\n    msg --&gt; readsf[\"readsf~\"]\n    readsf --&gt; freeverb[\"freeverb~\"]\n    freeverb --&gt; send[\"s~ audio.input\"]\n\n\n\n\n\n\n\n\n3.4.2.2 Virtual Acoustic Environment\nInstead of relying on a physical room’s acoustics, this patch uses freeverb~ objects to create a simulated acoustic space:\n\nEvery audio signal (input and playback) passes through a freeverb~ object\nThis creates consistent reverberation that can be adjusted and controlled\nMultiple freeverb~ objects process different stages of the audio for cumulative effects\n\n\n\n3.4.2.3 Enhanced Control Flow\nThe patch includes numbered controls for better workflow:\n\n\n\n\n\n\n\n\nButton\nLabel\nFunction\n\n\n\n\n1\nStart recording\nBegins recording the input source to record-A.wav\n\n\n2\nPlayback audio\nPlays the speech.wav file through reverb into the system\n\n\n3\nStop recording\nStops the current recording process\n\n\n\n\n\n3.4.2.4 Multiple Send/Receive Paths\nThe patch uses additional send/receive pairs to manage signal routing:\n\ns~/r~ audio.input - Routes input signals to the recording object\ns~/r~ player.A - Routes playback from file A to recorder B\ns~/r~ player.B - Routes playback from file B back to recorder A\nthrow~/catch~ audio - Collects all signals to be sent to the output\n\n\n\n\n3.4.3 Data Flow\n\n\n\n\n\nflowchart TB\n  %% Top level source\n  AudioFile[Audio File]\n  \n  %% Horizontal arrangement of recorders A and B\n  subgraph PLAYBACK RECORDERS\n    direction LR\n    subgraph RecorderA [DEVICE A]\n      RecordA[writesf~ A.wav]\n      PlayA[readsf~ A.wav]\n      RecordA ==&gt; PlayA\n    end\n    \n    subgraph RecorderB [DEVICE B]\n      RecordB[writesf~ B.wav]\n      PlayB[readsf~ B.wav]\n      RecordB ==&gt; PlayB\n    end\n  end\n  \n  subgraph VIRTUAL ROOM\n  %% Reverb block in the middle\n    VirtualRoom(((freeverb~&lt;br&gt;Virtual Room)))\n  end  \n  \n  %% Output at the bottom  \n  Output[dac~]\n  \n  %% Clear connections showing signal flow\n  AudioFile --&gt; RecordA\n  AudioFile --&gt; Output\n  \n  PlayA ==&gt; VirtualRoom\n  PlayB ==&gt; VirtualRoom\n  \n  VirtualRoom ==&gt; RecordB\n  VirtualRoom ==&gt; RecordA\n  VirtualRoom --&gt; Output\n  \n  %% Feedback path showing recursion\n  PlayB -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordA\n  PlayA -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordB\n\n\n\n\n\n\nThe operation of this patch begins with the establishment of initial audio source options, where users can choose between pre-recorded file input or live input sources. For pre-recorded file input, clicking the “2. Playback audio” button triggers the playback of speech.wav through a readsf~ object, which then processes the audio through freeverb~ before sending it to the s~ audio.input routing system. This approach provides consistent source material that can be used repeatedly for experimentation and comparison across different processing generations. While live input capability is not explicitly shown in this version, the architecture could easily accommodate an adc~ object that would capture real-time microphone input and route it through the same s~ audio.input pathway.\nThe recording of the first generation commences when the user clicks the “1. Start recording” button, which initiates the capture process to record-A.wav. The signal from s~ audio.input is captured by the corresponding r~ audio.input object and sent directly to a writesf~ object that handles the actual file writing operations. This resulting file contains the source material with initial reverb processing applied, establishing the foundation for subsequent generations. The recording process maintains the temporal structure and amplitude characteristics of the original material while introducing the first layer of artificial room acoustics that will be progressively enhanced through the recursive process.\nThe playback and re-recording phase begins automatically after stopping the initial recording with the “3. Stop recording” button. The patch executes a carefully orchestrated sequence where it opens and plays record-A.wav through another readsf~ object, processes this playback through an additional freeverb~ object for cumulative reverb effects, and then routes the processed signal along two parallel paths. The first path sends the audio to throw~ audio for real-time monitoring, allowing users to hear the progressive transformation as it occurs. The second path directs the same signal to s~ player.A, where it is received and simultaneously recorded to record-B.wav, creating the next generation of the recursive process.\nThe recursive process continues as each newly created recording becomes the source for the subsequent generation. When record-B.wav is created, it can be played back through its own readsf~ object, processed through yet another freeverb~ instance, and routed to both the monitoring system via throw~ audio and to s~ player.B for capture back to record-A.wav. This alternating cycle can continue indefinitely, with each generation accumulating more of the virtual room’s characteristics as defined by the freeverb~ parameters. The cumulative effect gradually transforms the original source material into a sonic representation of the artificial acoustic space, with the original speech or audio content becoming increasingly obscured while the rhythmic and temporal elements remain as the structural foundation.\nThe output stage serves as the central collection and distribution point for all audio signals in the system. All audio signals that have been sent to throw~ audio are collected by the corresponding catch~ audio object, which functions as a mixing bus that combines signals from whichever recording generation is currently active. This consolidated audio signal is then sent to an out~ object that handles the final output routing to speakers or headphones, ensuring that users can monitor the entire process in real-time and experience the gradual transformation as it unfolds through successive generations.\n\n\n3.4.4 Key Objects Table\n\n\n\n\n\n\n\nObject\nPurpose in This Patch\n\n\n\n\nfreeverb~\nCreates virtual room acoustics by adding reverberation\n\n\nreadsf~\nPlays audio files (source material or previous generations)\n\n\nwritesf~\nRecords audio to file (for each generation)\n\n\ns~/r~\nRoutes audio between different parts of the patch\n\n\nthrow~/catch~\nMixes and outputs all audio signals\n\n\ndel\nAdds small delays to ensure proper sequencing of operations\n\n\nbng\nProvides buttons for user interaction\n\n\nout~\nSends audio to speakers/headphones\n\n\n\n\n\n3.4.5 Creative Applications\n\nExperiment with reverb settings: Try different room sizes, damping, and wet/dry mix\nUse different source materials: Test various speech samples, musical phrases, or sound effects\nCreate hybrid processes: Record the first generation in a real room, then switch to the virtual environment\nBuild compositional sequences: Layer different generations to create evolving textures\nCompare real vs. virtual: Process the same source through both the original and freeverb patches to contrast physical and virtual acoustics\n\nThis virtual approach offers a controlled laboratory for exploring the core concepts of Lucier’s piece, making it accessible even without an ideal acoustic space, while also opening new creative possibilities that wouldn’t be possible in a physical implementation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/recplay.html#references",
    "href": "chapters/recplay.html#references",
    "title": "3  Rec & Play",
    "section": "References",
    "text": "References\n\n\n\n\nHasse, Sarah. 2012. “I Am Sitting in a Room.” Body, Space & Technology 11. https://doi.org/10.16995/bst.71.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html",
    "href": "chapters/synthesis.html",
    "title": "4  Sound Synthesis",
    "section": "",
    "text": "4.1 Between Technique and Aesthetics\nSound synthesis is the art and science of producing sound, whether by transforming existing recordings or by generating new audio signals electronically or mechanically. It can be grounded in mathematics, physics, or even biology, and it emerges at the intersection of creative expression and technical knowledge. As both a musical and computational process, synthesis enables the shaping of sound with precision and imagination, allowing for the creation of sonic textures that range from the familiar to the entirely new.\nAt its core, synthesis is a process of construction—a method for bringing elements together into a cohesive whole. This notion of “making” is essential, as it signals not just assembly, but intention. Synthesis is inherently creative. While the tools and technologies involved—oscillators, filters, software environments like Pd—are increasingly accessible and powerful, they remain just that: tools. It is the skill, intuition, and aesthetic sensibility of the artist or programmer that gives them meaning.\nModern tools have made sound synthesis more available than ever. From high-fidelity sample libraries to algorithmic engines capable of real-time manipulation, the technological barrier has lowered dramatically. But while the means have changed, the importance of the human element remains. Without creative intention, sound synthesis risks becoming a purely mechanical operation.\nIt is a common misconception to associate synthesis solely with complex or futuristic electronic timbres. In fact, synthesized sound spans a vast range—from simple sine waves to layered, expressive textures. The process itself is plural: it includes subtractive, additive, granular, physical modeling, and many other techniques. Each offers a different lens on how sound can be built and manipulated.\nInterestingly, the term “synthesis” has roots that extend beyond audio. It also refers to the creation of new compounds in chemistry or the combining of ideas into a theory. In sound, as in those other domains, synthesis implies thoughtful construction—a deliberate act of shaping something new from constituent parts. Throughout this chapter, we will return often to the idea of synthesis not just as a technique, but as a mode of inquiry and expression. It invites experimentation, demands understanding, and rewards careful listening.\nOscillatory motion will be analyzed in detail—not only in terms of technical implementation but also through its creative and conceptual applications. Using example Pd patches provided, we will explore how oscillation becomes an expressive tool in interactive works and live sound experiments. This approach will form the core of the module’s practical component, where students are encouraged to adapt the exercises to their own interests: whether by replicating existing works, developing new compositions, or analyzing the results of comparative experiments between different oscillators and their spectral behaviors.\nThis chapter begins with the idea of delving deeper into oscillatory movements from both a technical and aesthetic perspective. Building upon tools introduced in previous chapters we will work with oscillators and explore their many applications. Drawing on the text The Poetics of Signal Processing (Sterne and Rodgers 2011), we will approach signal processing not merely as a set of mathematical operations, but as a form of artistic expression. We will explore what could be described as the “rawness” of analog oscillators—that tangible, organic quality inherent in their operation. We will examine how this rawness both contrasts with and complements the digital techniques we’ve developed so far. The use of oscillators in their various forms will serve as a starting point for a series of hands-on exercises, where theory and practice are deeply intertwined.\nThrough this combination of theory, practice, and critical reflection on signal processing, the module aims to open up new sonic possibilities. The references provided are intended not only to contextualize student work, but also to inspire expanded thinking about how these principles might inform personal projects. This practical and flexible framework is reflected in the module’s initial activity on oscillatory movement. Readers will have the freedom to choose their working environment and the context in which they wish to develop their experiments. Oscillator implementation can be approached from multiple angles: waveform manipulation, waveshaper design, or even the creation of interactive sound pieces. In this way, the module not only deepens our understanding of the technical aspects of signal processing, but also invites critical reflection on the role of these technologies in shaping meaning and sonic aesthetics. Special emphasis is placed on the oscillator’s capacity to act as a driver of creativity and expression.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sound Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#between-technique-and-aesthetics",
    "href": "chapters/synthesis.html#between-technique-and-aesthetics",
    "title": "4  Sound Synthesis",
    "section": "",
    "text": "4.1.1 Poetics of Signal Processing\nOne of the most important aspects of this chapter is the idea of “poetics” in signal processing. This concept is not just about the technical aspects of sound manipulation, but also about the cultural and artistic implications of these processes. The term “poetics” suggests a deeper engagement with the materiality of sound and its representation in various contexts. I’m going to talk a bit about my reflections and observations on the text Poetics of Signal Processing (Sterne and Rodgers 2011). First, I want to mention the authors, who I find very interesting. The first is Jonathan Sterne, a professor and director of the Culture and Technology program at McGill University in Canada. His work focuses on the cultural dimension of communication technologies, and he specializes in the history and theory of sound in the modern Western world. Then we have another author, a sound artist and musician named Tara Rodgers. She’s an electronic music composer, programmer, and historian of electronic music. She holds a PhD in Communication and has worked in the field of Women’s Studies. She created the platform Pink Noises, which is worth checking out—especially because it gathers a series of interviews with improvisers, composers, and instrument builders. There’s a cross-section of gender, sexuality, feminism, music, sound studies, theater, performance, and performative arts in general.\nLet’s get into some comments on specific sections of the article. The first is “The Sonic Turn,” which describes the emergence of a new culture of listening beginning in the second half of the 20th century. This is discussed through the work of authors like Cox and Kahn. There’s also a growing interest in oral history and anthropology among social scientists, and the emergence of sound art within the art world during the 20th century.Another key point is the growing interest in listening itself, and in the creative possibilities enabled by recording, reproduction, and other forms of sound transmission. This leads us to ask: where do today’s sound technologies come from? What are we going to address in this course?\nWe can start by discussing the “audible past,” or what the article refers to as the “auditory past.” The text mentions that between 1900 and 1925, sound becomes an object of thought and practice. Before this period, sound was thought of in more idealized terms—mainly through the lens of voice and music, along with all the structures they imply. During this period, there were significant socioeconomic and cultural shifts—capitalism, rationalism, science, and colonialism—that influenced ideas and practices related to sound and listening. These changes were not only cultural but social as well. In this “audible past,” even the most basic mechanical elements of sound reproduction technologies were shaped by how they had been used up until then. In this way, sound technologies are tied to habits—they sometimes enable new habits, like new ways of listening, or sometimes they solidify and reinforce existing ones.\nSo let’s reflect on the “poetics” of signal processing. At first, that might sound surprising—poetics? But we can start with a very basic unit: the signal. Signals have a certain materiality. Sound has materiality—it occupies space in a transmission, recording, or playback channel. It exists in a medium and can be manipulated in various ways. There’s a clear distinction between analog electrical signals, electronic signals, and digital ones. Each implies different content and meanings. So, signal processing occurs in the medium of sound transmission, but in a technologized era—that is, the present. This involves manipulating sound in what we might call a “translucent state.” Here, the transducer becomes central: we’ll talk a lot in the course about this idea of converting one form of energy—measurable by a certain magnitude—into another. This concerns almost everything in sound or image that reaches our senses through electronic media. This also exists in the domains of the musician, the playback device, the listener, and the interstices between them.\nRegarding the poetics of signal processing as signal [music plays briefly], the article refers mostly to the figurative dimensions of the process itself. These are the ways that processing is represented in the discourse of audio technology, particularly from a technical or engineering perspective. Signal processing carries cultural meanings—it’s not an isolated technical fact. It has cultural significance. Two metaphorical frameworks commonly used in everyday language among users and creators are discussed in the article: cooking and travel. These are two metaphors I find fascinating.\nLet’s begin with cooking—specifically, “the raw and the cooked.” This draws from the work of anthropologist Claude (Lévi-Strauss 1964), who analyzed the raw, the cooked, and the rotten. The axis of raw/cooked belongs to culture, while fresh/rotten relates more to nature. This is a very important reference: cooking is a cultural operation. Fire—the act of cooking—is the basis of a social order, of stability.\nIn this sense, when we talk about the raw and the cooked in relation to sound, rawness doesn’t mean purity. It’s a relative condition—it refers to the availability of audio for further processing. This is very useful when thinking in opposition to the Hi-Fi culture. We might even think of a scale of rawness, or various degrees of it, which relate to how sound can be manipulated and used.\nWithin this raw/cooked metaphor, terms like slicing (cutting into slices) and dicing (cutting into cubes) appear—these are actual signal processing terms and very fitting metaphors.\nThinking further with this metaphor: raw audio might be seen as passive, something that must be “cooked” through technological processes. This reveals the technologized nature of music technologies, where composition becomes a kind of masculine performance of technological mastery. And I want to stress this point again: composition is often framed as a male-dominated act of technical skill.\nPaul Théberge (Théberge 1997) analyzes musicians as consumers within the sound tech industry. In one chapter of his book, he studies advertising in music tech magazines, showing how the marketing of music technologies has been directed predominantly at men. Fortunately, this is changing slowly. In the raw/cooked metaphor, the idea of sound as a material to be processed and preserved for future use emerges in the late 19th century—just like technologies developed to preserve and can food. It’s a very strong metaphor: processed food and processed sound were both invented to extend and control organic life through technological preservation.\nNow let’s move to the other metaphor—travel. Signal processing can be thought of as a journey, which I find very exciting. We can connect this to topology—a mathematical field that studies spatial relationships. The term topos refers to place. In this sense, we can think of electronics as the arrangement and interaction of various components. A synthesizer circuit, or an oscillator, can be conceived as a space in itself—a map. Early texts in electroacoustics from the late 19th and early 20th centuries started to describe sound and electricity as fluid media. They used water metaphors—talking about waves, oscillations, flow, and current. This “processing as travel” metaphor involves the idea of particles moving through space, with the destination originally being the human ear. Today, that destination could be a transducer—or a computer.\nEven the inner ear was once conceptualized as a terrain made up of interconnected parts through which vibrations would travel. These metaphors matter: sea voyages during the historical periods mentioned in the article also symbolized scientific exploration and the conquest of the unknown. One example I love is that in the 1800s, Lord Kelvin created what could be considered the first synthesizer—but it didn’t produce sound. It was a mechanical device designed to predict tides. I’ll share some images to illustrate this. It essentially summed simple waves into one more complex waveform.\nSo, to conclude this idea of processing as travel, the text also reflects on how maritime metaphors privilege a particular kind of subject—a white, Western male as the ideal “navigator” of synthetic sound waves. This is a clearly colonial and masculinist rhetoric. Generating and controlling electronic sounds becomes associated with a kind of pleasure aligned with capitalism, and also with danger—of disobedient or unruly sounds.\nSwitching to a less metaphorical aspect, the text discusses Helmholtz’s On the Sensations of Tone (Helmholtz 1954). It laid the epistemological foundations for synthesis techniques. Helmholtz argued that any sound could be broken down into volume, pitch, and timbre. For him, sound was a material with clearly defined properties. These properties could be analyzed and then mimicked using synthesis techniques. However, other researchers, like Jessica Roland, explored different approaches. She compared sound to things like rain and wind. Her approach emphasized experience, memory, and the use of synthesis as a kind of onomatopoeia—imitation of natural phenomena. For Roland, unpredictability and chaos are at the heart of synthesis.\nIn conclusion, one of the key points of the article is that metaphors in audio-technical discourse—supposedly neutral or instrumental—are actually shaped by the cultural positions of specific subjects living in specific societies. They are deeply entangled with issues of gender, race, class, and culture. The language of technical culture is highly metaphorical and filled with implicit assumptions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sound Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#sound-sources",
    "href": "chapters/synthesis.html#sound-sources",
    "title": "4  Sound Synthesis",
    "section": "4.2 Sound Sources",
    "text": "4.2 Sound Sources\nOscillators are a fundamental component of sound synthesis and play a crucial role in the creation of electronic music. They generate periodic waveforms, which can be manipulated to produce a wide range of sounds. In this section, we will explore the different types of oscillators, their characteristics, and how they can be used creatively in sound design.\n\n4.2.1 Oscillators\nSound sources in synthesizers are largely based on mathematics. There are two fundamental types:\n\nwaveforms\nrandom signals (noise)\n\nWaveforms are typically described as simple geometric shapes—sawtooth, square, pulse, sine, and triangle being the most common. These shapes are mathematically straightforward and electronically feasible to generate. On the other hand, random waveforms produce noise, a constantly shifting mixture of all frequencies.\n\n\n\nBasic sound sources showing sine oscillator for tonal content and noise generator for random spectral content.\n\n\nOscillators are one of the core building blocks of synthesizers, often implemented as function generators, which produces a waveform that may be continuous or triggered and can take arbitrary shapes. In a basic analog subtractive synthesizer, an oscillator usually outputs a few continuous waveforms, with frequency controlled by voltage. Since these sources typically output a continuous signal, modifiers must be applied to shape timbre or envelope the sound.\n\n\n4.2.2 Sine & Cosine\nA pure tone consists of a single frequency and is produced by a sine wave oscillator, which can be implemented using either the sine or cosine function. These functions take an angle value, or “phase,” as input. Below, we see the angle “alpha,” the sine’s amplitude value, and the cosine’s output denoted as x.\n\n\n\nUnit circle with angle alpha demonstrating the geometric foundation of sine and cosine functions\n\n\nIn the resulting graph, amplitude is on the vertical axis and angle on the horizontal axis. The cosine output traces the same function as the sine but starts at 1, meaning it has a different initial phase. Sine and cosine are essentially the same waveform offset by 90 degrees of phase.\n\n\n\nSine wave as the fundamental periodic waveform\n\n\nPd’s native [sin] and [cos] objects take angle values in radians \\((0 \\text{ to } 2\\pi)\\). However, the audio object [cos~] uses a linear range from 0 to 1 to represent a full cycle. The ELSE library provides the [pi] object, which outputs the constant \\(\\pi\\). This can be stored in a [value] object and accessed within [expr]. To convert a linear 0–1 range into radians, multiply it by \\(2\\pi\\):\n\\[\\text{radians} = \\text{linear\\_input} \\times 2\\pi\\]\nThen, [cos] and [sin] yield amplitude values accordingly:\n\\[\\text{amplitude} = \\cos(\\text{radians}) \\text{ or } \\sin(\\text{radians})\\]\n\n\n\n\n\nSine and cosine oscillators converting linear phase input to trigonometric waveform output\n\n\n\n\n\n4.2.3 Phasor\nIn the following example, we implement a sine oscillator using the [sin~] object and the native [phasor~] object.\n\n\n\nLinear phase ramp from phasor~ (top) converted to sine wave output (bottom), illustrating the basic oscillator construction using phase-to-amplitude conversion\n\n\nTwo graphs illustrate this: in the top one, the horizontal axis is time, and the vertical axis shows a steadily increasing phase, forming a linear ramp. In the bottom graph, this ramp is transformed into a sine waveform, with amplitude on the vertical axis.\nThe [phasor~] object outputs a linear ramp from 0 to just under 1, representing a complete cycle. It is ideal for driving objects like [cos~] and [sin~], which expect a 0–1 input representing phase progression.\nThe input to [phasor~] is frequency, expressed in cycles per second (hertz). This defines how many full 0–1 cycles occur per second.\nNote that [phasor~] never actually reaches 1—it wraps around to 0. Due to its cyclic nature, 1 is functionally equivalent to 0, just like 360° equals 0° in circular geometry.\nThe output of [phasor~] can be described as a “running phase.” It defines the angular increment applied to the phase at every audio sample.\n\n\n\n\nPhasor~ object generating linear phase ramp from 0 to 1 at specified frequency\n\n\n\n\n\n4.2.4 Oscillator\nIn the analog domain, oscillators are commonly referred to as VCOs (Voltage Controlled Oscillators). VCOs allow frequency or pitch to be controlled via voltage. Some VCOs also feature voltage control inputs for modulation (typically FM) and for altering the waveform shape—usually the pulse width of square waves, although some VCOs allow shaping other waveforms as well.\nMany VCOs include an additional input for synchronization with another VCO’s signal. Phase sync forces the VCO to reset its phase in sync with the incoming signal, limiting operation to harmonics of the input frequency. This results in a harsh, buzzy tone. Softer sync techniques can yield timbral variations rather than locking to an exact frequency.\nA typical VCO offers controls for coarse and fine tuning, waveform selection (often sine, triangle, square, sawtooth, and pulse), pulse width modulation (PWM), and output level. Some VCOs also provide multiple simultaneous waveform outputs and sub-octave outputs one or two octaves below the main signal. Pulse width modulation (PWM) allows dynamic alteration of the pulse waveform shape.\nTo summarize, an oscillator is typically defined by:\n\nWaveform function (sine, sawtooth, square, triangle)\nFrequency (Hz)\nInitial phase (degrees)\nPeak amplitude (optional)\n\nHow do we control these parameters in our model?\n\n\n\n4.2.5 VCO\nIn this example, [phasor~] and [cos~] form an oscillator. The [cos~] object outputs amplitude values from -1 to 1, yielding a maximum amplitude of 1 without additional gain control.\n\n\n\nPhasor~ and cos~ oscillator combination with frequency control input, creating a basic VCO equivalent in the digital domain.\n\n\nThe waveform produced is a cosine. While [phasor~] sets the frequency, it can also define the initial phase. Since sine and cosine are essentially phase-shifted versions of the same function, we can easily produce sine waves as well. However, the initial phase does not affect the perceived pitch of a pure tone.\nTry this patch with different phase offsets. Note that [phasor~] also accepts negative frequencies, reversing the phase direction.\nConnecting [phasor~] to [cos~] replicates the functionality of the [osc~] object.\n\n\n\n4.2.6 Waveforms\nThe sine wave is the simplest oscillator, generating a pure tone. Other basic and musically useful waveforms include triangle, sawtooth, and square.\nThe sine wave is a smooth, rounded waveform based on the sine function. It contains only one harmonic—the fundamental—which makes it less suitable for subtractive synthesis as it lacks overtones to filter.\nA triangle wave consists of two linear slopes. It contains small amounts of odd harmonics, providing just enough spectral content for filtering.\nA square wave contains only odd harmonics and produces a hollow, synthetic sound. A sawtooth wave contains both odd and even harmonics and sounds bright. Some pulse waves may contain even more harmonic content than basic sawtooth waves. Variants like “super-saw” replace linear slopes with exponential ones and alternate teeth with gaps, producing an even richer harmonic spectrum.\n\n\n4.2.7 Waveshapers\nThis section focuses on creating oscillators in Pd using the [phasor~] object. The only true oscillator in Pd Vanilla is [osc~], a sine wave oscillator. Even standard waveforms must be built manually.\nAs mentioned earlier, [osc~] is essentially [phasor~] connected to [cos~]. The [phasor~] object outputs a 0–1 ramp, functionally similar to a sawtooth wave with half the amplitude and an offset:\n\\[0 \\leq \\text{phasor output} &lt; 1\\]\n[cos~] multiplies this ramp by \\(2\\pi\\) and computes its cosine:\n\\[\\text{Output} = \\cos(2\\pi \\times \\text{phasor output})\\]\nThe result is a sine wave oscillator where the complete transformation can be expressed as:\n\\[\\begin{align*}\n\\text{Phase} &= \\text{phasor}(f) \\\\\n\\text{Sine wave} &= \\cos(2\\pi \\times \\text{Phase})\n\\end{align*}\\]\n\n\n\nMathematical approach to sine wave generation using expr~ with pi calculation stored in value object for educational clarity.\n\n\nAnother way to construct this oscillator is by using [expr~] and [value]. Here we use the [pi] abstraction to calculate π (or approximate it by sending 1 to [atan] and multiplying the result by 4). We then multiply by 2 and store it in a [value] object. [value] acts like a global variable: any object using the same name accesses the same value. [expr~] can then use this to calculate the cosine, just like [cos~]. While this approach may be more CPU-intensive, it helps deepen understanding of oscillator construction in Pd.\n\n\n\nBasic sine oscillator implementation using phasor~ phase generator and cos~ waveshaper\n\n\n\n4.2.7.1 Sawtooth Oscillator\nSince [phasor~] already produces a ramp, generating a sawtooth wave is straightforward. Simply multiply [phasor~] by 2 to get the correct amplitude, then subtract 1 to shift the range to -1 to 1.\n\n\n\nSawtooth oscillator using phasor~ with amplitude scaling and DC offset adjustment.\n\n\n\n\n4.2.7.2 Square Wave Oscillator\nTo create a square wave, you can use [expr~] (included in Pd Vanilla), but [&gt;~] is faster and more CPU-efficient. A square wave toggles between -1 and 1. The [&gt;~] object compares its left input signal to a threshold (right input or argument). It outputs 1 if the input is greater than the threshold, and 0 otherwise. Using 0.5 as the threshold with a [phasor~] input yields a square wave: 0 for half the cycle, 1 for the other half.\n\n\n\nSquare wave generator using phasor~ input compared against 0.5 threshold with &gt;~ object for efficient binary waveform creation.\n\n\n\n\n4.2.7.3 Triangle Wave Oscillator\nAmong standard waveforms, the triangle wave is the most complex to construct. Starting with [phasor~], which is an upward ramp from 0 to 1, we create an inverted version by multiplying it by -1 and then adding 1. This gives us a descending ramp from 1 to 0.\nNow we have both ascending and descending ramps. Sending both to [min~] (which outputs the smaller of two values) gives us a triangle waveform spanning 0 to 0.5. [min~] effectively splices the ascending and descending ramps to form a symmetric triangle wave.\n\n\n\nTriangle oscillator construction using phasor~ and inverted ramp processed through min~ to create symmetric triangular waveform.\n\n\n\n\n\n4.2.8 Frequency\nFrequency is presented here in terms of angular velocity! One common unit of measurement is the hertz (Hz), which equals “cycles per second.” Frequency also determines a period of oscillation, which is simply the inverse of frequency. For example, a frequency of 100 Hz corresponds to a period of 0.01 seconds (or 10 milliseconds):\n\\[\\text{Period} = \\frac{1}{\\text{Frequency}}\\] \\[\\text{Period} = \\frac{1}{100 \\text{ Hz}} = 0.01 \\text{ s} = 10 \\text{ ms}\\]\n\n\nWe can convert between Hz and milliseconds using this same relationship. Another way to express period is in number of samples, which requires the sampling rate to perform the conversion:\n\nAngular velocity units require both an angle and a time unit. One cycle per second defines a full cycle (360 degrees) as the angular unit, and seconds as the time unit. Other units are also possible. For example, the angle may be expressed in radians and the time unit as a single sample, yielding a unit of “radians per sample.”\nTo convert Hz to radians per sample, multiply by 2π and divide by the sampling rate. See below for this conversion and the [hz2rad] and [rad2hz] objects from the ELSE library that handle it.\n\n\n4.2.9 Phase\nThe term phase can be used in various contexts, often making it ambiguous and potentially confusing. A useful strategy is to adopt more specific terminology instead of simply referring to “phase” in isolation. On its own, “phase” refers to a stage within a cycle—much like the four phases of the moon. Sound waveforms are cyclical, and we can speak of a positive or negative phase, as shown below. However, this original meaning is rarely used in music theory. Here, we focus on other more relevant applications and interpretations of phase (see right and below).\n\n\n\nSine wave phase diagram showing positive and negative half-cycles that define different stages within a complete oscillation cycle.\n\n\nInitial phase refers to the point in the cycle where the oscillation begins.\nInstantaneous phase: In music theory, “phase” often refers to the instantaneous phase—a specific point in time, not a stage in a sequence. It’s helpful to adopt the term instantaneous explicitly to denote a single position within a given cycle.\nSince instantaneous phase refers to a single position within a cycle, it can also be represented as an angle. This leads to a synonymous relationship between phase and angle, though it’s important to stress that both denote a position within the cycle.\n\n\n\nInstantaneous phase visualization showing the relationship between angular position and waveform generation\n\n\nTwo oscillators operating at the same frequency can be in phase or out of phase. Being in phase means they are synchronized—there is no phase difference. Being out of phase indicates a lack of synchronization, i.e., a phase difference. This difference can take many forms, but two specific cases are of particular interest: quadrature phase and phase opposition.\nQuadrature phase is the phase difference between sine and cosine waves, which equals a quarter of a cycle (90 degrees).\nPhase opposition is the maximum possible phase difference—half a cycle or 180 degrees.\n\n\n4.2.10 Polarity\nAs we’ve seen, phase opposition leads to signal cancellation—but only under certain waveform conditions! This occurs with sine waves, for instance, but not with all waveforms or signals. Note how, in the figure to the right, inverting the sign of every amplitude value in a waveform results in cancellation when added to the original signal.\nInverting polarity means changing the sign, or multiplying by -1. For sine waves, this results in the same effect as a 180-degree phase opposition. However, a true phase inversion is different, as it involves a time or phase shift.\nDespite this distinction, the term phase inversion is often misused when it really refers to a polarity inversion—which is neither a time shift nor a phase shift.\nYes, this can be very confusing and requires careful attention. That’s why this tutorial prefers the term polarity inversion, although many audio devices refer to a 180-degree phase shift when they are, in fact, performing a polarity inversion.\nBoth phase and polarity inversion produce the same result for sine waves due to their symmetrical waveforms, where the second half of the cycle mirrors the first with opposite sign. Other waveforms with this property include triangle and square waves (with a 0.5 pulse width).\n\n\n\nWaveform polarity comparison showing sine wave cancellation through inversion versus sawtooth wave asymmetry that prevents phase-based cancellation.\n\n\nSawtooth waves, however, do not share this symmetry. Therefore, phase opposition is not equivalent to polarity inversion in this case. The only way to achieve full cancellation of a sawtooth wave is through polarity inversion. Refer to the graphs below: only the original sawtooth combined with its polarity-inverted version results in complete cancellation.\nThe native [phasor~] and [osc~] objects include a right inlet that accepts control data to reset the phase. Whenever the inlet receives a number from 0 to 1, the waveform resets to that initial phase position. Note that this is unrelated to phase modulation techniques discussed earlier.\n\n\n\nSawtooth polarity inversion\n\n\nThe [osc~] object does not support phase modulation; we implemented it using [phasor~] in the previous examples. Hence, using a [phasor~] together with a [cos~] enables both phase modulation and oscillator resetting.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sound Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#additive-synthesis",
    "href": "chapters/synthesis.html#additive-synthesis",
    "title": "4  Sound Synthesis",
    "section": "4.3 Additive Synthesis",
    "text": "4.3 Additive Synthesis\nAdditive synthesis is a method for constructing complex waveforms by combining multiple sine waves of different frequencies and amplitudes. Unlike subtractive synthesis, which begins with a harmonically rich signal and removes unwanted components through filtering, additive synthesis starts from fundamental building blocks—sine waves—and gradually assembles them into a desired sound. While the conceptual clarity of this approach is appealing, its practical implementation typically demands a user interface capable of managing a high number of parameters simultaneously, often making additive synthesizers more complex to operate.\nThe theoretical foundation of additive synthesis lies in the work of the French mathematician Jean-Baptiste Joseph Fourier. In 1807, Fourier demonstrated that any periodic waveform could be represented as a sum of sine waves, each with a specific frequency and amplitude. This revelation forms the basis of what we now call Fourier analysis and synthesis. While Fourier’s original formulation applied only to repetitive waveforms to maintain mathematical tractability, modern techniques extend these principles to non-periodic signals by allowing sine wave parameters to vary over time.\nA helpful analogy is to imagine describing written text over a telephone to someone who has never seen it. One might begin by breaking words into letters, and letters into lines, curves, and dots. This level of description works well if the words remain static. However, if the words change, the description must be updated continuously. Likewise, Fourier synthesis allows us to describe and reproduce complex waveforms from basic sinusoidal components, provided we are equipped to update these components when the signal changes.\nThe simplest application of Fourier synthesis is the reconstruction of a sine wave—which, unsurprisingly, requires just one sine wave. A sine wave contains only a single frequency component—the fundamental—and no additional harmonics.\n\n4.3.1 Harmonics and Waveform Construction\nMore complex waveforms are created by adding sine waves at integer multiples of the fundamental frequency. These multiples are called harmonics. If the fundamental frequency is denoted by ( f ), then the additional components will be ( 2f, 3f, 4f, ), corresponding to the second, third, fourth harmonics, and so on. This organization gives rise to the standard harmonic series found in many acoustic and electronic sounds.\nThe terminology surrounding harmonics often overlaps with that of overtones. The first harmonic (fundamental) has no overtone. The second harmonic is the first overtone, the third harmonic is the second overtone, and so forth. This relationship is summarized in the following table:\n\n\n\nFrequency\nHarmonic\nOvertone\n\n\n\n\n( f )\n1\n—\n\n\n( 2f )\n2\n1\n\n\n( 3f )\n3\n2\n\n\n( 4f )\n4\n3\n\n\n( 5f )\n5\n4\n\n\n( 6f )\n6\n5\n\n\n( 7f )\n7\n6\n\n\n( 8f )\n8\n7\n\n\n( 9f )\n9\n8\n\n\n( 10f )\n10\n9\n\n\n\n\n\n4.3.2 Harmonic Synthesis\nAlthough additive synthesis often begins with the goal of reconstructing a known waveform, this is a limited perspective. In practice, the visual shape of a waveform does not reliably indicate its harmonic content. Minor alterations in phase relationships can drastically alter the shape of a waveform without affecting its sonic character. This is especially true at higher frequencies, where the human auditory system is less sensitive to phase differences.\nFor example, sine waves are considered perceptually pure. Introducing odd-numbered harmonics yields a triangular waveform, which retains a clean quality but feels richer than a pure sine. Square waves contain only odd harmonics, lending them a hollow timbre. If the second harmonic of a square wave is phase-shifted, the visual waveform changes, yet the auditory perception remains largely unaffected. Sawtooth waves, containing both even and odd harmonics, are brighter and richer in harmonic content. Modifying the phase of their components alters the shape without significantly changing the perceived sound.\nPulse waves, derived from narrow square waves, introduce more harmonics as the duty cycle deviates from 50%. Interestingly, a 10% and a 90% pulse wave share identical harmonic content, illustrating the symmetry of their spectrum. The square wave (50% duty cycle) is unique in that it contains only odd harmonics. By contrast, a waveform composed exclusively of even harmonics resembles a square wave shifted up by an octave—its fundamental frequency is ( 2f ).\n\n\n4.3.3 Practical Considerations\nPerfect waveforms, such as ideal square waves, require an infinite number of harmonics. In practice, synthesizers approximate these shapes with a limited set of partials, usually between 32 and 64 harmonics. For instance, a fundamental frequency of 55 Hz (low A) yields the 32nd harmonic at 1760 Hz and the 64th at 3520 Hz. At 440 Hz (concert A), the 45th harmonic reaches 19,800 Hz, which is near the upper limit of human hearing.\n\n\n\nFrequency\nHarmonic\n\n\n\n\n55 Hz\n1\n\n\n110 Hz\n2\n\n\n165 Hz\n3\n\n\n220 Hz\n4\n\n\n275 Hz\n5\n\n\n…\n…\n\n\n3520 Hz\n64\n\n\n\n\n\n4.3.4 Inharmonic Content and Real-World Complexity\nReal-world sounds often exhibit inharmonic components—frequencies that are not integer multiples of the fundamental. These include:\n\nNoise: Broad-spectrum energy with no harmonic organization.\nBeat Frequencies: Resulting from detuned harmonics, producing low-frequency amplitude modulations.\nSidebands: Occurring from modulation, producing spectral mirror images around a carrier frequency.\nInharmonics: Regularly structured but non-harmonic components, typical of metallic or bell-like sounds.\n\nMany additive synthesizers limit themselves to harmonic content, occasionally supplemented by noise generators. While deterministic in design, this approach overlooks the stochastic and dynamic nature of real-world sound phenomena.\n\n\n4.3.5 Harmonic Analysis\nTo construct realistic timbres, additive synthesis relies on knowledge of harmonic spectra from real instruments. Fourier analysis provides a systematic method for decomposing complex signals into their constituent sine waves. This is conceptually akin to sweeping a narrow band-pass filter across the spectrum and measuring energy at each frequency. The narrower the filter bandwidth, the higher the resolution. Simple musical tones require relatively coarse resolution; more complex sounds demand finer spectral discrimination.\nFourier analysis can be performed with analog hardware or more commonly with digital processing techniques, offering flexibility and precision for real-time applications.\n\n\n4.3.6 Envelopes in Additive Synthesis\nControlling the amplitude of each harmonic over time requires an envelope generator (EG) and a voltage-controlled amplifier (VCA) per harmonic. Ideally, the final sound envelope results from the sum of individual harmonic envelopes. To simplify control while preserving expressivity, a global EG and VCA can also be applied post-summing to modulate the overall output.\nOne common configuration is the ADSR envelope—Attack, Decay, Sustain, Release—which requires only four parameters and is relatively easy to implement in analog circuitry. Though more complex designs like DADSR offer additional flexibility, they demand more components and are usually reserved for digital implementations.\n\n\n\nClassic ADSR envelope shape illustrating attack, decay, sustain, and release phases\n\n\nAdditive synthesis provides a powerful framework for sound construction, especially when paired with modern computational tools. It invites a deep understanding of harmonic behavior and spectral composition, allowing composers and designers to sculpt timbre with surgical precision—layer by layer, sine by sine.\nThe following example created by Miller Puckette (Puckette 2007) encapsulate an ADSR envelope generator within a reusable Pd abstraction named adsr. This design enables easy replication of the envelope generator and flexible parameter control, either through creation arguments or real-time updates via control inlets.\nThe abstraction accepts five creation arguments, corresponding respectively to the:\n\npeak level\nattack time\ndecay time\nsustain level (expressed as a percentage of the peak), and\nrelease time\n\nFor instance, the arguments [adsr 1 100 200 50 300] define a peak amplitude of 1.0, an attack time of 100 milliseconds, a decay time of 200 milliseconds, a sustain level at 50% of the peak, and a release time of 300 milliseconds.\n\n\n\nMiller Puckette’s ADSR abstraction with creation arguments and control inlets for dynamic envelope parameter adjustment\n\n\nIn addition to these arguments, the abstraction provides six control inlets. The first inlet serves as the trigger input, responsible for initiating or releasing the envelope. The remaining five inlets allow dynamic control over each of the ADSR parameters, overriding the corresponding creation arguments when values are received. The abstraction outputs an audio signal that modulates amplitude according to the ADSR envelope.\nInternally, the implementation of the adsr abstraction is both efficient and modular. The signal path consists of the standard [line~] object, used to generate smooth audio ramps, and [outlet~], which delivers the envelope signal to the external patch. Control flow within the abstraction is handled through three [pack] objects—each corresponding to a message that controls one segment of the envelope: attack, decay, and release.\nThe attack segment uses a [pack] object to format a message that instructs [line~] to ramp from 0 to the peak level over the attack time duration. This peak level is represented by $1, the first creation argument, while the duration is $2, the second. Both values can also be modified via the corresponding control inlets.\nThe decay segment presents a more complex behavior. Once the attack segment is completed, the envelope should transition from the peak to the sustain level. This transition is delayed using [del $2], which defers execution by the duration of the attack. The sustain level is computed by multiplying the peak value by the sustain percentage (the fourth creation argument or inlet), and dividing by 100. This result becomes the new target value for [line~], reached over the duration specified by the decay time.\nThe release segment is comparatively straightforward. Upon receiving a trigger value of 0, the abstraction sends a ramp to zero over the specified release time. This is implemented using a [pack] object that combines the zero target with the release time value, which is either $5 (from creation arguments) or provided via the corresponding inlet.\nThe trigger logic supports three types of behavior:\n\nA positive number initiates an attack and decay cycle.\nA zero triggers the release segment, allowing the envelope to fade back to zero.\nA negative number functions as a reset, immediately sending the output to zero and initiating a new onset.\n\nThis flexibility enables the abstraction to support a variety of amplitude modulation scenarios in real-time audio synthesis, making it both pedagogically instructive and practically versatile for sound design tasks.\n\n\n4.3.7 Twenty-Oscillator Harmonic System\nThis Pd patch implements a comprehensive additive synthesis engine that shows the fundamental principles of harmonic spectrum construction through the controlled combination of multiple sine wave oscillators. The system employs twenty independent oscillator modules, each operating at harmonically related frequencies with individual amplitude envelope control, creating a powerful platform for exploring the timbral possibilities of additive synthesis while maintaining real-time performance capabilities.\nThe patch architecture emphasizes modular design through the use of custom abstractions, enabling scalable harmonic control while providing educational insight into the mathematical relationships that govern harmonic series construction. By implementing individual envelope generators for each partial, the system demonstrates how complex timbral evolution can be achieved through coordinated amplitude control across multiple frequency components.\n\n\n\nAdditive synthesis engine with twenty independent oscillators, each with harmonic frequency relationships and individual envelope control\n\n\n\n4.3.7.1 Patch Overview\nThe additive synthesis system implements a comprehensive harmonic generation pipeline consisting of four primary processing stages:\n\nGlobal Parameter Control: Manages fundamental frequency and timing coordination\nIndividual Oscillator Modules: Twenty independent oscilador abstractions with harmonic frequency relationships\nAmplitude Envelope Management: Coordinated envelope generation across all partials\nAudio Signal Summation: Collection and mixing of all oscillator outputs\n\nThe system demonstrates classic additive synthesis principles where complex timbres emerge from the controlled combination of simple sinusoidal components, each contributing specific harmonic content to the overall spectral composition.\n\n\n4.3.7.2 Critical Processing Components\noscilador Abstraction: Each instance implements a complete harmonic partial with integrated envelope generation. The abstraction receives two arguments: harmonic number (1-20) and envelope duration in milliseconds, enabling independent control over both frequency relationships and temporal characteristics.\nGlobal Parameter Distribution: The send/receive system (s freq, s timeScale) ensures synchronized parameter updates across all oscillator instances while maintaining modular architecture that supports easy scaling to different numbers of partials.\nAudio Signal Aggregation: The catch~ audio object employs Pd’s audio signal collection mechanism to sum all oscillator outputs without explicit connection cables, demonstrating efficient signal routing in complex patches.\n\n\n4.3.7.3 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Global Frequency Control] --&gt; B[Twenty Harmonic Oscillators]\n    B --&gt; C[Individual Envelope Generation]\n    C --&gt; D[Audio Signal Collection]\n    D --&gt; E[Mixed Output]\n    F[Global Time Scaling] --&gt; C\n    \n    style A fill:#e1f5fe\n    style E fill:#f3e5f5\n    style C fill:#fff3e0\n\n\n\n\n\n\nThe additive synthesis patch orchestrates a multi-oscillator system that transforms simple frequency and timing parameters into complex harmonic spectra through coordinated oscillator management and envelope control. This implementation demonstrates how additive synthesis principles can be systematically applied to create timbral complexity from elementary sinusoidal components while maintaining real-time performance and user control.\nThe global parameter management stage establishes the foundation for harmonic spectrum generation through centralized control of fundamental frequency and timing coordination. When users adjust the frequency control, the new value propagates through the s freq send object to all twenty oscillator instances simultaneously, ensuring that harmonic relationships remain mathematically correct as the fundamental frequency changes. This distribution system maintains the precise frequency ratios that define harmonic series relationships, where each oscillator operates at an integer multiple of the fundamental frequency. The timing control system operates in parallel through the s timeScale, enabling users to adjust the overall temporal character of the synthesis while preserving the relative envelope timing relationships between different partials. The hradio interface provides discrete envelope duration options that range from brief percussive attacks to sustained harmonic evolutions, allowing for diverse timbral expressions within the additive framework.\nThe individual oscillator processing stage represents the core of the additive synthesis engine, where each oscilador abstraction implements a complete harmonic partial with sophisticated envelope control. Each oscillator instance receives two critical arguments during instantiation: the harmonic number (1, 2, 3… up to 20) and the base envelope duration in milliseconds. The harmonic number determines the frequency multiplication factor applied to the global fundamental frequency, creating the precise integer relationships that define harmonic spectra. Within each oscilador abstraction, the frequency calculation multiplies the received fundamental frequency by the harmonic number, generating frequencies such as 440 Hz (fundamental), 880 Hz (second harmonic), 1320 Hz (third harmonic), and so forth. The envelope duration argument provides each oscillator with its temporal characteristics, which are further modified by the global time scaling factor to enable coordinated timing adjustments across the entire harmonic spectrum.\nThe envelope generation and coordination stage implements sophisticated temporal control that enables complex timbral evolution through coordinated amplitude modulation across all harmonic partials. Each oscilador abstraction contains an independent envelope generator that controls the amplitude trajectory of its associated sine wave oscillator. The envelope system employs bang-triggered activation that initiates synchronized envelope sequences across all oscillators, creating coherent timbral attacks and evolutions. The envelope duration calculation combines the base duration argument with the global time scaling factor, enabling users to modify the overall temporal character of the synthesis while maintaining the relative timing relationships between different partials. This coordination mechanism allows for sophisticated timbral effects where lower harmonics might sustain longer than higher harmonics, or where specific partials activate at different times to create complex spectral evolution patterns.\nThe audio signal collection and output stage demonstrates efficient signal routing architecture that aggregates all oscillator outputs without requiring complex cable connections between modules. Each oscilador abstraction contains a throw~ audio object that sends its processed audio signal to the global collection point implemented by the catch~ audio object in the main patch. This signal routing approach enables the system to sum all twenty harmonic partials automatically while maintaining clean patch organization and supporting easy modification of the oscillator count. The collected audio signal undergoes final processing through the out~ object, which provides level control and audio interface connectivity for monitoring and recording the synthesized output.\n\n\n\n\n\nflowchart LR\n    A[Parameter Distribution] --&gt; B[Harmonic Generation]\n    B --&gt; C[Envelope Application]\n    C --&gt; D[Signal Collection]\n\n\n\n\n\n\n\n\n4.3.7.4 Processing Chain Details\nThe additive synthesis system maintains harmonic precision and temporal coordination through several interconnected processing stages:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nUser Controls\nGlobal parameter distribution\nSynchronized frequency and timing\n\n\n2\nGlobal Parameters\nHarmonic frequency calculation\nTwenty harmonic frequencies\n\n\n3\nHarmonic Frequencies\nSine wave generation with envelopes\nIndividual partial signals\n\n\n4\nMultiple Audio Signals\nSignal aggregation and mixing\nCombined harmonic spectrum\n\n\n\n\n\n4.3.7.5 Harmonic Frequency Distribution\n\n\n\n\n\n\n\n\n\nOscillator\nHarmonic Number\nFrequency Relationship\nExample (440 Hz fundamental)\n\n\n\n\n1\n1\nf × 1\n440 Hz\n\n\n2\n2\nf × 2\n880 Hz\n\n\n3\n3\nf × 3\n1320 Hz\n\n\n4\n4\nf × 4\n1760 Hz\n\n\n5\n5\nf × 5\n2200 Hz\n\n\n6-10\n6-10\nf × 6-10\n2640-4400 Hz\n\n\n11-15\n11-15\nf × 11-15\n4840-6600 Hz\n\n\n16-20\n16-20\nf × 16-20\n7040-8800 Hz\n\n\n\n\n\n4.3.7.6 Envelope Timing Control\nThe time scaling system provides proportional control over envelope durations:\n\n\n\nhradio Setting\nTime Scale Factor\nEffect on 1000ms Base Duration\n\n\n\n\n0\n0.1\n100 ms (very fast attack)\n\n\n1\n0.25\n250 ms (fast attack)\n\n\n2\n0.5\n500 ms (medium attack)\n\n\n3\n1.0\n1000 ms (normal attack)\n\n\n4\n2.0\n2000 ms (slow attack)\n\n\n5\n5.0\n5000 ms (very slow attack)\n\n\n\n\n\n4.3.7.7 Signal Architecture\nModular Design: Each oscillator operates independently while sharing global parameters, enabling flexible harmonic control and easy system modification.\nEfficient Routing: The throw/catch system eliminates cable clutter while maintaining clear signal flow organization.\nScalable Architecture: The abstraction-based design supports easy addition or removal of harmonic partials without patch restructuring.\n\n\n4.3.7.8 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Additive Synthesis\n\n\n\n\ns freq / r freq\nGlobal frequency distribution\nDistributes fundamental frequency to all oscillators\n\n\noscilador\nHarmonic oscillator abstraction\nImplements individual partial with envelope control\n\n\ncatch~ audio\nAudio signal collection\nAggregates all oscillator outputs\n\n\ns timeScale / r timeScale\nTiming distribution\nCoordinates envelope durations across oscillators\n\n\nout~\nAudio output\nFinal signal delivery\n\n\n\n\n\n4.3.7.9 Creative Applications\n\nClassic Additive Synthesis: Recreate the timbres of historical additive synthesizers like the Kawai K5000 or early computer music systems by manually adjusting individual harmonic amplitudes\nSpectral Animation: Use external control data (MIDI, OSC, sensors) to modulate individual oscillator amplitudes, creating dynamic spectral evolution and movement\nOrgan Stop Simulation: Configure different combinations of harmonics to simulate pipe organ stops, drawbar organs, or other harmonic instruments\nSpectral Morphing: Sequence different harmonic amplitude configurations to create smooth timbral transitions between distinct spectral shapes\nMicrotonal Additive Synthesis: Modify individual oscillator frequencies to explore non-harmonic spectra and microtonal intervals while maintaining the additive framework\nAlgorithmic Composition: Use mathematical functions or stochastic processes to control harmonic amplitudes over time, creating evolving textures with embedded structural logic\n\n\n\n\n4.3.8 Additive Clone: Scalable Harmonic Synthesis\nThis Pd patch implement a dynamic additive synthesis using Pd’s clone object to create a scalable system of harmonic oscillators. Unlike fixed multi-oscillator systems, this implementation allows real-time adjustment of the number of active partials, from 1 to 20, while maintaining proper amplitude scaling to prevent clipping. The patch showcases advanced Pd programming techniques including dynamic object creation, parameter distribution, and automatic scaling based on psychoacoustic principles.\nThe system emphasizes efficiency and modularity by using identical harmonic abstractions that receive initialization parameters and operate independently. This approach enables exploration of how partial count affects timbral complexity while demonstrating the relationship between harmonic density and perceived loudness.\n\n\n\nDynamic additive synthesis system using clone object for scalable harmonic generation with real-time partial count control and psychoacoustic amplitude scaling\n\n\n\n4.3.8.1 Patch Overview\nThe additive clone system implements a dynamic harmonic generation pipeline consisting of four primary processing stages:\n\nPartial Count Control: Interface for selecting number of active harmonics (1-20)\nParameter Generation: Algorithmic calculation of frequency multipliers and amplitude coefficients\nDynamic Instance Management: Clone-based creation of harmonic oscillators with scaling\nAudio Collection: Signal aggregation with psychoacoustic amplitude compensation\n\nThe patch demonstrates how algorithmic parameter generation can control large numbers of synthesis components while maintaining musical coherence and preventing amplitude overflow.\n\n\n4.3.8.2 Critical Dynamic Components\nClone Object Management: The clone harmonic object dynamically creates instances based on the resize message, enabling real-time adjustment of synthesis complexity without patch reconstruction.\nPsychoacoustic Scaling: The expr~ pow($f2, -0.6) calculation in each harmonic applies amplitude reduction based on the total number of active partials, using research-based scaling that matches human loudness perception.\nModular Parameter Generation: The until and mod system creates a counting sequence that generates unique harmonic numbers for each clone instance while handling array wraparound.\n\n\n4.3.8.3 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Partial Count Selection] --&gt; B[Parameter Generation Loop]\n    B --&gt; C[Clone Instance Creation]\n    C --&gt; D[Individual Harmonic Processing]\n    D --&gt; E[Amplitude Scaling]\n    E --&gt; F[Audio Collection]\n    F --&gt; G[Mixed Output]\n    \n    style A fill:#e1f5fe\n    style G fill:#f3e5f5\n    style E fill:#fff3e0\n\n\n\n\n\n\nThe additive clone patch implmenting a dynamic synthesis system that adapts the number of active harmonic oscillators in real-time while maintaining proper amplitude relationships and preventing audio clipping. This implementation demonstrates how Pd’s clone object can be combined with algorithmic parameter generation to create scalable synthesis systems that remain musically coherent across different levels of harmonic complexity.\nThe partial count control stage establishes the foundation for dynamic synthesis through user interface management that directly controls system complexity. The hradio interface enables selection of partial counts from 1 to 20, with each selection triggering a complete reconfiguration of the synthesis system. The selected value propagates through two parallel pathways: immediate distribution via the s n-partials send system that informs all existing harmonic instances of the new partial count, and initiation of the parameter generation sequence that calculates appropriate settings for the requested number of oscillators. This dual distribution ensures that both the scaling calculations within individual harmonics and the overall system configuration remain synchronized as users adjust the synthesis complexity in real-time.\nThe algorithmic parameter generation stage transforms the simple partial count selection into comprehensive initialization data for each harmonic oscillator through systematic mathematical processing. The until object creates an iteration loop that executes the specified number of times, generating unique parameter sets for each harmonic instance. During each iteration, the counter value undergoes processing through the mod object, which provides frequency multiplier values that cycle through a predetermined sequence, enabling harmonic relationships that extend beyond simple integer multiples if desired. The counter value also serves directly as the harmonic number identifier, while the fundamental frequency received through r freq provides the base frequency reference. These three values combine in the pack f f f object to create complete initialization messages that contain all information necessary for each harmonic instance to configure its frequency relationships and amplitude characteristics.\nThe dynamic instance management stage demonstrates Pd’s advanced object creation capabilities through an implementation of the clone system that adapts to changing synthesis requirements. When the resize message reaches the clone harmonic object, it dynamically adjusts the number of active harmonic instances, creating new oscillators when the count increases or deactivating instances when the count decreases. Each parameter pack message generated by the algorithmic calculation stage initializes a harmonic instance with its specific frequency multiplier and amplitude coefficient. The clone system manages this distribution automatically, ensuring that each active instance receives appropriate initialization data while maintaining efficient resource utilization. This dynamic approach enables the system to scale from simple single-oscillator tones to complex 20-partial harmonic spectra without requiring manual patch reconfiguration or predetermined oscillator allocation.\nThe individual harmonic processing stage occurs within each harmonic abstraction instance, where distributed parameters control sophisticated oscillator and envelope systems enhanced with automatic amplitude scaling. Each harmonic receives its frequency multiplier through the initialization message and combines it with the global fundamental frequency to generate its specific harmonic frequency. The critical innovation within each harmonic involves the expr~ pow($f2, -0.6) calculation that applies amplitude scaling based on the total number of active partials received through the r n-partials system. This scaling factor implements psychoacoustic research that shows how human loudness perception requires amplitude reduction as the number of harmonic components increases, preventing the perception of increased loudness as synthesis complexity grows. The power law exponent of -0.6 provides a compromise between mathematical precision and musical utility, ensuring that 20-partial synthesis produces manageable amplitude levels while maintaining sufficient dynamic range for musical expression.\n\n\n\n\n\nflowchart LR\n    A[Partial Selection] --&gt; B[Dynamic Configuration]\n    B --&gt; C[Scaled Audio Output]\n\n\n\n\n\n\n\n\n4.3.8.4 Processing Chain Details\nThe dynamic additive synthesis design maintains musical coherence through coordinated processing stages:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nUser Selection\nPartial count validation and distribution\nSystem reconfiguration triggers\n\n\n2\nPartial Count\nIterative parameter calculation\nHarmonic initialization data\n\n\n3\nParameter Messages\nDynamic clone instance management\nActive harmonic oscillators\n\n\n4\nInstance Configuration\nIndividual harmonic synthesis with scaling\nAmplitude-compensated audio\n\n\n5\nScaled Audio Streams\nSignal collection and mixing\nBalanced harmonic spectrum\n\n\n\n\n4.3.8.4.1 Scaling Factor Analysis\nThe psychoacoustic amplitude scaling prevents clipping while maintaining perceptual balance:\n\n\n\n\n\n\n\n\n\nActive Partials\nScaling Factor\nIndividual Amplitude\nPerceptual Effect\n\n\n\n\n1\n1.000\n100%\nFull single-tone amplitude\n\n\n5\n0.315\n31.5%\nBalanced complex tone\n\n\n10\n0.200\n20.0%\nRich harmonic spectrum\n\n\n15\n0.151\n15.1%\nDense harmonic texture\n\n\n20\n0.122\n12.2%\nMaximum complexity without clipping\n\n\n\n\n\n\n4.3.8.5 Clone Abstraction Architecture\nDynamic Scalability: Easy modification of maximum partial count through hradio range adjustment.\nEfficient Resource Management: Inactive instances consume minimal CPU resources.\nParameter Synchronization: Automatic distribution of configuration changes to all active instances.\nAmplitude Safety: Built-in scaling prevents output clipping across all partial count settings.\n\n\n4.3.8.6 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Dynamic Synthesis\n\n\n\n\nclone harmonic\nDynamic instance manager\nCreates variable number of oscillator instances\n\n\nuntil\nParameter loop generator\nExecutes calculation sequence for each partial\n\n\nmod\nFrequency calculation\nGenerates harmonic number sequence\n\n\nexpr~ pow($f2, -0.6)\nAmplitude scaling\nPrevents clipping using psychoacoustic scaling\n\n\ns n-partials / r n-partials\nPartial count distribution\nDistributes active partial count to all instances\n\n\n\n\n\n4.3.8.7 Creative Applications\n\nReal-time Timbral Morphing: Automate the partial count using LFOs or envelopes to create evolving timbres that transition between simple and complex harmonic structures\nInteractive Spectral Control: Map external controllers or sensors to the partial count parameter, enabling performers to sculpt harmonic complexity through gesture or movement\nSpectral Analysis Tools: Use the system to reconstruct and study natural instrument timbres by adjusting partial counts to match analyzed harmonic content\n\n\n\n\n4.3.9 Spectral Melody\nThis patch implements an additive synthesis system that demonstrates how harmonic partial generation can be controlled through algorithmic distribution and temporal sequencing. By utilizing Pd’s clone object to create eight instances of a partial abstraction, the system generates complex harmonic spectra where each partial operates with independent amplitude envelopes and frequency relationships. The patch emphasizes real-time control over both spectral content and temporal evolution, creating a platform for exploring the relationship between harmonic structure and melodic perception.\nThe implementation showcases advanced Pd programming techniques including dynamic object creation, parameter distribution through send/receive networks, and sophisticated envelope generation that enables complex spectral animation. By combining systematic harmonic relationships with stochastic envelope timing, the patch creates evolving textures that bridge the gap between traditional additive synthesis and contemporary algorithmic composition.\n\n\n\nSpectral melody generator using clone object, mathematical amplitude scaling, and randomized envelope timing for complex harmonic evolution.\n\n\n\n4.3.9.1 Patch Overview\nThe patch implements a comprehensive harmonic generation pipeline consisting of five primary processing stages:\n\nParameter Management: Global frequency and timing control with user interface\nAlgorithmic Partial Distribution: Mathematical calculation of harmonic relationships and amplitude scaling\nDynamic Synthesis Engine: Eight-instance clone system for parallel partial generation\nStochastic Envelope Control: Randomized attack and decay timing for each partial\nAudio Collection and Output: Signal aggregation and final audio delivery\n\nThe patch demonstrates how mathematical relationships can be embedded within musical structures, where harmonic series principles guide frequency distribution while algorithmic processes control temporal evolution and spectral dynamics.\n\n\n4.3.9.2 Critical Algorithmic Components\nClone Object Architecture: The clone partial 8 object creates eight independent instances of the partial abstraction, each receiving unique initialization parameters. This approach enables parallel processing while maintaining individual control over each harmonic component.\nMathematical Amplitude Distribution: The expression (8 - $f1)/8 implements inverse amplitude scaling where lower harmonics receive higher amplitudes, following natural harmonic decay patterns found in acoustic instruments.\nParameter Pack System: The pack f f f object bundles harmonic number, frequency multiplier, and amplitude coefficient into a unified message that initializes each partial instance with its specific characteristics.\n\n\n4.3.9.3 Data Flow\n\n\n\n\n\nflowchart TD\n    A[User Controls] --&gt; B[Parameter Distribution]\n    B --&gt; C[Algorithmic Calculation]\n    C --&gt; D[Clone Instance Generation]\n    D --&gt; E[Individual Partial Processing]\n    E --&gt; F[Stochastic Envelope Control]\n    F --&gt; G[Audio Collection]\n    G --&gt; H[Mixed Output]\n    \n    style A fill:#e1f5fe\n    style H fill:#f3e5f5\n    style C fill:#fff3e0\n    style F fill:#fff3e0\n\n\n\n\n\n\nThe spectral melody patch implement an algorithmic synthesis process that transforms simple user parameters into complex harmonic spectra through systematic mathematical calculations and dynamic object instantiation. This implementation demonstrates how Pd’s cloning capabilities can be combined with algorithmic parameter generation to create scalable additive synthesis systems that maintain both mathematical precision and timbral expressivity.\nThe parameter initialization and distribution stage establishes the foundation for the entire synthesis process through coordinated management of fundamental frequency and timing parameters. When users adjust the frequency slider, the new value propagates through the s root-freq send object to all active partial instances, ensuring that harmonic relationships remain mathematically correct as the fundamental frequency changes. The duration control system operates in parallel, with the time slider feeding the s total-time distribution network that coordinates envelope timing across all eight partials. The bang trigger initiates the algorithmic calculation sequence that generates the specific parameters needed for each partial instance, creating a synchronized parameter update system that maintains harmonic coherence while enabling real-time musical control. This distribution architecture ensures that all synthesis components receive updated parameters simultaneously, preventing temporal misalignment that could compromise the spectral integrity of the additive synthesis process.\nThe algorithmic parameter generation stage represents the mathematical core of the system, where simple counting operations become sophisticated harmonic calculations through coordinated processing chains. The until object creates an iteration loop that executes eight times, with each iteration generating the specific parameters needed for one partial instance. During each iteration, the counter value undergoes multiple simultaneous calculations: direct passage as the harmonic number, incrementation by one to create the frequency multiplier, and processing through the amplitude calculation expression (8 - $f1)/8 to generate amplitude coefficients that follow natural harmonic decay patterns. This mathematical approach ensures that the first partial operates as the fundamental with maximum amplitude, while subsequent partials represent increasingly higher harmonics with proportionally reduced amplitudes. The pack f f f object collects these three calculated values into a unified parameter message that contains all information necessary for initializing each partial instance with its unique harmonic characteristics.\nThe dynamic synthesis instantiation stage demonstrates Pd’s advanced object creation capabilities through the sophisticated implementation of the clone partial 8 object. Each parameter pack message generated by the algorithmic calculation stage triggers the creation and initialization of a partial instance with specific harmonic characteristics. The clone object manages eight independent instances of the partial abstraction, distributing the parameter messages to initialize each instance with its unique harmonic number, frequency multiplier, and amplitude coefficient. This distribution process creates a parallel processing architecture where eight independent synthesis units operate simultaneously, each contributing a specific harmonic component to the overall spectral composition. The modular design enables easy scaling to different numbers of partials by simply modifying the clone argument, demonstrating the flexibility inherent in this approach to additive synthesis system design.\nThe individual partial processing stage occurs within each partial abstraction instance, where the distributed parameters control sophisticated oscillator and envelope systems that generate the actual harmonic content. Each partial receives the global root frequency through the receive system and multiplies it by its assigned frequency multiplier to generate its specific harmonic frequency. The amplitude coefficient controls the maximum level for that partial’s envelope system, while the total time parameter influences the temporal characteristics of the amplitude envelope. Within each partial, stochastic processes introduce controlled randomization into the envelope timing, creating complex temporal relationships between different harmonics that evolve organically over time. This combination of deterministic harmonic relationships with stochastic temporal control enables the system to generate spectra that maintain harmonic coherence while exhibiting natural variation and evolution that prevents static or mechanical sonic characteristics.\nThe signal collection and output coordination stage aggregates the eight independent audio streams into a unified spectral composition through Pd’s efficient audio signal routing system. Each partial instance employs a throw~ audio object that sends its processed audio signal to the global collection point implemented by the catch~ audio object in the main patch. This signal routing approach eliminates the need for complex cable connections while maintaining efficient audio summing that preserves the amplitude relationships established during the parameter calculation stage. The collected audio signal represents the complete harmonic spectrum generated by all eight partials, with the spectral content and temporal evolution determined by the interaction between the algorithmic parameter calculations and the stochastic processes operating within each individual partial instance.\n\n\n\n\n\nflowchart LR\n    A[Parameter Input] --&gt; B[Algorithmic Processing]\n    B --&gt; C[Instance Generation]\n    C --&gt; D[Signal Collection]\n\n\n\n\n\n\n\n\n4.3.9.4 Processing Chain Details\nThe spectral melody system maintains mathematical precision and timbral coherence through several interconnected processing stages:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nUser Controls\nParameter distribution to global sends\nSynchronized control signals\n\n\n2\nBang Trigger\nIterative calculation loop (8 cycles)\nParameter packs for each partial\n\n\n3\nParameter Packs\nClone instance initialization\nEight active partial synthesizers\n\n\n4\nDistributed Parameters\nIndividual partial processing\nIndependent audio streams\n\n\n5\nMultiple Audio Streams\nSignal collection and mixing\nCombined harmonic spectrum\n\n\n\n\n\n4.3.9.5 Harmonic Distribution Calculation\nThe mathematical relationships governing partial generation follow systematic patterns:\n\n\n\n\n\n\n\n\n\nPartial Number\nFrequency Multiplier\nAmplitude Coefficient\nHarmonic Function\n\n\n\n\n1\n1\n1.000\nFundamental\n\n\n2\n2\n0.875\nSecond harmonic\n\n\n3\n3\n0.750\nThird harmonic\n\n\n4\n4\n0.625\nFourth harmonic\n\n\n5\n5\n0.500\nFifth harmonic\n\n\n6\n6\n0.375\nSixth harmonic\n\n\n7\n7\n0.250\nSeventh harmonic\n\n\n8\n8\n0.125\nEighth harmonic\n\n\n\n\n\n4.3.9.6 Clone Abstraction Architecture\nThe clone partial 8 implementation provides:\n\nScalable Instance Management: Easy modification of partial count through argument change\nParameter Distribution: Automated routing of initialization data to instances\nIndependent Processing: Each instance operates with complete autonomy\nEfficient Signal Routing: Built-in audio aggregation through throw/catch system\n\n\n\n4.3.9.7 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Spectral Synthesis\n\n\n\n\nclone partial 8\nDynamic instance creation\nGenerates eight independent partial synthesizers\n\n\nuntil\nIteration control\nExecutes parameter calculation loop eight times\n\n\npack f f f\nParameter bundling\nCombines harmonic number, frequency multiplier, and amplitude\n\n\nexpr (8 - $f1)/8\nAmplitude calculation\nCreates inverse amplitude scaling for higher harmonics\n\n\ns root-freq / r root-freq\nGlobal frequency distribution\nDistributes fundamental frequency to all partials\n\n\ns total-time / r total-time\nTiming coordination\nCoordinates envelope durations across all instances\n\n\ncatch~ audio\nSignal collection\nAggregates audio output from all partial instances\n\n\n\n\n\n4.3.9.8 Creative Applications\n\nAlgorithmic Harmonic Composition: Use external control data (MIDI, OSC, sensors) to modulate the root frequency and timing parameters, creating evolving harmonic progressions that maintain mathematical relationships\nSpectral Morphing Sequences: Program sequences of different fundamental frequencies and timing values to create smooth transitions between distinct harmonic spectra, exploring timbral evolution over time\nInteractive Performance: Map physical controllers to the frequency and timing parameters, enabling real-time spectral sculpting during live performance where harmonic content responds to performer gestures\nStochastic Harmonic Exploration: Modify the amplitude calculation algorithm or introduce randomization into the frequency multipliers to explore non-traditional harmonic relationships while maintaining the systematic approach\nEnvironmental Sound Synthesis: Use the harmonic framework as a foundation for recreating natural sound textures by modifying the amplitude distribution to match spectral analysis of environmental recordings\nCollaborative Network Music: Network multiple instances of the patch where different performers control different aspects of the spectral generation, creating collective harmonic compositions\nTemporal Spectral Counterpoint: Create multiple simultaneous instances with different timing parameters to generate polyphonic textures where independent harmonic voices evolve at different rates",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sound Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#amplitude-ring-modulation",
    "href": "chapters/synthesis.html#amplitude-ring-modulation",
    "title": "4  Sound Synthesis",
    "section": "4.4 Amplitude & Ring Modulation",
    "text": "4.4 Amplitude & Ring Modulation\nAmplitude Modulation (AM) and Ring Modulation (RM) are two techniques that manipulate the amplitude of a signal using another signal. While they share similarities, they produce distinct results and are used in different contexts. In this section, we will explore the differences between AM and RM, their applications, and how they can be implemented in sound synthesis.\n\n4.4.1 Amplitude Modulation (AM)\nWe can modulate the amplitude of any signal—referred to as the carrier—by multiplying it with an oscillating signal, called the modulator. The modulator is typically another oscillator, and its frequency determines the modulation frequency.\n\n\n\nClassic amplitude modulation with sine wave carrier and unipolar modulator, creating tremolo-like amplitude variation\n\n\nIn the example provided, both the carrier and modulator are sine wave oscillators. This is what we call “classic” AM, where the modulator signal includes a DC offset, making it unipolar, ranging only from 0 to 1. This unipolarity ensures that the carrier’s amplitude is scaled without ever becoming negative.\n\n\n4.4.2 Ring Modulation (RM)\nRM is a particular form of AM where both the carrier and modulator signals are bipolar, meaning they oscillate between -1 and 1 without any DC offset. In this configuration, there’s no functional distinction between carrier and modulator—both behave symmetrically.\n\n\n\nRing modulation configuration with bipolar signals creating sidebands\n\n\nNonetheless, in practical terms, the carrier is usually an audio signal such as a musical instrument, while the modulator remains a simple oscillator. A key technical detail is that when the modulator signal is negative, it inverts the polarity of the carrier signal—producing a unique and often metallic timbre.\n\n\n4.4.3 DC Offset\nAM schemes can involve various DC offset settings—not just limited to AM or RM. In our example, preset configurations for AM and RM are available, but one can also manually adjust peak levels and DC offset using sliders. Observe how, in the frequency spectrum of classic AM, we see two sidebands—above and below the carrier frequency—each at half the amplitude of the original carrier. These sidebands are spaced apart by the modulation frequency.\n\n\n\nDC offset control interface showing continuous transition between AM and RM modes with real-time spectral analysis of carrier and sideband frequencies.\n\n\nIn contrast, RM removes the original carrier frequency entirely from the spectrum, leaving only the sidebands, which typically carry more energy than in AM.\nBy adjusting amplitude and DC offset with sliders, we can morph continuously between AM and RM modes, gaining nuanced control over the presence of the original frequency component and the energy distribution in the sidebands.\n\n\n4.4.4 Audio Samples & Modulation\nIn this example, an audio sample replaces the oscillator as the carrier signal. This demonstrates how AM can function as an audio effect processor rather than just a synthesis technique. In fact, much of what we traditionally associate with synthesis techniques is often more accurately described as audio processing.\nConversely, many effects processors—such as filters—are integral to sound synthesis. The boundary between synthesis and processing is thus fluid and contextual.\n\n\n\nRing modulation applied to audio samples with carrier selection and modulation controls\n\n\nTry both the classic AM and RM examples. In both cases, sidebands are generated for each sine wave component within the carrier signal. AM retains the carrier’s original sine components, which coexist and interact with the generated sidebands. For this reason, AM is commonly used for tremolo effects (which we’ll examine later). On the other hand, RM removes the original sine components entirely, yielding a more sonically distinctive result.\n\n\n4.4.5 Other Waveforms\nUsing more complex waveforms for the modulator signal leads to the creation of additional partials within any AM patch—including RM. Sine waves are typically favored as modulators since they offer clean and controlled results, especially when applying AM as an audio effect.\n\n\n\nComplex waveform modulation showing carrier and modulator waveform selection with spectral complexity progression from sine waves to sawtooth waves\n\n\nHowever, in synthesis contexts, more intricate and harmonically rich methods—such as frequency and phase modulation—offer more efficient and versatile approaches for generating complex timbres. We’ll explore these in the following sections.\n\n\n4.4.6 Tremolo\nTremolo is essentially AM using a low-frequency modulator (or a Low Frequency Oscilator - LFO). The key addition is a depth parameter, ranging from 0 to 1, which determines the modulation intensity. At 0, no modulation occurs (dry signal), while a depth of 1 results in full tremolo, where the carrier’s amplitude is modulated across its full range.\n\n\n\nTremolo effect with low-frequency modulation showing depth control from 0 (no effect) to 1 (full amplitude modulation)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sound Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#frequency-modulation-fm",
    "href": "chapters/synthesis.html#frequency-modulation-fm",
    "title": "4  Sound Synthesis",
    "section": "4.5 Frequency Modulation (FM)",
    "text": "4.5 Frequency Modulation (FM)\nIn general terms, to modulate a signal means to alter it in some way. In the context of this course, however, we refer specifically to using a modulating signal to control a parameter—such as amplitude, as previously discussed. We now turn to the basic structure of frequency modulation (FM), where an oscillator acts as the modulator.\nThe signal being modulated is called the carrier; in the case of FM, it is also referred to as the carrier frequency. In contrast, we have the modulating frequency, which corresponds to the frequency of the modulating oscillator. The depth of frequency variation is determined by the amplitude of the modulator and is commonly referred to as the modulation index. The modulation process itself is straightforward: we add the modulating signal to the frequency input of the carrier oscillator.\n\n\n\nFM synthesis with carrier oscillator, modulator input, and frequency deviation control, showing the fundamental structure of frequency modulation.\n\n\nBy default, we have a carrier frequency of 400 Hz, a low modulation frequency of 1 Hz, and a modulation index of 100. This means the modulating signal oscillates between -100 and +100 Hz, causing the carrier frequency to vary between 300 and 500 Hz. Note that when the modulation frequency is low, the result is a vibrato-like effect.\n\n4.5.1 FM Simple\nWe apply the same structure as before. Besides a vibrato example, this section includes fully developed FM examples. As with amplitude modulation, FM produces sidebands spaced by intervals equal to the modulation frequency. However, FM can generate many more sidebands, potentially resulting in a much richer spectrum.\n\n\n\nFM synthesis interface with carrier frequency, modulation frequency, and index controls, creating complex harmonic spectra through frequency modulation.\n\n\nThe higher the modulation index, the greater the number of resulting partials—enabling the creation of dense and complex waveforms. When the carrier and modulator frequencies share a simple harmonic ratio, the resulting waveform is harmonic. Otherwise, it tends to be inharmonic.\n\n\n4.5.2 Other Waveforms\nIn this patch, we experiment with different oscillator combinations. Waveforms are arranged according to spectral complexity—from simple sine waves to rich sawtooth waves (the only waveform here containing both even and odd harmonics). The more complex the waveform used, the more intricate (and unpredectible) the FM result becomes.\n\n\n\nFM synthesis interface with waveform selection for carrier and modulator arranged by harmonic complexity\n\n\nOn the modulator side, the waveforms are idealized and band-unlimited—perfect in theory. However, the frequency-modulated oscillator has a limited bandwidth, which imposes practical constraints on the resulting spectrum.\n\n\n4.5.3 Exponential Frequency\nWe can also use exponential pitch values (such as MIDI note numbers) instead of linear frequency input. The key difference here is that frequency deviation—the modulation index—is now expressed in semitones, not Hertz. This shift affects the entire modulation behavior.\n\n\n\nFM synthesis with exponential frequency control using MIDI note numbers, where modulation index is expressed in semitones rather than Hz, creating asymmetrical waveform characteristics.\n\n\nAs a result, the output waveform becomes asymmetrical and significantly different in character. The main change in the patch is the use of [mtof~] to convert MIDI pitch to frequency in Hz.\n\n\n4.5.4 Ratio\nIt is common practice to define the modulating frequency as a ratio of the carrier frequency. This allows us to work with a single frequency input while maintaining a consistent sonic character across different pitches.\n\n\n\nFrequency modulation with ratio-based modulator control, maintaining consistent harmonic relationships between carrier and modulator frequencies across different pitch ranges.\n\n\nThis approach preserves the relationship between the carrier and modulator frequencies, which is crucial since this ratio determines how the additional spectral components—partials—are distributed. Harmonic ratios (such as 0.5 or 2) yield harmonic results, while non-integer ratios lead to inharmonic spectra.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sound Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#subtractive-synthesis",
    "href": "chapters/synthesis.html#subtractive-synthesis",
    "title": "4  Sound Synthesis",
    "section": "4.6 Subtractive Synthesis",
    "text": "4.6 Subtractive Synthesis\nSubtractive synthesis is frequently—and mistakenly—viewed as synonymous with analog synthesis. While there are many methods of sound synthesis, subtractive synthesis remains the most prevalent in commercial analog synthesizers. Often presented through user interfaces brimming with knobs and switches, it can appear overwhelming to newcomers. However, its intuitive one-to-one mapping between physical controls and sound parameters makes it especially suitable for educational contexts. Furthermore, it provides a useful entry point for exploring core acoustic principles and models of sound production.\n\n4.6.1 Conceptual Model: Source and Modifier\nAt the heart of subtractive synthesis lies a conceptual framework: the separation of sound production into three functional components: 1. source, 2. modifier, and 3. controller.\nThis framework mirrors the structure of many acoustic instruments, most clearly in wind instruments. Take, for example, the clarinet: its vibrating reed acts as the source, while the resonant body functions as a modifier. The reed alone produces a sharp, buzzing tone; the body of the clarinet, essentially a cylindrical tube, shapes this tone through its resonant properties, which are determined by physical dimensions like length and bore diameter. Together, they produce the rich timbral identity of the clarinet.\nThis model, while illustrative, is not universally applicable. Consider the guitar: its sound emerges from the interplay between the plucked string (source) and the resonant body (modifier). Yet unlike the clarinet, the source and modifier in the guitar are tightly coupled and difficult to separate experimentally. The string cannot be plucked in isolation, nor can the body’s resonant characteristics be accurately studied without the tensioned string.\nDespite such limitations, the core idea—modifying a source signal to shape its timbre—is both intuitive and powerful. It underpins most approaches to sound synthesis and provides a compelling framework for creative exploration. In a subtractive synthesis context, this source-modifier model is streamlined: the sound source is designed to contain rich harmonic content, and the modifier (typically a filter) removes selected frequency components to shape the sound. The process is thus subtractive in nature—the unwanted frequencies are “subtracted” from the original signal.\n\n\n4.6.2 Sound Sources\nThe primary sound sources in analog subtractive synthesizers are waveform generators and noise sources. As mentioned, common waveforms include sawtooth, square, pulse, sine, and triangle waves—each chosen for their mathematical simplicity and electronic feasibility. These shapes carry predictable harmonic content: for instance, sawtooth waves contain both even and odd harmonics, while square waves contain only odd harmonics.\nRandom waveforms, typically perceived as noise, include all frequencies and are often used to generate percussive or atmospheric textures. The devices that produce these waveforms—oscillators—are descendants of function generators. In subtractive synthesis, these oscillators produce continuous signals, with frequency typically controlled by voltage. These signals serve as raw material for further shaping via modifiers.\n\n\n4.6.3 Filters\nFilters play a central role in subtractive synthesis. At their core, filters are amplifiers whose gain varies with frequency. In most synthesizer contexts, filters are designed such that their maximum gain is one; thus, it’s more accurate to say that they attenuate different frequency components to varying degrees. When a filter’s parameters can be controlled via voltage, it becomes a Voltage-Controlled Filter (VCF). VCFs are essential modifiers because they allow dynamic control over a signal’s spectral content, significantly influencing the resulting timbre.\n\n\n4.6.4 Classification of Filters\nFilters are typically categorized by their frequency response—how they affect different parts of the frequency spectrum. There are four main types:\n\nLow-pass filters (LPF) allow frequencies below a cutoff point to pass and attenuate higher frequencies. The cutoff frequency is defined as the point at which the signal is attenuated by 3 dB (the “half-power” point). The slope of attenuation is determined by the filter’s design: a one-pole filter attenuates at 6 dB per octave, a two-pole at 12 dB, and so on. Filters with steeper slopes, such as four-pole filters (24 dB/octave), tend to sound more “synthetic” and dramatically alter timbre.\nHigh-pass filters (HPF) do the reverse, removing low frequencies and allowing higher frequencies to pass. This has the effect of “thinning” the sound by removing its fundamental or lower harmonics. HPFs are often used in combination with LPFs to further sculpt the timbre, especially when the goal is to mimic instruments with weak or missing fundamentals.\nBand-pass filters (BPF) permit only a specified range of frequencies to pass. This is useful for emphasizing particular harmonic regions or simulating acoustic resonances. A BPF can be implemented by combining an HPF and an LPF in series. The central frequency of the pass-band is its resonant peak, and the width of the pass-band is termed the bandwidth.\nNotch filters, or band-reject filters, remove a narrow band of frequencies, allowing all others to pass. These are useful for eliminating unwanted harmonics or resonant peaks, especially in applications such as feedback suppression or timbral correction.\n\nEach filter type can be modulated dynamically, allowing timbral variation over time. For instance, sweeping the cutoff frequency of an LPF from high to low removes successive harmonics, gradually “closing” the filter and making the sound darker and more subdued.\nThe subtractive model—source rich in harmonics, modified via filtering—forms the foundational paradigm for analog synthesis. While conceptually straightforward, it allows for a wide array of sonic possibilities. By carefully selecting waveforms, modulating filter parameters, and controlling amplitude envelopes, one can sculpt intricate, evolving timbres.\nThis approach, despite its age, remains vital in modern synthesis. Its clarity, intuitiveness, and direct mapping between sound components and control structures make it an enduring framework for both education and creative practice. Whether one is emulating acoustic instruments or crafting entirely synthetic textures, subtractive synthesis offers a practical and powerful toolset for sonic exploration.\n\n\n4.6.5 Filters in Pd\nAs mentioned, a filter is any process that modifies an incoming sound. From an acoustic perspective, filters are omnipresent: the reverberant characteristics of a room act as filters, as do the resonant bodies of musical instruments. Even the human vocal tract can be considered a natural filter. In our context, however, we define filters more narrowly: as signal processing modules—such as equalizers—that selectively amplify or attenuate specific frequency components of a sound.\nThe most common representation of how a filter affects sound is its frequency response. This is typically displayed as a graph where the horizontal axis represents frequency (in kilohertz), and the vertical axis shows amplitude (in decibels, dB). Positive values (above 0 dB) indicate frequencies that are being boosted, while negative values (below 0 dB) show those that are attenuated. In addition to frequency response, filters also affect the phase of the signal, which we refer to as phase response—representing the phase shift introduced as a function of frequency.\n\n\n4.6.6 Low-Pass Filtering\nThe object lop~ in Pd (short for “low-pass”) implements a basic low-pass filter. This filter allows frequencies below a certain threshold, called the cutoff or roll-off frequency, to pass through while progressively attenuating higher frequencies. This transition is not abrupt: the filtering effect begins at 0 Hz and gradually increases, resulting in a -3 dB drop at the cutoff point. Beyond this, the attenuation rate is approximately 6 dB per octave.\n\n\n\nLow-pass filter with frequency cutoff control, attenuating higher frequencies at 6dB per octave slope\n\n\nThis filter is useful not only in audio synthesis but also in signal smoothing applications—such as envelope followers or data processing from physical sensors. At high cutoff frequencies (above ~5 kHz), the filter’s response deviates slightly from this ideal behavior.\n\n\n4.6.7 High-Pass Filtering\nA high-pass filter, such as hip~ in Pd, performs the inverse operation of a low-pass: it permits frequencies above a defined cutoff point and attenuates those below it. Interestingly, a high-pass effect can also be derived by subtracting the output of a low-pass filter from the original signal. While hip~ is often employed to remove DC offsets, it too has a 6 dB per octave slope and exhibits some gain anomalies at higher cutoff values.\n\n\n\nHigh-pass filter with cutoff control, removing frequencies below the threshold while preserving higher frequency content for signal clarity\n\n\n\n\n4.6.8 Resonant Filters\nA resonant filter emphasizes a specific frequency band over others. A typical example is the band-pass filter, which allows only a defined range of frequencies to pass while attenuating those outside the band. The center frequency of this band is commonly denoted as ( f_0 ), with ( f_1 ) and ( f_2 ) marking the -3 dB points below and above it, respectively.\nThe bandwidth of the filter is defined as the difference between these two frequencies:\n\\[ \\text{Bandwidth} = f_2 - f_1 \\]\nThe musical interval between ( f_1 ) and ( f_2 ) can also be used to conceptualize the resonance more musically, especially when represented on a logarithmic scale like MIDI pitch.\n\n\n\nBand-pass filter frequency response showing center frequency f₀ with -3dB bandwidth points f₁ and f₂, illustrating the relationship between resonant peak and filter selectivity.\n\n\n\n\n4.6.9 The Q Factor\nThe sharpness or “quality” of a filter’s resonance is quantified by the Q factor, which is calculated as:\n\\[ Q = \\frac{f_0}{f_2 - f_1} \\]\nA higher Q value indicates a narrower and more pronounced resonance, while a lower Q results in a broader and more subtle response. In essence, Q and bandwidth are inverse measures of the same characteristic.\n\n\n\nFilter Q factor visualization showing the relationship between resonance sharpness and bandwidth, with higher Q values\n\n\n\n\n4.6.10 Band-Pass Filters\nThe bp~ object in Pd provides a straightforward implementation of a band-pass filter, accepting center frequency and Q as inputs. It is computationally efficient and practical, though it tends to attenuate frequencies above the center more than those below, thus producing an asymmetrical response. With a Q of 1, bp~ approximates the behavior of a lop~ filter. As with other vanilla filters, its frequency response changes noticeably at higher frequencies.\n\n\n\nBand-pass filter with frequency and Q controls, creating resonant peak emphasis while attenuating frequencies outside the passband.\n\n\n\n\n4.6.11 Voltage-Controlled Filters\nThe vcf~ object (short for “voltage-controlled filter”) is another band-pass filter in the vanilla distribution, notable for its ability to accept audio-rate signals to modulate the center frequency—mimicking the behavior of analog synthesizers. While the Q parameter is controlled by control-rate data, the central frequency can be dynamically shaped by audio input.\n\n\n\nVCF implementation with audio-rate frequency modulation capability, providing both standard and complementary phase outputs for enhanced filtering flexibility.\n\n\nThis object has two outputs: the right outlet approximates a bp~ filter’s frequency response, whereas the left outlet provides a complementary phase response. At low Q values, both outputs show a characteristically asymmetrical frequency curve, and deviations become more pronounced at higher frequencies (above ~7 kHz).\n\n\n4.6.12 Emulating the Moog Filter\nThe bob~ object is a recent addition to Pd’s vanilla library, designed to model the iconic Moog filter created by Robert Moog. It allows both the cutoff frequency and Q to be modulated at audio rate, making it one of the most expressive and flexible filters in the system. Though officially described as a low-pass resonant filter, its architecture lends itself to a wide variety of applications. Users are encouraged to consult its help patch for in-depth understanding and examples.\n\n\n\nMoog-style resonant low-pass filter with audio-rate cutoff and Q modulation, providing the characteristic warm analog filtering sound\n\n\n\n\n4.6.13 Biquad Filters\nThe biquad~ object is a versatile, coefficient-driven filter capable of implementing multiple filter types based on its settings. “Biquad” stands for “biquadratic,” reflecting its mathematical foundation in second-order polynomials. However, configuring biquad~ manually is a non-trivial task for most musicians or artists.\nTo simplify this process, the bicoeff object from the ELSE library allows users to generate coefficients for nine different filter types, based on Robert Bristow-Johnson’s “Cookbook formulae for audio EQ biquad filter coefficients”(Bristow-Johnson 2024). This GUI-based object visualizes the amplitude response in real time. The X-axis is logarithmic (typically in MIDI pitch), and the Y-axis represents gain in dB. Interactive controls allow users to modify frequency, bandwidth, and gain directly by clicking and dragging on the graph.\n\n\n\nBiquad filter interface with bicoeff object providing real-time frequency response visualization and interactive control\n\n\nTogether, these filtering tools illustrate the conceptual and practical range of subtractive and resonant filtering in sound synthesis. Whether you are shaping a simple tone, cleaning up sensor data, or building complex timbral transformations, understanding how filters operate is essential. They not only alter the spectral content of signals but also engage with broader perceptual dimensions—allowing composers, performers, and designers to sculpt sound with nuance and precision.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sound Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#references",
    "href": "chapters/synthesis.html#references",
    "title": "4  Sound Synthesis",
    "section": "References",
    "text": "References\n\n\n\n\nBristow-Johnson, Robert. 2024. “Cookbook Formulae for Audio Equalizer Biquad Filter Coefficients.” https://webaudio.github.io/Audio-EQ-Cookbook/audio-eq-cookbook.html.\n\n\nHelmholtz, Hermann. 1954. On the Sensations of Tone as a Physiological Basis for the Theory of Music. 2nd ed. New York: Dover Publications.\n\n\nLévi-Strauss, Claude. 1964. Le Cru Et Le Cuit. Vol. 1. Mythologiques. Paris: Plon.\n\n\nPuckette, Miller. 2007. The Theory and Technique of Electronic Music. World Scientific Publishing. https://doi.org/10.1142/6277.\n\n\nSterne, Jonathan, and Tara Rodgers. 2011. “Poetics of Signal Processing.” Differences 22 (2-3): 31–53. https://doi.org/10.1215/10407391-1428834.\n\n\nThéberge, Paul. 1997. Any Sound You Can Imagine: Making Music/Consuming Technology. Hanover & London: Wesleyan University Press.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sound Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html",
    "href": "chapters/sonification.html",
    "title": "5  Sonification",
    "section": "",
    "text": "5.1 Introduction\nIn this chapter, we will explore the concept of sonification, its applications, and how it can be implemented using Pd. We will also discuss the importance of understanding data types and classifications in the context of sonification.\nImagine hearing the changes in global temperature over the past thousand years. What does a brainwave sound like? How can sound be used to enhance a pilot’s performance in the cockpit? These intriguing questions, among many others, fall within the realm of auditory display and sonification.\nResearchers in Auditory Display explore how the human auditory system can serve as a primary interface channel for communicating and conveying information. The goal of auditory display is to foster a deeper understanding or appreciation of the patterns and structures embedded in data beyond what is visible on the screen. Auditory display encompasses all aspects of human-computer interaction systems, including the hardware setup (speakers or headphones), modes of interaction with the display system, and any technical solutions for data collection, processing, and computation needed to generate sound in response to data.\nIn contrast, sonification is a core technique within auditory display: the process of rendering sound from data and interactions. Unlike voice interfaces or artistic soundscapes, auditory displays have gained increasing attention in recent years and are becoming a standard method alongside visualization for presenting data across diverse contexts. The international research effort to understand every aspect of auditory display began with the founding of the International Community for Auditory Display (ICAD) in 1992. It is fascinating to observe how sonification and auditory display techniques have evolved in the relatively short time since their formal definition, with development accelerating steadily since 2011.\nAuditory display and sonification are now employed across a wide range of fields. Applications include chaos theory, biomedicine, interfaces for visually impaired users, data mining, seismology, desktop and mobile computing interaction, among many others. Equally diverse is the set of research disciplines required for successful sonification: physics, acoustics, psychoacoustics, perceptual research, sound engineering, and computer science form the core technical foundations. However, psychology, musicology, cognitive science, linguistics, pedagogy, social sciences, and philosophy are also essential for a comprehensive, multifaceted understanding of the description, technical implementation, usage, training, comprehension, acceptance, evaluation, and ergonomics of auditory displays and sonification in particular.\nIt is clear that in such an interdisciplinary field, a narrow focus on any single discipline risks “seeing the trees but missing the forest.” As with all interdisciplinary research efforts, auditory display and sonification face significant challenges, ranging from differing theoretical orientations across fields to even the very vocabulary used to describe our work.\nInterdisciplinary dialogue is crucial to advancing auditory display and sonification. However, the field must overcome the challenge of developing and employing a shared language that integrates many divergent disciplinary ways of speaking, thinking, and approaching problems. On the other hand, this very challenge often unlocks great creative potential and new ideas, as these varied perspectives can spark innovation and fresh insights.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#introduction",
    "href": "chapters/sonification.html#introduction",
    "title": "5  Sonification",
    "section": "",
    "text": "Multidisciplinary landscape of sonification research encompassing technical, perceptual, and cultural domains essential for effective auditory display design and implementation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#data-types",
    "href": "chapters/sonification.html#data-types",
    "title": "5  Sonification",
    "section": "5.2 Data Types",
    "text": "5.2 Data Types\nIn the realm of digital data, understanding the nature and classification of data types is essential for their effective processing, storage, and analysis. The table above presents a detailed taxonomy of data types broadly categorized into Static and Stream / Realtime data, further subdivided into various subtypes. This section will unpack these classifications, elaborating on their characteristics, common formats, and sources.\n\n5.2.1 Static Data\nStatic data refers to data that is stored and remains unchanged until explicitly modified. It is typically collected, stored, and then analyzed or processed offline or asynchronously. This type of data is fundamental in many computational tasks, including machine learning, data mining, and archival storage.\n\n5.2.1.1 Structured Data\nStructured data is organized in a predefined manner, typically conforming to a schema or model that allows easy querying and manipulation.\nExamples:\n\nDatasets (CSV, XLS): Tabular data with rows and columns, where each column has a defined datatype (e.g., numeric, string, datetime).\nFields: The basic elements of structured data that have specific data types, such as integers, floating-point numbers, text strings, or timestamps.\nClasses: Labels used in supervised learning, often binary (two classes) or multiclass (multiple categories).\nImages: Digital images can be structured if stored with metadata or standardized file formats. Examples include compressed formats like JPG and uncompressed formats like BMP.\nMIDI Files: Musical Instrument Digital Interface files encode structured note and control information.\nAudio: Raw waveforms or extracted descriptors (e.g., Mel-frequency cepstral coefficients - MFCCs, Fourier transforms) represent audio signals in structured formats.\nAudio Formats: Common audio file formats such as MP3 (compressed), FLAC (lossless compressed), and WAV (uncompressed).\n\nSources: Data portals (e.g., Kaggle, UCI Machine Learning Repository), social media platforms like Instagram, Reddit, Flickr (for images), and audio repositories.\n\n\n5.2.1.2 Semi-structured Data\nSemi-structured data does not conform to a rigid schema like structured data but still contains tags or markers to separate semantic elements.\nExamples:\n\nMarkup Languages: HTML, XML, JSON, and YAML files are examples of semi-structured data because they contain hierarchical tags and attributes describing the data, but the content may be variable.\n\nSources: Web content, APIs that deliver data in JSON or XML formats.\n\n\n5.2.1.3 Unstructured Data\nUnstructured data lacks a predefined data model or schema. It is the most common form of data generated and can be more challenging to analyze without preprocessing.\nExamples:\n\nTexts: Documents, emails, articles, or social media posts are typical unstructured data examples.\n\nSources: Document collections, text corpora, email archives.\n\n\n\n5.2.2 Stream / Realtime Data\nStream or realtime data refers to continuous data generated in real-time, often requiring immediate or near-immediate processing. These data types are crucial in applications like live monitoring, interactive systems, and real-time analytics.\nExamples:\n\nAudio Streams: Continuous audio feeds such as online radio broadcasts.\nVideo Streams: Live video feeds from CCTV cameras or autonomous vehicles.\nSensor Data: Real-time telemetry or environmental data from IoT devices, Arduino boards, or embedded systems.\nLive MIDI: Streaming musical performance data, used in live concerts or interactive installations.\nOSC (Open Sound Control): A protocol used for networking sound synthesizers, computers, and other multimedia devices, enabling real-time control.\n\nSources: Online radio platforms, surveillance systems, smart vehicles, embedded IoT devices, live music performance setups.\n\n\n5.2.3 Data Classification\nThe classification of data into static and stream/realtime reflects fundamentally different operational paradigms in digital systems. Static data often allows batch processing, archival, and complex analysis without stringent timing constraints. In contrast, stream data demands continuous, low-latency processing to support live feedback, monitoring, or interaction.\nThe structured / semi-structured / unstructured distinction highlights the complexity of dealing with data formats:\n\nStructured data is well-suited for traditional databases and straightforward analysis.\nSemi-structured data requires flexible parsers and understanding of nested or tagged data.\nUnstructured data often necessitates advanced techniques such as natural language processing or computer vision for meaningful extraction.\n\nUnderstanding these data types and their sources is critical when designing systems for data ingestion, storage, processing, and analysis — especially in fields such as machine learning, multimedia processing, and IoT applications.\n\n\n\n\n\n\n\n\n\nType\nSubtype\nExamples\nSources\n\n\n\n\nStatic\nStructured\nSemi-structured Unstructured\n\nDatasets (CSV, XLS)\nFields: numeric, string, datetime\nClasses: binary, multiclass\nImages (compressed: JPG, uncompressed: BMP)\nMIDI files\nAudio: raw, descriptors, Fourier\nAudio formats: MP3, FLAC, WAV\nMarkup languages: HTML, XML, JSON, YAML\nTexts\n\n\nData portals\nInstagram, Reddit, Flickr\nAudio repositories\nWeb, APIs\nDocument collections\n\n\n\nStream / Realtime\n—\n\nAudio streams (e.g. online radio)\nVideo streams (e.g. CCTV, autonomous vehicles)\nSensor data (e.g. Arduino, real-time telemetry)\nLive MIDI\nOSC (Open Sound Control)\n\n\nOnline radio platforms\nSurveillance systems, smart vehicles\nIoT devices, embedded systems\nLive performance setups\nInteractive art and media systems\n\n\n\n\n\n5.2.3.1 References\nGantz, J., & Reinsel, D. (2012). The Digital Universe in 2020: Big Data, Bigger Digital Shadows, and Biggest Growth in the Far East. IDC iView.\nMarr, B. (2016). Big Data in Practice: How 45 Successful Companies Used Big Data Analytics to Deliver Extraordinary Results. Wiley.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#sonification-as-a-creative-framework",
    "href": "chapters/sonification.html#sonification-as-a-creative-framework",
    "title": "5  Sonification",
    "section": "5.3 Sonification as a Creative Framework",
    "text": "5.3 Sonification as a Creative Framework\nSonification, traditionally defined as the systematic, objective, and reproducible translation of data into sound, finds itself at a dynamic intersection between scientific inquiry and creative expression. While often framed within the methodological rigor of data representation, its deeper potential lies equally in its artistic embodiment—an expressive convergence of logic, perception, and auditory imagination. This duality is vital when considering the use of software in the development of creative code practices for sonification. In its essence, sonification provides a means to explore and interpret data temporally. It transforms static information into dynamic sonic experiences, revealing patterns not just to the analytical mind but to the aesthetic ear. As Gresham-Lancaster (Gresham-Lancaster 2012) argues, the field must extend beyond scientific formalism to embrace a broader spectrum of cultural and musical meaning.\nA foundational concept introduced by Gresham-Lancaster is the distinction between first-order and second-order sonification. First-order sonification typically involves straightforward mapping of data points to sound parameters—e.g., a numerical value controlling oscillator frequency or filter cutoff. This direct translation preserves the integrity of the data but may lack emotional resonance or contextual clarity for listeners unfamiliar with the dataset or mappings. Second-order sonification introduces a higher level of abstraction. Here, data is not merely converted to sound but is used to control compositional structures such as rhythm, harmony, timbre, and formal development. This could manifest through software designs that incorporate statistical analysis of datasets to determine musical motifs or drive stochastic processes that evolve over time. By embedding data within culturally familiar or stylistically resonant frameworks—be it ambient textures, rhythmic patterns, or harmonic progressions allows the sonification to become more legible and emotionally impactful.\nA key insight is that listeners inevitably interpret sound within a cultural and stylistic frame. Whether intentional or not, sonification software maps data to sonic outputs will be perceived through the lens of genre conventions—ambient, noise, glitch, minimalism, etc. This reinforces the necessity for designers of sonification systems to consciously engage with aesthetic decisions. Rather than viewing these as distractions from scientific rigor, they should be recognized as essential design variables that determine how well the sonification communicates. For instance, layering the sonified stream within an evolving drone texture or embedding it in rhythmic pulses tied to diurnal cycles may enhance the intelligibility of subtle shifts. These choices, while extramusical, profoundly affect the user’s ability to detect and interpret meaningful changes.\nThe expressive capacity of sonification technics invites an analogy with sculpture, as cited in Xenakis’ conception of raw mathematical outputs as “virtual stones” to be artistically shaped. The creative coder similarly refines the raw outputs of data-driven patches, sculpting auditory forms through iterative experimentation, parameter tuning, and aesthetic discernment. This process reveals sonification not as a deterministic output of data, but as a collaboration between the structure of information and the intuition of the artist-programmer. Moreover, by incorporating techniques such as timbral mapping, adaptive filtering, and data-driven granular synthesis, it becomes a laboratory for crafting sonic experiences that honor both the source data and the listener’s perceptual world. This is particularly effective when the goal is to embed sonification within broader artistic or performative contexts—installations, interactive systems, or generative concerts—where expressivity and audience engagement are critical.\nThe evolution of sonification within the creative coding domain demands a reconceptualization of what constitutes success in sonification. Beyond reproducibility, the true measure lies in perceptual clarity, aesthetic engagement, and communicative power. As Gresham-Lancaster asserts, the inclusion of musical form, stylistic awareness, and cultural embedding are not luxuries, but necessities for sonification to become a widely adopted and meaningful practice Pd, with its flexibility, open-ended structure, and visual coding paradigm, provides an ideal environment to develop and test both first-order and second-order sonification systems. By embracing both scientific precision and artistic insight, Pd-based sonification becomes a model for interdisciplinary practice—where the auditory exploration of data is not just informative but transformative.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#data-sonification-artworks-database",
    "href": "chapters/sonification.html#data-sonification-artworks-database",
    "title": "5  Sonification",
    "section": "5.4 Data Sonification Artworks Database",
    "text": "5.4 Data Sonification Artworks Database\nThe following links provides an overview of notable sonification artworks, highlighting their creators, data sources, and the unique approaches they employ to transform data into sound.\nData Sonification Archive - curated collection is part of a broader research endeavor in which data, sonification and design converge to explore the potential of sound in complementing other modes of representation and broadening the publics of data. With visualization still being one of the prominent forms of data transformation, we believe that sound can both enrich the experience of data and build new publics.\nSonification Art database by Samuel Van Ransbeeck - It contains now 256 projects, making it an interesting catalogue for sonification art. Besides adding new works, I also cleaned up some things.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#data-humanism-a-visual-manifesto-of-giorgia-lupi",
    "href": "chapters/sonification.html#data-humanism-a-visual-manifesto-of-giorgia-lupi",
    "title": "5  Sonification",
    "section": "5.5 Data Humanism: A Visual Manifesto of Giorgia Lupi",
    "text": "5.5 Data Humanism: A Visual Manifesto of Giorgia Lupi\n\nData is now recognized as one of the foundational pillars of our economy, and the idea that the world is exponentially enriched with data every day has long ceased to be news.\nBig Data is no longer a distant dystopian future; it is a commodity and an intrinsic, iconic feature of our present—alongside dollars, concrete, automobiles, and Helvetica. The ways we relate to data are evolving faster than we realize, and our minds and bodies are naturally adapting to this new hybrid reality built from physical and informational structures. Visual design, with its unique power to reach deep into our subconscious instantly—bypassing language—and its inherent ability to convey vast amounts of structured and unstructured information across cultures, will play an even more central role in this quiet yet inevitable revolution.\nPioneers of data visualization like William Playfair, John Snow, Florence Nightingale, and Charles Joseph Minard were the first to harness and codify this potential in the 18th and 19th centuries. Modern advocates such as Edward Tufte, Ben Shneiderman, Jeffrey Heer, and Alberto Cairo have been instrumental in the field’s renaissance over the past twenty years, supporting the transition of these principles into the world of Big Data.\nThanks to this renewed interest, an initial wave of data visualization swept across the web, reaching a wider audience beyond the academic circles where it had previously been confined. Unfortunately, this wave was often ridden superficially—used as a linguistic shortcut to cope with the overwhelming nature of Big Data.\n“Cool” infographics promised a key to mastering this untamable complexity. When they inevitably failed to deliver on this overly optimistic expectation, we were left with gigabytes of illegible 3D pie charts and cheap, translucent user interfaces cluttered with widgets that even Tony Stark or John Anderton from Minority Report would struggle to understand.\nIn reality, visual design is often applied to data merely as a cosmetic gloss over serious and complicated problems—an attempt to make them appear simpler than they truly are. What made cheap marketing infographics so popular is perhaps their greatest contradiction: the false claim that a few pictograms and large numbers inherently have the power to “simplify complexity.” The phenomena that govern our world are, by definition, complex, multifaceted, and often difficult to grasp. So why would anyone want to dumb them down when making critical decisions or delivering important messages?\nYet, not all is bleak in this sudden craze for data visualization. We are becoming increasingly aware that there remains a considerable gap between the real potential hidden within vast datasets and the superficial images we typically use to represent them. More importantly, we now recognize that the first wave succeeded in familiarizing a broader audience with new visual languages and tools.\nHaving moved past what we might call the peak of infographics, we are left with a general audience equipped with some of the necessary skills to welcome a second wave of more meaningful, thoughtful visualization.\nWe are ready to question the impersonality of a purely technical approach to data and begin designing ways to connect numbers with what they truly represent: knowledge, behaviors, and people.\nCertainly. Here’s a refined and academically styled rephrasing of the provided section, maintaining clarity, structure, and alignment with the tone of an academic book:\n\n5.5.1 Data Humanism and Personal Data\nData Humanism is a perspective initially articulated by Giorgia Lupi (Lupi 2017), grounded in the belief that meaningful engagement with data necessitates attention to its underlying contexts and the inclusion of subjective perspectives throughout the processes of data collection, analysis, and representation—particularly when the data reflects human experience. Rather than reducing information to mere numbers or abstractions, Data Humanism emphasizes the value of personalized, context-rich, and narratively driven interpretations. It advocates for a relationship with data that is not only analytical but also emotional, aesthetic, and reflective (Canossa et al. 2022).\nThis orientation has gained considerable traction within the domains of personal informatics and personal visualization, which investigate how individual-centered representations—visual or tangible—can support self-awareness, reflective thinking, and behavior change. Within these fields, Data Humanism has informed two primary dimensions: data representation and the sensemaking process.\n\n\n5.5.2 Data Representation\nFrom the perspective of Data Humanism, effective data representation involves the creation of complex and personalized visual forms. “Complexity” here does not imply obfuscation, but rather a deliberate move beyond conventional charts and graphs, toward expressive visual metaphors capable of revealing unexpected connections and enriching the narrative potential of the data (Kim et al. 2019).\nPersonalization plays a crucial role in enabling individuals to define and structure data in accordance with their own conceptual frameworks, thus making the resulting representations more relevant and resonant. Moreover, contextual information—embedded at all stages of the data pipeline from collection to display—is essential for constructing coherent and situated personal narratives. This approach aligns with broader discourses in data visualization research that call for expressive and nuanced visual forms, capable of communicating layered meanings rather than merely delivering rapid information.\n\n\n5.5.3 Sensemaking Process\nData Humanism also foregrounds the importance of deep, deliberate engagement in the interpretive act of making sense of data. As Lupi (Lupi 2017) observes, insight does not emerge from superficial scanning but from a sustained exploration of context and meaning. In this light, the process of data sensemaking is framed as investigative, interpretive, and relational, acknowledging the imperfection and approximation that are often intrinsic to human data.\nThis approach encourages individuals to participate actively in the shaping of their own data narratives—through exploration, translation, and imaginative visualization—which in turn facilitates deeper personal connections and empathetic understanding of others. These practices resonate with the concept of slow technology (Hallnäs and Redström 2001), which posits that slower, more reflective interactions can enrich user experience, enabling more space for contemplation and insight. Here, “slowness” is not a limitation but a strategic feature, fostering sustained attention and interpretive depth.\nRecent work in personal visualization has explored how Data Humanism can be applied in practice. One avenue involves sketch-based visualization tools, which leverage the intuitive, open-ended nature of drawing to support the creation of personalized visual representations. Another involves constructive visualizations: physical, often non-digital artifacts that users assemble and manipulate to give form to data. Additionally, digital platforms have been developed that enable the design of expressive visualizations capable of capturing qualitative aspects of personal context, broadening the expressive range of data visual design.\nOverall, Data Humanism advocates for data representations that embrace subjectivity and slowness as virtues rather than limitations. It proposes that personal data is best understood not through abstract generalization, but through thoughtful, expressive, and often collaborative processes. In this book, we extend this line of inquiry by examining how Data Humanism principles can be integrated into collaborative design practices for personal visualization—exploring new ways to humanize, materialize, and narrativize the data that defines and reflects our lives.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#electrocardiogram-data",
    "href": "chapters/sonification.html#electrocardiogram-data",
    "title": "5  Sonification",
    "section": "5.6 Electrocardiogram Data",
    "text": "5.6 Electrocardiogram Data\nThis Pd patch represents an approach to biomedical data sonification, transforming electrocardiogram (ECG) readings into audible waveforms. By treating physiological data as audio samples, the patch creates an intersection between medical diagnostics and sound synthesis, offering both artistic and analytical possibilities for exploring cardiac rhythms through auditory perception.\n\n\n\nElectrocardiogram sonification patch converting physiological data into audio through array storage\n\n\n\n5.6.1 Patch Overview\nThe electrocardiogram synthesis patch implements a data-to-audio pipeline that processes filtered ECG data stored in text format. The system loads cardiac waveform data into Pd arrays and uses audio synthesis techniques to render the biological signals as sound, enhanced with spatial reverberation effects.\nThe electrocardiogram.txt file contains preprocessed electrocardiogram (ECG) data representing cardiac electrical activity captured from a real patient or medical recording device.\n\n5.6.1.1 Format Specification\n\nFile Type: Plain text format (.txt)\nData Points: 47,499 individual samples\nValue Range: Approximately 544 to 1,023\nSampling Structure: One numerical value per line\nData Type: Integer values representing digitized voltage measurements\n\n\n\n5.6.1.2 Signal Properties\nThe ECG data exhibits characteristic patterns of cardiac electrical activity:\n\n\n\n\n\n\n\n\nParameter\nValue\nDescription\n\n\n\n\nBaseline Level\n~830-850\nResting electrical potential\n\n\nPeak Amplitude\n1,023\nMaximum QRS complex values\n\n\nMinimum Values\n544\nDeep S-wave deflections\n\n\nDuration\nVariable\nComplete cardiac cycles with P-QRS-T patterns\n\n\n\nThe high sample count (47,499 points) provides sufficient temporal resolution to capture multiple complete heartbeat cycles, enabling both detailed analysis of individual cardiac events and broader rhythm pattern recognition through auditory display techniques.\n\n\n\n5.6.2 Data Flow\n\n\n\n\n\ngraph TD\n    A[electrocardiogram.txt] --&gt; B[readtxt subpatch]\n    B --&gt; C[Array Sizing]\n    A --&gt; D[Array Loading]\n    D --&gt; E[tabread4~ Audio Reader]\n    F[Phasor~ Oscillator] --&gt; E\n    E --&gt; G[freeverb~ Reverb]\n    G --&gt; H[Audio Output]\n    B --&gt; I[Data Count Output]\n\n\n\n\n\n\nThe readtxt subpatch serves as a preprocessing module that:\n\nOpens the electrocardiogram.txt file\nCounts the total number of data points\nCalculates appropriate array dimensions\nProvides sizing information for memory allocation\n\n\n\n\n\n\ngraph LR\n    A[readtxt subpatch] --&gt; B[Array Sizing]\n    B --&gt; C[Array Loading]\n    C --&gt; D[Audio Conversion]\n\n\n\n\n\n\nThe electrocardiogram sonification patch implements a data transformation pipeline that converts static physiological measurements into dynamic audio experiences. This process involves multiple interconnected stages of data handling, each contributing essential functionality to the overall system’s ability to render cardiac rhythms as meaningful sonic representations.\nThe initial phase of data flow centers on the comprehensive analysis and preparation of the source electrocardiogram dataset. The readtxt subpatch performs a preliminary scan of the electrocardiogram.txt file, systematically counting the total number of individual data points contained within the text document. This preprocessing step proves crucial for subsequent memory allocation decisions, as Pd requires explicit array dimensioning before data can be loaded into memory buffers. The subpatch communicates the determined data quantity to the main patch, enabling dynamic array sizing that accommodates datasets of varying lengths without requiring manual configuration adjustments. Following this analysis phase, the system initiates the actual data transfer process, where numerical values representing cardiac electrical activity are systematically loaded from the text file into Pd’s internal array structures. This loading operation transforms the static file-based data into a format suitable for real-time audio processing, creating a bridge between stored physiological measurements and live sonic synthesis.\nThe audio synthesis stage represents the core transformation where biological data becomes audible waveform. The phasor~ object generates a continuous ramp signal that serves as the fundamental timing mechanism for array traversal, essentially functioning as a playback head that moves through the stored ECG data at a controllable rate. This oscillator’s frequency parameter directly determines the speed at which the cardiac data is sonified, allowing for temporal scaling that can compress hours of ECG recordings into minutes of audio or extend brief cardiac events for detailed auditory analysis. The tabread4~ object performs the critical function of converting discrete array values into continuous audio signals through sophisticated interpolation algorithms. Rather than simply stepping through individual data points, which would produce harsh discontinuities and aliasing artifacts, the four-point interpolation scheme creates smooth transitions between adjacent values, resulting in audio output that maintains the natural flow characteristics of the original cardiac rhythms while remaining suitable for human auditory perception.\nThe final processing stage introduces spatial and timbral enhancements that transform the raw sonified data into aesthetically coherent audio suitable for both analytical and artistic applications. The freeverb~ object applies algorithmic reverberation that adds spatial depth and acoustic warmth. This reverberation processing serves multiple purposes: it masks potential digital artifacts from the sonification process, creates a more immersive listening environment that encourages extended auditory analysis, and provides timbral coherence that helps listeners focus on meaningful patterns rather than being distracted by the mechanical quality of direct data-to-audio conversion. The spatial processing also contributes to the patch’s potential for integration into larger sonic environments, where the ECG sonification might coexist with other audio elements in installations or performance contexts.\nThroughout this entire data flow pathway, the system maintains precise temporal relationships between the original cardiac measurements and their auditory representations, ensuring that the sonification preserves the essential rhythmic and morphological characteristics that define cardiac health and pathology. This fidelity to source data temporal structure enables the sonification to serve legitimate diagnostic and educational purposes while simultaneously opening creative possibilities for artistic exploration of physiological processes.\n\n\n\n\n\ngraph LR\n    A[Data Size] --&gt; B[Array Efficiency]\n    B --&gt; C[Playback Smoothness]\n    C --&gt; D[Audio Quality]\n\n\n\n\n\n\nThe patch maintains signal integrity through:\n\n4-point interpolation for anti-aliasing\nConfigurable playback rates for temporal scaling\nReverb processing for spatial contextualization\n\nThis electrocardiogram synthesis patch demonstrates Pd’s versatility in scientific data visualization through sound, creating meaningful auditory representations of biological processes while maintaining the temporal and amplitude characteristics essential for medical interpretation.\n\n\n5.6.3 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nParameters\n\n\n\n\nreadtxt\nSubpatch for file analysis\nCounts data points in text file\n\n\ntextfile\nFile reading mechanism\nLoads ECG data from external source\n\n\ntable/array\nData storage buffer\nHolds ECG samples for playback\n\n\nphasor~\nPlayback control oscillator\nSweeps through array data\n\n\ntabread4~\nAudio sample interpolator\nReads array as continuous audio\n\n\nfreeverb~\nSpatial audio processor\nAdds reverberation to output",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#ecg-controlled-step-sequencer",
    "href": "chapters/sonification.html#ecg-controlled-step-sequencer",
    "title": "5  Sonification",
    "section": "5.7 ECG-Controlled Step Sequencer",
    "text": "5.7 ECG-Controlled Step Sequencer\nThis patch demonstrates an innovative approach to data-driven rhythm generation, where electrocardiogram readings function as temporal control signals for a step sequencer. By interpreting cardiac rhythm data as beat timing information, the patch creates a unique hybrid system that transforms physiological patterns into musical sequences, establishing a direct relationship between biological rhythms and electronic music production.\n\n\n\nECG-controlled step sequencer using cardiac rhythm data as timing source for audio sample playback, with counter-based sequential data access and BPM scaling for musical rhythm generation.\n\n\n\n5.7.1 Patch Overview\nThe ECG-controlled step sequencer implements a rhythm generation algorithm that uses cardiac data as the fundamental timing source for audio playback. Rather than sonifying the ECG waveform directly, this patch extracts temporal information from the physiological data to control the rhythmic structure of a step sequencer, creating polyrhythmic patterns that reflect the natural variability of human heartbeat intervals.\n\n5.7.1.1 Data Processing Components\nReadTxt Subpatch\nThe readtxt subpatch functions as the primary data interface, managing both file loading and data quantification:\n\nOpens and reads the electrocardiogram.txt file\nCounts total data points for array sizing\nLoads data into local table for sequential access\nProvides file path management and error handling\n\nCounter Subpatch\nThe counter mechanism implements sequential data traversal with automatic cycling:\n\nMaintains current position in ECG dataset\nIncrements through data points on each trigger\nResets to beginning when reaching data endpoint\nProvides configurable upper limits for partial data playback\n\n\n\n\n5.7.2 Data Flow\n\n\n\n\n\ngraph TD\n    A[electrocardiogram.txt] --&gt; B[readtxt subpatch]\n    B --&gt; C[counter subpatch]\n    C --&gt; D[ECG Value Reader]\n    D --&gt; E[BPM Scaling]\n    E --&gt; F[Delay Object]\n    F --&gt; G[Bang Trigger]\n    G --&gt; H[Audio Playback]\n    I[Audio File] --&gt; J[audiotable Array]\n    J --&gt; K[tabplay~ Object]\n    G --&gt; K\n    K --&gt; L[Audio Output]\n\n\n\n\n\n\nThe ECG-controlled step sequencer shows a multi-stage data transformation process that converts static cardiac measurements into dynamic rhythmic triggers. This system demonstrates how biological timing patterns can be extracted and repurposed as musical control data, creating compositions that maintain organic temporal characteristics while serving structured musical functions.\nThe initial data preparation phase centers on the comprehensive loading and organization of the electrocardiogram dataset. The readtxt subpatch performs dual functions: it analyzes the text file structure to determine data quantity and simultaneously loads the numerical values into Pd’s table system for efficient random access. This preprocessing ensures that the cardiac data becomes immediately available for real-time sequential reading without file system delays that would disrupt musical timing. The subpatch communicates essential metadata to the main patch, including total data point count and successful loading confirmation, enabling the counter system to establish appropriate cycling boundaries for continuous operation.\nThe rhythmic control generation stage represents the core innovation where cardiac data becomes musical timing information. The counter subpatch maintains a position index that advances through the ECG dataset on each bang trigger, creating a sequential reading mechanism that treats the physiological data as a tempo map. Each ECG value retrieved through tabread undergoes scaling multiplication to convert the raw cardiac measurement into a meaningful delay time for the del object. This scaling factor determines the relationship between cardiac electrical activity levels and musical tempo, allowing for both literal interpretations where higher cardiac values produce faster rhythms, or inverted mappings where increased cardiac activity corresponds to longer inter-beat intervals, mimicking cardiac refractory periods.\nThe audio playback coordination stage transforms the ECG-derived timing signals into actual sound events through sophisticated sample triggering mechanisms. Each bang generated by the delay object initiates playback of audio material stored in the audiotable array via the tabplay~ object. The soundfiler object ensures that external audio files are properly loaded and resized within the array structure, while the spigot gate provides manual control over the sequencer’s operational state. This architecture enables the system to function as a complete musical instrument where the natural variability of cardiac rhythms generates complex polyrhythmic patterns that would be difficult to achieve through conventional programmed sequencing methods.\n\n\n\n\n\ngraph LR\n    A[ECG Data Loading] --&gt; B[Sequential Access]\n    B --&gt; C[BPM Conversion]\n    C --&gt; D[Delay Generation]\n    D --&gt; E[Audio Triggering]\n\n\n\n\n\n\n\n\n5.7.3 Processing Chain Details\nThe patch maintains sonic coherence through several design considerations: the scaling factor allows adjustment of the overall tempo range to match rhythmn contexts, the counter’s cycling behavior ensures continuous operation without interruption, and the audio loading system supports various sample types from percussive hits to sustained tones. This flexibility enables the ECG step sequencer to function across diverse sound applications, from experimental compositions exploring biological rhythms to dance music productions requiring organic timing variations.\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nText File\nData parsing and counting\nArray population\n\n\n2\nArray Position\nSequential value retrieval\nRaw ECG values\n\n\n3\nECG Values\nMultiplication scaling\nDelay times (ms)\n\n\n4\nDelay Times\nTemporal scheduling\nBang triggers\n\n\n5\nBang Triggers\nAudio sample initiation\nSound output\n\n\n\n\n\n5.7.4 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nParameters\n\n\n\n\nreadtxt\nECG data file reader\nLoads and counts data points from text file\n\n\ncounter\nSequential data access\nSteps through ECG values line by line\n\n\ntabread\nArray value retrieval\nReads individual ECG measurements\n\n\ndel\nRhythmic timing control\nCreates delays based on ECG-derived BPM\n\n\ntabplay~\nAudio sample playback\nTriggers audio file samples\n\n\nsoundfiler\nAudio file loader\nLoads external audio into array\n\n\n\n\n\n5.7.5 Creative Applications\n\nCardiac rhythm layering: Load different ECG datasets from multiple subjects to create polyrhythmic compositions where each person’s heartbeat triggers different audio samples\nMedical data sonification: Use ECG data from various cardiac conditions (arrhythmia, tachycardia, bradycardia) to generate distinct rhythmic patterns for educational or artistic purposes\nBiometric beat matching: Sync live performances to pre-recorded cardiac rhythms, creating music that literally follows the pulse of the performer or subject\nPhysiological drum machines: Replace traditional step sequencer programming with real cardiac data to generate organic, non-repetitive percussion patterns\nHeart rate variability exploration: Use ECG datasets recorded during different emotional states or physical activities to trigger corresponding audio textures\nInteractive health installations: Design gallery pieces where visitors’ real-time heart rates control sample playback, creating personalized sonic experiences\nTemporal scaling experiments: Multiply ECG values by extreme factors to compress hours of cardiac data into minutes of rapid-fire triggering or extend brief arrhythmias into extended compositions\nCollaborative cardiac compositions: Network multiple ECG step sequencers where participants’ combined heart rhythms create collective musical pieces that reflect group physiological states",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#image-sonification-with-rgba-data",
    "href": "chapters/sonification.html#image-sonification-with-rgba-data",
    "title": "5  Sonification",
    "section": "5.8 Image Sonification with RGBA Data",
    "text": "5.8 Image Sonification with RGBA Data\nDigital images contain vast amounts of visual information encoded as pixel data, where each pixel represents color values across different channels. The RGBA image sonification patch transforms this visual data into auditory experiences by converting pixel color values into audio samples. This approach creates a direct mapping between visual and auditory domains, where the Red, Green, Blue, and Alpha channels of an image become independent audio streams that can be manipulated and mixed in real-time.\nThe sonification process reveals hidden patterns and textures within images that may not be immediately apparent through visual inspection alone. By treating pixel data as audio samples, we can explore the rhythmic, harmonic, and textural qualities inherent in visual compositions, creating a synesthetic bridge between sight and sound.\n\n\n\nImage sonification patch converting RGBA pixel data into audio through channel separation and frequency mapping for cross-modal artistic exploration.\n\n\n\n5.8.1 Patch Overview\nThe RGBA image sonification system consists of three main components working in concert:\n\nImage Loading and Analysis: Handles file import and pixel data extraction\nRGB-arrays (Data Processing): Normalizes and stores channel data in arrays\nSonification Engine: Converts pixel data to audio with real-time controls\n\nThe patch workflow follows a clear data pipeline: images are loaded and decomposed into RGBA channels, pixel values are normalized and stored in arrays, and finally converted to audio samples with frequency and amplitude modulation capabilities.\n\n5.8.1.1 Critical Data Processing Objects\npix_dump: This object serves as the bridge between GEM’s visual processing and Pd’s audio domain. It outputs a continuous stream of RGBA values (typically 0-255 range) that represent the complete image dataset.\nArray Management: Each RGBA channel requires its own array for independent manipulation. The arrays store normalized values (-1 to 1) making them suitable for direct audio interpretation.\ntabread4~: Provides crucial interpolation between array values, creating smooth audio transitions rather than harsh digital stepping when reading pixel data as audio samples.\n\n\n\n5.8.2 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Image File] --&gt; B[pix_dump]\n    B --&gt; C[RGBA Pixel Stream]\n    C --&gt; D[RGB-arrays Subpatch]\n    D --&gt; E[Normalized Arrays R,G,B,A]\n    E --&gt; F[Sonification Subpatch]\n    F --&gt; G[Audio Output]\n    H[Frequency Control] --&gt; F\n    I[Amplitude Control] --&gt; F\n    \n    style A fill:#e1f5fe\n    style G fill:#f3e5f5\n    style D fill:#fff3e0\n    style F fill:#fff3e0\n\n\n\n\n\n\nThe RGBA image sonification patch implements a comprehensive data transformation pipeline that converts visual information into auditory experiences through multiple interconnected processing stages. This workflow demonstrates how digital image data can be systematically decomposed, analyzed, and reconstituted as meaningful sonic representations while preserving the essential characteristics of the original visual content.\nThe initial image decomposition stage establishes the foundation for the entire sonification process through the critical function of the pix_dump object. When an image file is loaded into the system, whether in JPEG, PNG, TIFF, or other supported formats, the pix_dump object performs a comprehensive analysis that extracts every pixel’s color information as a sequential stream of numerical values. This extraction process follows a systematic pattern where each pixel contributes four distinct values representing its Red, Green, Blue, and Alpha channel intensities. The resulting data stream takes the form of R1 G1 B1 A1 R2 G2 B2 A2 R3 G3 B3 A3, creating a linear representation of the two-dimensional visual information. This linearization process effectively flattens the spatial relationships of the image into a temporal sequence, transforming visual space into a time-based data structure suitable for audio processing. The pixel values extracted during this stage typically range from 0 to 255, representing the standard 8-bit color depth used in most digital image formats.\nThe channel separation and normalization stage represents a critical transformation where the raw pixel data becomes suitable for audio synthesis applications. The continuous RGBA stream generated by the image decomposition process undergoes systematic demultiplexing, where individual color channel values are separated and directed into dedicated processing pathways. This separation enables independent manipulation of each color channel, allowing for sophisticated sonic textures that can emphasize specific visual characteristics of the original image. The Red channel array captures the warm color information and often contains significant luminance data, while the Green channel typically holds the most visually sensitive information due to human perception characteristics. The Blue channel frequently contains fine detail and cooler color information, and the Alpha channel represents transparency data that may reveal composition and layering information within the original image. Each separated channel undergoes normalization processing that converts the original 0-255 range into the -1 to 1 range required for audio synthesis, ensuring that the pixel data can be directly interpreted as audio sample values without amplitude clipping or distortion.\nThe audio synthesis stage transforms the normalized pixel arrays into dynamic audio streams through sophisticated control mechanisms that enable real-time manipulation of the sonification parameters. Each of the four normalized arrays becomes an independent audio source controlled by tabread4~ objects that perform interpolated reading of the stored pixel data. The frequency parameter, typically ranging from 0.1 to 20 Hz, determines the rate at which the system traverses through the pixel data, effectively controlling the temporal scaling of the visual information. Lower frequency values in the 0.1 to 1 Hz range reveal macro-structures and overall compositional elements of the image, allowing listeners to perceive broad color transitions and major visual forms through gradual sonic evolution. Higher frequencies in the 5 to 20 Hz range expose fine details and textures, creating rapid sonic fluctuations that correspond to pixel-level variations in the original image. The amplitude control parameter provides independent volume adjustment for each RGBA channel, enabling dynamic balancing that can emphasize particular color information or create complex sonic textures through channel interaction. The array index parameter allows for positional control within the image data, enabling targeted exploration of specific regions or systematic scanning of the entire visual content.\nThe signal mixing stage represents the culmination of the sonification process, where the four independent audio channels combine to create a unified sonic representation of the original image. This mixing process employs configurable amplitude controls that enable sophisticated real-time adjustment of the channel balance, allowing users to isolate individual color channels for detailed analysis or create weighted combinations that emphasize particular visual characteristics. Individual channel isolation proves particularly valuable for understanding how specific color information contributes to the overall visual composition, while weighted mixing enables the creation of custom sonic interpretations that highlight particular aspects of the image content. Dynamic balance adjustment during playback allows for evolving sonic textures that can reveal temporal patterns within the visual data or create artistic interpretations that maintain connection to the source material while achieving aesthetic coherence. The mixing stage also incorporates safeguards against amplitude overflow and provides signal conditioning that ensures the final audio output remains within acceptable dynamic ranges for both analytical listening and artistic presentation contexts.\nThroughout this entire data flow pathway, the system maintains precise correspondence between visual and auditory domains, ensuring that the sonification preserves meaningful relationships between image characteristics and sonic parameters. This fidelity enables the patch to serve both analytical purposes, where visual patterns can be detected through auditory analysis, and creative applications, where image data becomes source material for musical composition and sound design. The modular architecture of the data flow also supports extension and customization, allowing users to insert additional processing stages or modify the parameter mappings to achieve specific analytical or artistic objectives.\nThe data transformation process involves several critical stages:\n\n5.8.2.1 Stage 1: Image Decomposition\n\n\n\n\n\nflowchart LR\n    A[Image File] --&gt; B[pix_dump] --&gt; C[R1 G1 B1 A1 R2 G2 B2 A2 R3 G3 B3 A3]\n\n\n\n\n\n\n\n\n5.8.2.2 Stage 2: Channel Separation and Normalization\n\n\n\n\n\nflowchart LR\n    A[RGBA Stream] --&gt; B[Channel Demux]\n    B --&gt; C[Red Array]\n    B --&gt; D[Green Array]\n    B --&gt; E[Blue Array]\n    B --&gt; F[Alpha Array]\n    \n    C --&gt; G[Normalize -1 to 1]\n    D --&gt; H[Normalize -1 to 1]\n    E --&gt; I[Normalize -1 to 1]\n    F --&gt; J[Normalize -1 to 1]\n\n\n\n\n\n\n\n\n5.8.2.3 Stage 3: Audio Synthesis\nEach normalized array becomes an audio source with independent control parameters:\n\n\n\nParameter\nControl Range\nAudio Effect\n\n\n\n\nFrequency\n0.1 - 20 Hz\nPlayback speed through pixel data\n\n\nAmplitude\n0 - 1\nVolume level for each RGBA channel\n\n\nArray Index\n0 - Array Size\nPosition within image data\n\n\n\n\n\n5.8.2.4 Stage 4: Signal Mixing\nThe four audio channels (RGBA) are mixed using configurable amplitude controls, allowing for: - Individual channel isolation - Weighted mixing of color information - Dynamic balance adjustment during playback\n\n\n\n5.8.3 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Sonification\n\n\n\n\npix_dump\nPixel data extraction\nConverts image to sequential RGBA values\n\n\narray\nData storage\nHolds normalized pixel values for each channel\n\n\ntabread4~\nArray interpolation\nSmooth audio sample reading with interpolation\n\n\nphasor~\nTiming control\nGenerates read position for array traversal\n\n\n*~\nAudio mixing\nAmplitude control and channel mixing\n\n\ndac~\nAudio output\nFinal audio rendering\n\n\n\n\n\n5.8.4 Creative Applications\n\nVisual Music Composition: Transform paintings, photographs, or digital art into musical compositions where visual elements directly correlate to audio characteristics\nColor-Sound Synesthesia: Explore the relationship between color perception and auditory experience through real-time sonification\nInteractive Installations: Create responsive environments where visual input generates immediate audio feedback\nPattern Recognition: Audio playback can reveal repetitive structures, gradients, and textures in images that may be difficult to perceive visually\nData Archaeology: Sonify hidden or corrupted image data to identify patterns or anomalies\nComparative Analysis: Compare multiple images by listening to their sonified representations\n\n\n\n5.8.5 Technical Considerations\nFrequency Scaling: The frequency control determines how quickly the system traverses through the pixel data. Lower frequencies (0.1-1 Hz) reveal macro-structures and overall image composition, while higher frequencies (5-20 Hz) expose fine details and textures.\nChannel Weighting: Different RGBA channels contribute varying amounts of perceptual information:\n\nRed channel often contains luminance information\nGreen channel typically has the highest visual sensitivity\nBlue channel may contain fine detail information\nAlpha channel represents transparency data\n\nArray Size Optimization: Large images produce extensive arrays that may require memory management considerations. The patch can be optimized for specific image dimensions based on available system resources.\n\n\n5.8.6 Performance Tips\n\nStart with low frequencies (0.1-0.5 Hz) to understand overall image structure\nIsolate individual channels to understand their unique contribution to the image\nUse amplitude controls to balance channels based on their visual prominence\nExperiment with different image types - photographs, abstract art, and technical diagrams produce distinctly different sonic characteristics\n\nThis sonification approach opens new possibilities for cross-modal artistic expression and analytical exploration, demonstrating how Pd’s flexible architecture enables innovative connections between disparate data domains.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#image-scanner",
    "href": "chapters/sonification.html#image-scanner",
    "title": "5  Sonification",
    "section": "5.9 Image Scanner",
    "text": "5.9 Image Scanner\nThis patch implements a linear scanning sonification that transforms visual image data into audio through systematic pixel sampling. By creating multiple scan points that traverse an image from left to right, the patch converts spatial color information into frequency-controlled oscillators, creating a real-time audio representation of visual content. This approach enables users to “hear” images through systematic exploration of pixel data across defined scan lines.\nThe scanning process reveals the vertical distribution of visual information within images, where different scan lines capture distinct horizontal slices of the visual content. Each scan point operates independently, creating polyphonic textures that reflect the complexity and variation present in the source image.\n\n\n\nImage scanner with configurable scan lines traversing visual content from left to right, converting pixel intensity into frequency-controlled oscillators for polyphonic sonification.\n\n\n\n5.9.1 Patch Overview\nThe linear image scanner consists of four primary subsystems that work together to create a comprehensive image-to-audio conversion pipeline:\n\nImage Loading and Display: Manages file import and visual presentation through GEM\nDynamic Scan Point Generation: Creates configurable numbers of scanning instances\nPixel Data Extraction: Samples color values at specific image coordinates\nAudio Synthesis: Converts pixel data into frequency-controlled oscillators\n\nThe patch employs dynamic patching techniques to generate scalable numbers of scan points, each implemented as an instance of the linear-scan abstraction. This modular approach enables flexible configuration while maintaining efficient processing of multiple simultaneous scan lines.\n\n5.9.1.1 Critical Processing Components\npix_data Object: This GEM object serves as the core interface between visual and audio domains. It samples pixel color values at specified x,y coordinates within the loaded image. The normalize message controls coordinate interpretation: when enabled (default), coordinates range from (0.0, 0.0) at bottom-left to (1.0, 1.0) at top-right; when disabled, coordinates use actual pixel dimensions.\nDynamic Patch Generation: The generate-scanpoints subpatch implements dynamic patching techniques to create configurable numbers of linear-scan instances. This system calculates y-position arguments for each scan line and generates the corresponding patch objects programmatically.\nlinear-scan Abstraction: Each scan line is implemented as an abstraction that receives a y-position argument determining its vertical position within the image. The abstraction contains the pixel sampling logic and frequency mapping calculations specific to that scan line.\n\n\n\n5.9.2 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Image File] --&gt; B[pix_image/pix_texture]\n    B --&gt; C[Visual Display]\n    D[Number of Scan Points] --&gt; E[Dynamic Patch Generation]\n    E --&gt; F[linear-scan Abstractions]\n    F --&gt; G[pix_data Objects]\n    G --&gt; H[Pixel Color Extraction]\n    H --&gt; I[Frequency Mapping]\n    I --&gt; J[Oscillator Control]\n    J --&gt; K[Audio Output]\n    L[Scan Duration Control] --&gt; M[Temporal Coordination]\n    M --&gt; F\n    \n    style A fill:#e1f5fe\n    style K fill:#f3e5f5\n    style E fill:#fff3e0\n    style F fill:#fff3e0\n\n\n\n\n\n\nThe linear image scanner a real-time processing pipeline that transforms static visual information into dynamic audio representations through systematic pixel sampling and frequency mapping. This process demonstrates how spatial image data can be converted into temporal audio experiences while preserving meaningful relationships between visual characteristics and sonic parameters.\nThe image loading and preparation stage establishes the foundation for the entire scanning process through comprehensive file management and visual processing setup. When an image file is loaded via the import-image subpatch, the system employs GEM’s pix_image object to read various image formats including JPEG, PNG, and TIFF into Pd’s visual processing environment. The loaded image undergoes texture processing through the pix_texture object, which prepares the pixel data for efficient random access while simultaneously enabling visual display within the GEM rendering window. This dual functionality ensures that users can see the image being processed while the scanning system extracts pixel information for audio synthesis. The image data becomes available to multiple pix_data objects that will sample specific pixel locations, with the texture object maintaining the image in memory for continuous access throughout the scanning process.\nThe dynamic scan point generation stage represents a sophisticated implementation of Pd’s dynamic patching capabilities, enabling user-configurable scanning resolution. The generate-scanpoints subpatch analyzes the user-specified number of scan points and calculates the appropriate y-coordinate distribution across the image height. For each scan point, the system generates a new instance of the linear-scan abstraction with a specific y-position argument that determines where that scan line will sample pixels vertically within the image. This y-position argument undergoes normalization calculation to ensure proper distribution from 0.0 (bottom of image) to 1.0 (top of image), creating evenly spaced scan lines that cover the entire vertical dimension of the source image. The dynamic generation process creates the necessary patch connections and initializes each scan line instance with its unique vertical position parameter, enabling parallel processing of multiple image regions simultaneously.\nThe pixel sampling and coordinate management stage implements the core data extraction functionality where visual information becomes available for audio processing. Each linear-scan abstraction contains a pix_data object configured to sample pixels at its assigned y-coordinate while the x-coordinate varies continuously during the scanning process. The temporal control system coordinates the scanning motion through line objects that interpolate x-position values from 0.0 to 1.0 over the user-specified scan duration. This creates synchronized left-to-right motion across all scan lines, with each line sampling pixels at its fixed y-position while traversing the full width of the image. The pix_data objects output both grayscale and RGB color values for each sampled pixel, providing multiple data streams that can be used for different aspects of the audio synthesis process. The temporal coordination ensures that all scan lines move in synchronization, creating coherent scanning motion that preserves the spatial relationships present in the original image.\nThe frequency mapping and audio synthesis stage transforms the extracted pixel data into meaningful audio parameters through sophisticated scaling and oscillator control. Each scan line’s pixel sampling output undergoes frequency mapping that converts color intensity values into specific frequency ranges for audio oscillators. The frequency distribution system maps the y-position of each scan line to a specific frequency band, creating a spectral arrangement where scan lines at the bottom of the image control lower frequencies while scan lines at the top control higher frequencies. This vertical frequency mapping creates intuitive correspondence between image position and audio pitch, enabling listeners to perceive spatial relationships within the image through pitch relationships in the audio output. The pixel intensity values modulate the oscillator frequencies within each scan line’s assigned frequency band, so brighter pixels produce higher frequencies while darker pixels produce lower frequencies within that band. Multiple oscillators operating simultaneously create polyphonic textures that reflect the complexity and variation present across different regions of the source image.\n\n\n\n\n\nflowchart LR\n    A[Image Loading] --&gt; B[Scan Point Generation]\n    B --&gt; C[Pixel Sampling]\n    C --&gt; D[Frequency Mapping]\n    D --&gt; E[Audio Synthesis]\n\n\n\n\n\n\n\n\n5.9.3 Processing Chain Details\nThe scanning system maintains precise temporal and spatial coordination through several sophisticated control mechanisms:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nImage File\nGEM loading and texture preparation\nAccessible pixel data\n\n\n2\nScan Point Count\nDynamic abstraction generation\nMultiple linear-scan instances\n\n\n3\nTime Position\nSynchronized x-coordinate interpolation\nPixel sampling coordinates\n\n\n4\nPixel Color Values\nIntensity to frequency conversion\nOscillator control signals\n\n\n5\nMultiple Frequencies\nPolyphonic audio mixing\nCombined audio output\n\n\n\n\n5.9.3.1 Temporal Control Parameters\n\n\n\nParameter\nControl Range\nAudio Effect\n\n\n\n\nScan Duration\n100-90000 ms\nSpeed of left-to-right traversal\n\n\nFrequency Range\n100-5000 Hz\nSpan of available frequencies\n\n\nScan Point Count\n1-256 points\nNumber of simultaneous scan lines\n\n\nY-Position Distribution\n0.0-1.0\nVertical spacing of scan lines\n\n\n\n\n\n5.9.3.2 Coordinate Mapping System\nThe patch implements sophisticated coordinate management that ensures proper spatial relationships between image pixels and audio parameters:\nSpatial Mapping: Each scan line occupies a fixed y-coordinate determined by its position in the overall distribution, while x-coordinates vary temporally from left to right during scanning.\nFrequency Allocation: The vertical position of each scan line determines its base frequency range, creating spectral separation that preserves spatial information in the audio domain.\nTemporal Synchronization: All scan lines move simultaneously from left to right, maintaining spatial coherence while revealing temporal evolution of visual content.\n\n\n\n5.9.4 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Scanning\n\n\n\n\npix_image\nImage loading\nLoads image files into GEM processing chain\n\n\npix_texture\nTexture rendering\nEnables visual display and pixel access\n\n\npix_data\nPixel sampling\nExtracts color values at specified coordinates\n\n\nlinear-scan\nScan line abstraction\nImplements individual scanning instances\n\n\nosc~\nAudio oscillator\nGenerates frequency-controlled sine waves\n\n\nline\nTemporal interpolation\nControls scanning position over time\n\n\nmetro\nTiming control\nCoordinates scan timing across instances\n\n\n\n\n\n5.9.5 Creative Applications\n\nMusical Score Scanning: Transform written musical notation into audio by scanning staff lines, converting note positions into corresponding pitches and rhythms\nTexture Analysis: Explore the sonic characteristics of different visual textures by scanning patterns, fabrics, or surface details to reveal their rhythmic and harmonic qualities\nArchitectural Sonification: Scan building facades, floor plans, or structural diagrams to create audio representations of architectural spaces and proportions\nData Visualization Audio: Convert scientific charts, graphs, and data visualizations into audio to provide alternative access methods for visually impaired users\nArtistic Collaboration: Enable visual artists to hear their compositions while creating, providing real-time audio feedback during the visual creative process\nInteractive Installations: Create gallery pieces where visitors can upload images and immediately hear their sonic representation through the scanning system\nEducational Tools: Teach concepts of frequency, spatial relationship, and data representation by demonstrating direct correlations between visual and audio domains\nLive Performance Integration: Use the scanner with live video feeds or real-time image manipulation to create dynamic audio responses to visual performance elements\nAstronomical Data Exploration: Scan telescope images, star charts, or planetary surfaces to create audio representations of cosmic phenomena and celestial structures",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#camera-scanner",
    "href": "chapters/sonification.html#camera-scanner",
    "title": "5  Sonification",
    "section": "5.10 Camera Scanner",
    "text": "5.10 Camera Scanner\nThis patch extends the linear image scanning concept to real-time video input, creating dynamic sonification of live camera feeds. Unlike the static image scanner previously discussed, this patch processes continuous video streams from web cameras, enabling real-time exploration of moving visual content through audio. The patch maintains the core scanning methodology while introducing temporal dynamics that reflect the changing nature of live video input.\nThe live camera scanner transforms the temporal dimension of video into immediate sonic feedback, where movements, lighting changes, and scene transitions become audible through frequency modulation across multiple scan lines. This creates an interactive relationship between the physical environment captured by the camera and the generated audio landscape.\n\n\n\nLive camera scanner with real-time video processing, dynamic scan line generation, and enhanced controls for contrast adjustment and random scanning patterns.\n\n\n\n5.10.1 Patch Overview\nThe live camera scanner builds upon the established linear scanning framework while incorporating several enhancements specific to real-time video processing:\n\nLive Video Capture: Continuous camera input with adjustable contrast and color inversion\nEnhanced Scanning Controls: Random scanning modes and interpolated scan transitions\nReal-time Processing: Immediate response to visual changes in the camera feed\nExtended User Interface: Additional controls for camera manipulation and scanning behavior\n\nThe system maintains the same dynamic patching architecture and frequency mapping strategies established in the static image scanner, ensuring consistency in the sonification approach while adapting to the temporal nature of video input.\n\n5.10.1.1 Live Video Processing Components\npix_video Object: Captures real-time video input from connected cameras, providing the continuous data stream that replaces static image loading. The object supports multiple camera devices and delivers frame-by-frame pixel data to the scanning system.\nImage Enhancement Chain: The pix_2grey → pix_contrast → pix_invert processing chain offers real-time image manipulation capabilities that enhance the scanning process by adjusting visual characteristics to optimize sonic output.\nRandom Scanning System: Unlike static image scanning, the live camera scanner includes stochastic elements that introduce unpredictability to the scanning patterns, creating evolving audio textures that respond to both visual content and algorithmic variation.\n\n\n\n5.10.2 Data Flow\nThe live camera scanner use a continuous real-time processing pipeline that transforms streaming video data into dynamic audio representations. While maintaining the core scanning methodology established in the static image scanner, this system introduces temporal continuity and enhanced control mechanisms that respond to the evolving nature of live video input.\nThe live video capture and preprocessing stage establishes a continuous data source through integrated camera management and image enhancement systems. The pix_video object maintains a persistent connection to the selected camera device, delivering frame-by-frame pixel data at standard video refresh rates. This continuous stream undergoes real-time preprocessing through a sophisticated image enhancement chain that optimizes the visual content for effective sonification. The pix_2grey object converts color video to grayscale, reducing computational complexity while preserving luminance information essential for frequency mapping. The pix_contrast object provides real-time contrast adjustment, enabling users to emphasize or diminish visual details based on scanning requirements. The optional pix_invert stage creates alternative visual representations that can reveal hidden patterns or provide aesthetic variations in the sonic output. This preprocessing chain ensures that the video data reaches the scanning system in an optimized format that maximizes the effectiveness of the pixel-to-frequency mapping process.\nThe enhanced scanning control stage introduces sophisticated temporal management systems that extend beyond the basic left-to-right scanning pattern. The system maintains the established dynamic scan point generation methodology while incorporating additional control mechanisms for temporal behavior. The random scanning subsystem employs random and spigot objects to introduce stochastic elements into the scanning pattern, creating unpredictable position variations that generate evolving audio textures independent of visual content changes. The scan interpolation system provides smooth transitions between scanning positions, reducing audio artifacts that might result from abrupt position changes in the video stream. The temporal coordination mechanisms ensure that all scan lines maintain synchronized motion while accommodating the additional complexity introduced by random positioning and interpolated movement patterns.\nThe real-time frequency mapping stage adapts the established pixel-to-frequency conversion methodology to accommodate the continuous nature of video input. Each scan line continuously samples pixels at its assigned y-coordinate while the x-coordinate evolves through the scanning pattern, creating ongoing frequency modulation that reflects both spatial relationships within the current video frame and temporal changes across successive frames. The frequency distribution system maintains the vertical mapping where scan lines at the bottom of the image control lower frequencies while scan lines at the top control higher frequencies, preserving the spatial-to-spectral correspondence established in the static scanner. However, the continuous nature of video input introduces temporal frequency modulation that creates evolving harmonic textures as visual content changes over time. The amplitude and frequency controls provide real-time adjustment capabilities that enable users to respond to changing lighting conditions, scene transitions, or desired aesthetic effects during live performance or analysis situations.\n\n\n\n\n\nflowchart LR\n    A[Live Video Input] --&gt; B[Image Enhancement]\n    B --&gt; C[Dynamic Scanning]\n    C --&gt; D[Frequency Mapping]\n    D --&gt; E[Real-time Audio]\n\n\n\n\n\n\n\n\n5.10.3 Processing Chain Details\nThe live camera scanner maintains the established processing architecture while incorporating real-time enhancements:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nCamera Stream\nVideo capture and preprocessing\nEnhanced video frames\n\n\n2\nEnhanced Frames\nDynamic scan point coordination\nSynchronized scan positions\n\n\n3\nScan Positions\nReal-time pixel sampling\nContinuous color values\n\n\n4\nColor Values\nLive frequency mapping\nDynamic oscillator control\n\n\n5\nAudio Signals\nReal-time mixing and output\nLive audio stream\n\n\n\n\n5.10.3.1 Enhanced Control Parameters\n\n\n\n\n\n\n\n\nParameter\nControl Range\nLive Scanning Effect\n\n\n\n\nContrast\n0-10\nVisual enhancement for improved frequency mapping\n\n\nRandom Scan\nOn/Off\nStochastic scanning pattern generation\n\n\nInvert Colors\nOn/Off\nAlternative visual representation\n\n\nScan Interpolation\nSmooth/Linear\nTransition quality between scan positions\n\n\n\n\n\n5.10.3.2 Real-time Performance Considerations\nFrame Rate Coordination: The scanning system synchronizes with video frame rates to ensure smooth audio generation without dropouts or artifacts caused by timing mismatches between video capture and audio processing.\nAdaptive Frequency Scaling: Real-time adjustment capabilities enable the system to respond to varying lighting conditions and scene content, maintaining optimal frequency mapping across different visual environments.\nComputational Efficiency: The live processing requirements demand efficient algorithms that can maintain real-time performance while processing multiple scan lines simultaneously across continuous video streams.\n\n\n\n5.10.4 Key Objects and Their Roles\nBuilding upon the previously established scanning framework, the live camera scanner introduces several additional components:\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Live Scanning\n\n\n\n\npix_video\nCamera input capture\nProvides continuous video stream from webcam\n\n\npix_contrast\nImage enhancement\nAdjusts pixel intensity for better scanning contrast\n\n\npix_invert\nColor inversion\nCreates alternative visual representations\n\n\npix_2grey\nGrayscale conversion\nSimplifies color data for processing\n\n\nrandom\nStochastic positioning\nGenerates unpredictable scan patterns\n\n\n\n\n\n5.10.5 Creative Applications\n\nLive Performance Visualization: Create real-time audio responses to performer movements, gestures, or choreographed actions captured by the camera system\nEnvironmental Sonification: Transform ambient visual environments into evolving soundscapes that reflect changing lighting, weather conditions, or natural phenomena\nInteractive Installation Experiences: Enable gallery visitors to influence the sonic environment through their physical presence and movements within the camera’s field of view\nSurveillance Audio: Convert security camera feeds into audio monitoring systems that provide auditory alerts for visual changes or movement detection\nAccessibility Applications: Provide real-time audio descriptions of visual environments for visually impaired users, translating spatial and temporal visual information into sonic representations\nDance and Movement Analysis: Create audio feedback systems for movement training where dancers can hear the sonic representation of their spatial positioning and temporal dynamics\nTelepresence Audio: Enable remote participants to experience distant visual environments through real-time sonification of camera feeds from other locations\nMachine Vision Integration: Combine computer vision algorithms with live scanning to create responsive systems that sonify specific detected objects, faces, or movement patterns",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#references-1",
    "href": "chapters/sonification.html#references-1",
    "title": "5  Sonification",
    "section": "References",
    "text": "References\n\n\n\n\nCanossa, Alessandro, Luis Laris Pardo, Michael Tran, and Alexis Lozano Angulo. 2022. “From Data Humanism to Metaphorical Visualization–an Educational Game Case Study.” In International Conference on Human-Computer Interaction, 109–17. Cham: Springer.\n\n\nGresham-Lancaster, Scot. 2012. “Relationships of Sonification to Music and Sound Art.” AI and Society 27 (2): 207–12.\n\n\nHallnäs, Lars, and Johan Redström. 2001. “Slow Technology–Designing for Reflection.” Personal and Ubiquitous Computing 5: 201–12.\n\n\nKim, Nam Wook, Hyejin Im, Nathalie Henry Riche, Alicia Wang, Krzysztof Gajos, and Hanspeter Pfister. 2019. “Dataselfie: Empowering People to Design Personalized Visuals to Represent Their Data.” In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, 1–12. New York, NY, USA: Association for Computing Machinery.\n\n\nLupi, Giorgia. 2017. “Data Humanism: The Revolutionary Future of Data Visualization.” PrintMag. https://www.printmag.com/article/data-humanism-future-of-data-visualization/.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/space.html",
    "href": "chapters/space.html",
    "title": "6  Spatial Audio",
    "section": "",
    "text": "6.1 The Music Sound Space\nIn the realm of sound, the concept of space is often overlooked. Yet, it plays a crucial role in shaping our auditory experience. The spatial dimension of sound is not merely an acoustic phenomenon; it is a fundamental aspect of how we perceive sound. This chapter explores the multifaceted nature of sound space, its historical evolution, and its implications for contemporary electroacoustic music.\nSound always unfolds within a specific time and space. From the very moment sound is generated in music, space is implicitly present, enabling the possibility of organizing, reconstructing, and shaping that space to influence musical form. Thus, the use of space in music responds to concerns that go far beyond mere acoustic considerations.\nConceiving space as a structural element in the construction of sonic discourse requires us to address a complex notion—one that touches on multiple dimensions of composition, performance, and perception. This idea rests on the argument that space is a composite musical element, one that can be integrated into a compositional structure and assume a significant role within the formal hierarchy of sound discourse.\nSpatiality in music represents a compositional variable with a long historical lineage. The treatment of sound space on stage can be traced back to classical Greek theatre, where actors and chorus members used masks to amplify vocal resonance and enhance vocal directionality (Knudsen 1932). However, while compositional techniques addressing spatial sound have been developing for centuries, it is not until the early 20th century that a systematic use of spatial dimensions becomes central to musical structure. Landmark works such as Universe Symphony (1911–51) by Charles Ives, Déserts (1954) and Poème électronique (1958) by Edgard Varèse, Gruppen (1955–57) by Karlheinz Stockhausen, and Persephassa (1969) by Iannis Xenakis approach spatiality as an independent and structural dimension. In these works, space is interrelated with other sonic parameters such as timbre, dynamics, duration, and pitch.\nParticularly noteworthy is that these pieces do not merely establish sonic space as a new musical dimension—they also develop theoretical frameworks for the spatialization of sound. These theories consider physical, poetic, pictorial, and perceptual aspects of spatial sound. As a result, in recent decades the areas of research and application related to artistic-musical knowledge have expanded significantly, fostering interdisciplinary inquiry into sound space and its parameters.\nBeyond its central role in music, in recent years spatial sound has also become a subject of study across a variety of disciplines. One example of this is found in research into sound spatialization techniques in both real and virtual acoustic environments. These techniques have largely evolved based on principles of psychoacoustics, cognitive modeling, and technological advancements, leading to the development of specialized hardware and software for spatial sound processing. In the scientific realm, theses and research articles have explored, in great detail, the cues necessary to create an acoustic image of a virtual source and how these cues must be simulated—whether by electronic circuitry or computational algorithms.\nHowever, it must be noted that musical techniques for managing sound space have not yet fully integrated findings from perceptual science. As we will explore later, many artistic approaches rely on only a subset of the cues involved in auditory spatial perception, overlooking others that are essential for producing a convincing acoustic image.\nIn general terms, the spatial dimension of a musical composition can function as the primary expressive and communicative attribute for the listener. For instance, from the very first moment, the acoustic characteristics of a venue—real or virtual—directly affect how we perceive the sonic discourse. This dimension of musical space is further shaped by the placement of sound sources, whether instrumentalists or loudspeakers. A historical precedent for this can be found in the Renaissance period through the use of divided choirs in the Basilica of San Marco in Venice. This stylistic practice, known as antiphonal music, represents a clear antecedent of the deep connection between musical construction and architectural space (Stockhausen 1959). The style of Venetian composers was strongly influenced by the acoustics and architectural features of San Marco, involving spatially separated choirs or instrumental groups performing in alternation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatial Audio</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#the-music-sound-space",
    "href": "chapters/space.html#the-music-sound-space",
    "title": "6  Spatial Audio",
    "section": "",
    "text": "Floor plan of the Basilica of San Marco in Venice (Italy).\n\n\n\n6.1.1 Spatial Distribution and the Evolution of Listening Spaces\nDuring the European Classical and Romantic periods, the spatial distribution of musicians generally adhered to the French ideal inspired by 19th-century military band traditions. In this widely adopted setup, space was typically divided into two main zones: musicians arranged on a frontal stage and the audience seated facing them. This concert model—linear, frontal, and fixed—reflected a formalized, hierarchical approach to the musical experience.\nBy the 20th century, this front-facing paradigm began to be reexamined and reimagined. Composers and sound artists started to question the limitations of conventional concert hall layouts. A notable example is found in 1962, when German composer Karlheinz Stockhausen proposed a radical redesign of the concert space to accommodate the evolving needs of contemporary composition. His vision included a circular venue without a fixed podium, flexible seating arrangements, adaptable ceiling and wall surfaces for mounting loudspeakers and microphones, suspended balconies for musicians, and configurable acoustic properties. These directives were not merely architectural; they were compositional in nature, meant to transform the listening experience by embedding spatiality into the core of musical structure.\nThis reconceptualization of space enabled a new way of thinking about the sensations and experiences that could be elicited in the listener through the manipulation of spatial audio. The concert hall was no longer a passive container of sound, but an active participant in its articulation.\nIn parallel developments, particularly in the field of electroacoustic music during the mid-20th century, spatial concerns were often shaped by the available technology of the time. Spatiality lacked a clearly defined theoretical vocabulary and was not yet integrated systematically with other familiar musical parameters. Nonetheless, from its inception, electroacoustic music leveraged technology to expand musical boundaries—either through electronic signal processing or by interacting with traditional instruments.\nHowever, the notion of space in electroacoustic music differs significantly from that of instrumental music. In this context, spatiality is broad, multidimensional, and inherently difficult to define. It encompasses not only the individual sonic identity of each source—typically reproduced through loudspeakers—but also the relationships between those sources and the acoustic space in which they are heard. Critically, the audible experience of space unfolds through the temporal evolution of the sonic discourse itself.\nLoudspeakers in electroacoustic music afford a uniquely flexible means of organizing space as a structural element. The spatial potential enabled by electroacoustic technologies has been, and continues to be, highly significant. With a single device (see Fig. 2), it is possible to transport the listener into a wide range of virtual environments, expanding the scope of musical space beyond the physical limitations of the performance venue. This opens the door to sonic representations of distance, movement, and directionality—factors that can be fully integrated into the musical structure and treated as compositional dimensions in their own right.\n\n\n\nMultichannel electroacoustic concert hall.\n\n\n\n\n\n6.1.2 Spatial Recontextualization and Auditory Expectation\nOne of the defining characteristics of electroacoustic music is its capacity to recontextualize sound. Through virtual spatialization, a sound can acquire new meaning depending on how it functions within diverse contexts. For example, two sounds that would never naturally coexist—such as a thunderstorm and a mechanical engine in the same acoustic scene—can be juxtaposed to create a landscape that defies ecological realism. This deliberate disjunction can trigger a complex interaction between what is heard and the listener’s prior knowledge of those sound sources, drawn from personal experience.\nOur ability to interpret spatial cues in sound is deeply shaped by the patterns of interpersonal communication we engage in, the lived experience of urban or rural environments, and the architectural features of our surroundings. These formative influences are so embedded in our perceptual systems that we are often unaware of their role in shaping how we understand sensory information. Spatial cues are constantly processed in our everyday auditory experience and play a vital role in shaping our listening behaviors.\nIn the context of electroacoustic music, environmental cues may not only suggest associations with a physical location but also inform more abstract properties of the spatial discourse articulated by the piece. Jean-Claude Risset (Risett 1969) noted a tension in using environmental recordings in electroacoustic composition: the recognizable identity of “natural” sounds resists transformation without diminishing their spatial content. Nevertheless, identifiable sounds remain central to electroacoustic works, particularly those aligned with the tradition of musique concrète.\nUnder normal listening conditions, perception of a singular sound source is shaped by what Pierre Schaeffer (Schaeffer 2003) defined as the “sound object”—a sonic phenomenon perceived as a coherent whole, grasped through a form of reduced listening that focuses on the sound itself, independent of its origin or meaning. Our auditory system has evolved to identify and locate such sound objects in space by mapping them according to the physical attributes of their sources. A barking sound is understood as coming from a dog; a voice is mapped to a human speaker. As a result, the spatial interpretation of sound is closely linked to the listener’s expectations, shaped not only by the inherent acoustic characteristics of the signal but also by the listener’s accumulated learning and experience.\nUnlike incidental listening in everyday life, attentive listening in electroacoustic music generally occurs from a fixed position. Apart from minor head movements, the spatial information available to the listener depends entirely on the acoustic cues encoded in the sound. While spatial hearing has been extensively researched in the field of psychoacoustics, its compositional potential as a carrier of musical form remains underexplored. In particular, the dimension of distance has received little attention compared to azimuth (horizontal angle) and elevation (vertical angle). These two spatial dimensions have been rigorously studied and well-documented, forming the basis of many standard models of spatial perception.\nBy contrast, the auditory perception of distance remains one of the most enigmatic topics in both music and psychophysics. It involves a complex array of cues—many of which are still not fully understood or integrated into compositional practice. As such, distance remains an open and promising field of inquiry for creative and scientific exploration alike (Abregú, Calcagno, and Vergara 2012).\n\n\n\nSchematic representation of the azimuth, elevation, and distance axes.\n\n\n\n\n\n6.1.3 The Expanded Spatial Palette of Electroacoustic Music\nUnlike instrumental music, which relies on the physicality of pre-existing sound sources, electroacoustic music is not constrained by such limitations. This fundamental distinction allows composers not only to design entirely new sounds tailored to their spatial intentions but also to explore acoustic environments that defy conventional physical logic. While a performance of acoustic music offers visual and sonic cues grounded in the material world—a stage, a hall, visible instruments—electroacoustic music invites a more abstract engagement, where the listener navigates a virtual sound world.\nIn acoustic settings, physical space provides the listener with consistent spatial information. In contrast, electroacoustic environments—particularly those utilizing multichannel reproduction—radically extend the spatial canvas. These environments challenge and expand our auditory expectations, offering a broader spectrum of spatial strategies shaped by the interaction between sound diffusion technologies and the perceptual mechanisms of the listener.\nYet, this very potential also brings complications. The ephemeral and immaterial nature of loudspeaker-based sound makes it difficult to generate a spatial image as vivid or intuitive as that encountered in the physical world. Despite technological precision, virtual spatial environments often lack the tactile immediacy of real acoustic spaces.\nResearch in the field of computer music typically falls into two major domains: the study of spatial hearing and cognition, and the development of sound reproduction technologies. One of the advantages in electroacoustic contexts is the ability to isolate and manipulate spatial cues independently—a task nearly impossible with traditional acoustic instruments. This level of control opens up possibilities for treating space as a fully sculptable compositional parameter. However, practical results vary greatly, and many perceptual questions remain unresolved. It is within this intersection—where psychophysics, spatialization technologies, and compositional imagination meet—that the most fertile ground for innovation lies.\nA classic example is John Chowning’s Turenas (1972), which employed quadraphonic speaker arrangements and a mathematical model to simulate virtual spatial motion. Chowning implemented a distance-dependent intensity multiplier according to the inverse square law, applied independently to each channel. The outcome was a virtual “phantom source” whose movement through space was dynamically shaped and perceptually engaging. Interestingly, Chowning discovered that simple, well-structured paths—such as basic geometric curves—produced the most convincing spatial gestures. In On Sonic Art (Wishart 1996), one finds multiple visual representations of two-dimensional spatial trajectories; however, these diagrams often fail to translate into equally perceptible auditory experiences. For instance, while Lissajous curves are visually complex and elegant, their perceptual distinctiveness can be minimal. Clarity, it turns out, is key to audible spatial expression.\nSince the mid-20th century, various spatialization systems have emerged to simulate acoustic spaces through loudspeaker arrays. Among the most notable are Intensity Panning, Ambisonics, and Dolby 5.1. Although a detailed technical comparison of these systems lies beyond the scope of this section (see (Basso, Di Liscia, and Pampin 2009) for an extensive overview), it is worth noting that electroacoustic space only becomes meaningful if its structural characteristics—such as reverberation—can be clearly perceived. In orchestral music, spatial features are often inherent to the instrument layout and performance context. With fixed instrument positions, the spatial configuration is relatively stable. In virtual space, however, reverberation must be explicitly created and shaped to define the acoustic character of the environment in which sonic events unfold.\nUnlike the relatively fixed reverberation properties of physical rooms, virtual spaces offer remarkable plasticity. Composers can craft distinct acoustic identities for different sections of a piece, allowing each space to function as a compositional agent in its own right. This flexibility implies that in electroacoustic music, space must often be invented—not merely inherited from the sounding objects, as is typically the case with acoustic instruments.\nIt becomes clear, then, that spatial design can be a powerful structural element in music. Sound space carries poetic, aesthetic, and expressive dimensions. At the same time, convincing spatial construction often depends on our embodied experience of real-world acoustics. This dual grounding—in perceptual realism and creative abstraction—opens new theoretical and practical avenues for artists. The potential to reconceptualize spatiality through an expanded compositional lens can empower creators to engage more fully with the multidimensionality of sonic art. In doing so, it enables the development of richer analytical and taxonomical criteria that extend beyond sound itself, embracing broader cultural, perceptual, and technological contexts.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatial Audio</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#spatial-localization-of-sound",
    "href": "chapters/space.html#spatial-localization-of-sound",
    "title": "6  Spatial Audio",
    "section": "6.2 Spatial Localization of Sound",
    "text": "6.2 Spatial Localization of Sound\nWhen we perceive a sound event, we rely on a variety of auditory cues to infer the position of its source. These cues are associated with both direction and distance, and are typically classified into two main categories: binaural cues, which arise from comparisons between the two ears, and monaural cues, which are perceived by a single ear and are still critical for spatial localization.\nTo illustrate the binaural mechanism, consider a point sound source moving in a horizontal circular path around a listener’s head. At any given moment, the distance from the source to each ear will differ depending on the angular position of the source. This spatial configuration results in time and intensity differences between the two ears. These discrepancies are key to our spatial perception of sound.\nWhen a sound wave reaches the right ear before the left, the slight delay in arrival time is known as the Interaural Time Difference (ITD), while the difference in loudness is called the Interaural Level Difference (ILD). These cues are extremely subtle—at 90°, the maximum ITD is around 630 microseconds—yet the brain is remarkably adept at interpreting them to infer the lateral position of the source.\nThe ILD is largely caused by the acoustic shadowing effect of the head. For higher frequencies (with wavelengths shorter than the head’s diameter), the sound cannot diffract around the head, leading to more pronounced differences in intensity between the ears, depending on the angle of incidence.\nThese binaural cues work together most effectively between 800 Hz and 1600 Hz. Below 800 Hz, ITD is the dominant cue; above 1600 Hz, ILD becomes more effective. The distance between the listener and the sound source also affects these cues, with ILD being particularly sensitive to distance changes, while ITD remains relatively stable.\nBinaural localization does not require prior knowledge of the sound source. In contrast, monaural localization—when only one ear is used—often depends on recognizing the sound in advance. One key monaural cue is the Spectral Cue (also called the Monaural Spectral Cue), which arises from changes in the sound spectrum caused by interactions with the outer ear (the pinna).\nEven slight modifications in the signals reaching the auditory system can lead to significant shifts in the perceived spatial image. From an acoustic perspective, the pinna acts as a directional filter, primarily affecting high frequencies. It introduces spectral and temporal modifications to the signal depending on its angle of incidence and distance. These effects help the listener distinguish between sounds originating in front versus behind, and also assist in determining the elevation of the source.\nHistorically, the role of the pinnae in spatial hearing was underestimated, often seen merely as protective structures. Contemporary research, however, confirms their essential role in spatial perception and in reducing environmental noise, such as wind.\nWhen it comes to estimating source distance, familiarity with the sound plays a significant role. For example, spoken voice at a normal volume provides a relatively accurate cue for distance perception. Like light intensity or visual size, sound intensity decreases with distance, following the inverse square law. But intensity is not the only factor: air acts as a low-pass filter, attenuating high frequencies over distance. This effect is clearly noticeable in scenarios like an approaching airplane, where the sound not only grows louder but also becomes brighter and richer in frequency content.\nIn sum, sound localization is a complex perceptual process informed by a combination of binaural and monaural cues, spectral filtering by the outer ear, and familiarity with the sound source. These mechanisms interact continuously to allow listeners to orient themselves in a dynamic acoustic environment, whether real or simulated.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatial Audio</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#hearing-in-enclosed-spaces",
    "href": "chapters/space.html#hearing-in-enclosed-spaces",
    "title": "6  Spatial Audio",
    "section": "6.3 Hearing in Enclosed Spaces",
    "text": "6.3 Hearing in Enclosed Spaces\nWhen we experience sound in an enclosed environment, the first auditory cue that reaches our ears is known as the direct sound—the signal that travels straight from the source without interacting with any obstacles. This is followed by the first-order reflections, which occur when the sound bounces off a single surface—such as a wall—before reaching the listener. These are then succeeded by second-order reflections, involving two bounces, and progressively higher orders of reflection. Eventually, this cascade of reflections gives rise to a diffuse auditory sensation known as reverberation.\nIn a typical empty room—consider one with four walls, a ceiling, and a floor—the six primary surfaces contribute to the first-order reflections. These early reflections are particularly significant when the sound has a sharp or impulsive attack, as they assist the auditory system in identifying the spatial position of the sound source. Reverberation, by contrast, carries information about the acoustic properties and dimensions of the room. It plays a key role in shaping the overall perception of the space’s material and geometry.\nA critical metric used in room acoustics is the reverberation time, commonly denoted as T₆₀. This refers to the time it takes for the reverberant sound energy to decay by 60 decibels after the original sound source has ceased. A widely used formula for estimating this value is the Sabine equation: \\[T_{60} = 0.16 \\times \\frac{V}{A}\\]\nHere, V represents the volume of the room in cubic meters, and A is the total equivalent absorption area, calculated as the sum of the products of each surface’s area and its corresponding absorption coefficient. Importantly, absorption coefficients are frequency-dependent, meaning that the acoustic behavior of a room varies with different sound frequencies.\nTo account for this, acousticians often use a metric known as the Noise Reduction Coefficient (NRC), which averages absorption values across key frequencies—specifically 250 Hz, 500 Hz, 1000 Hz, and 2000 Hz. This provides a more practical estimate of how a room absorbs sound across the human auditory spectrum.\nA graphical representation of room acoustics typically illustrates the temporal distribution of reflections produced by an impulse response within the space. In such plots, the height of each line corresponds to the relative amplitude of each reflection. These visualizations help researchers and designers understand how sound energy evolves over time in an architectural space, offering insight into both clarity and spatial impression.\nUnderstanding how direct sound, early reflections, and reverberation interact is essential for both technical and artistic applications in sound design. Whether composing for multichannel electroacoustic environments or designing spatial sound installations, these acoustic principles are fundamental to crafting immersive and intelligible auditory experiences.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatial Audio</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#virtual-source-simulation-via-stereophonic-techniques",
    "href": "chapters/space.html#virtual-source-simulation-via-stereophonic-techniques",
    "title": "6  Spatial Audio",
    "section": "6.4 Virtual Source Simulation via Stereophonic Techniques",
    "text": "6.4 Virtual Source Simulation via Stereophonic Techniques\nStereophonic systems enable the simulation of virtual sound source localization using just two loudspeakers positioned to the left and right of the listener. By manipulating the amplitude of each signal sent to the speakers, we can generate the perceptual illusion of a sound source moving laterally across the auditory field. Additional transformations also allow us to simulate depth and even create the sensation of a source emanating from behind the speakers, constructing an illusory auditory space.\nThe perception of a virtual source positioned between two speakers arises from the system’s ability to artificially generate an Interaural Level Difference (ILD). If the signal emitted from the right speaker is louder than that from the left, the right ear perceives a higher intensity, prompting the auditory system to localize the source toward the right. The degree of perceived lateral displacement is directly proportional to the intensity difference.\nIn typical stereo setups, the speakers are arranged with a 60° separation. However, for purposes of spatial expansion and compatibility with quadraphonic systems (involving four speakers surrounding the listener), we assume a 90° separation. We define 0° as the location of the left speaker and 90° as the right. The objective is to calculate the appropriate amplitude for each speaker to position the virtual source at a desired angle θ within this range.\nTo ensure the virtual source appears equidistant from the listener during its movement, the total sound intensity emitted by both speakers must remain constant. That is:\n\\[\nI_L + I_R = k\n\\]\nAssuming intensity is normalized between 0 and 1, we can simplify this to:\n\\[\nI_L + I_R = 1\n\\]\nHere, \\(I_L\\) and \\(I_R\\) represent the intensities emitted by the left and right speakers, respectively. When the source is fully localized at the left, \\(I_L = 1\\) and \\(I_R = 0\\); when at the right, \\(I_L = 0\\) and \\(I_R = 1\\); and when centered, both are \\(0.5\\). However, in most digital audio systems we operate on amplitude, not intensity. Since intensity is proportional to the square of amplitude, we use the relation:\n\\[\nI \\propto A^2\n\\]\nThus, the condition of constant intensity becomes:\n\\[\nA_L^2 + A_R^2 = 1\n\\]\nA common misconception is to linearly scale amplitudes (e.g., \\(A_L = A_R = 0.5\\) when centered), but this would yield:\n\\[\n0.5^2 + 0.5^2 = 0.25 + 0.25 = 0.5\n\\]\n—only half the intended total intensity, which could be mistakenly perceived as an increase in distance. To preserve consistent intensity across the stereo field, we rely on a trigonometric identity:\n\\[\n\\sin^2(\\theta) + \\cos^2(\\theta) = 1\n\\]\nUsing this, we can define the amplitude assignments as:\n\\[\nA_R = \\sin(\\theta) \\\\\nA_L = \\cos(\\theta)\n\\]\nWhere θ ranges from 0° (left) to 90° (right). For example, at 45°, both \\(sin(45^\\circ)\\) and \\(cos(45^\\circ)\\) equal approximately 0.707. Squaring and summing these values confirms that the total intensity remains 1:\n\\[\nA_R^2 + A_L^2 = \\sin^2(45^\\circ) + \\cos^2(45^\\circ) = 0.5 + 0.5 = 1\n\\]\nThus, this method preserves the perceptual coherence of the virtual sound source’s position while maintaining a constant energy output, a critical aspect of spatial audio realism.\nIn summary, for any virtual angle \\(\\theta \\in [0^\\circ, 90^\\circ]\\), we can accurately simulate spatial position with consistent intensity using:\n\\[\nA_R = \\sin(\\theta) \\\\\nA_L = \\cos(\\theta)\n\\]\nThis technique forms the basis for further expansion into quadraphonic and multichannel spatialization systems.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatial Audio</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#implementing-a-stereo-spatialization-in-pd",
    "href": "chapters/space.html#implementing-a-stereo-spatialization-in-pd",
    "title": "6  Spatial Audio",
    "section": "6.5 Implementing a Stereo Spatialization in Pd",
    "text": "6.5 Implementing a Stereo Spatialization in Pd\nBefore diving into the implementation of a stereophonic spatialization system in Pd, it is essential to note that the objects used to compute trigonometric functions such as sine and cosine require the input angle to be expressed in radians, not degrees. Recall that one radian is the angle formed when the length of the arc of a circle equals the radius. From this definition, we derive the following equivalences:\n\n360° = \\(2\\pi\\) radians\n\n180° = \\(\\pi\\) radians\n\n90° = \\(\\frac{\\pi}{2}\\) radians\n\nTo convert degrees to radians, we multiply the angle in degrees by the ratio (\\(\\frac{2\\pi}{360}\\)). To optimize performance and avoid redundant calculations, we can precompute this factor:\n\\[\n\\text{Angle}_{\\text{rad}} = \\text{Angle}_{\\text{deg}} \\times \\frac{2\\pi}{360} = \\text{Angle}_{\\text{deg}} \\times 0.0174533\n\\]\n\n\n\nIntensity panning implementation with angular control and trigonometric amplitude distribution for stereophonic sound source positioning.\n\n\nThe patch illustrated below simulates a virtual sound source using stereophonic panning. This technique, commonly referred to as intensity panning, adjusts the balance between two loudspeakers. The source signa is split into two channels, each multiplied by the cosine and sine of the specified angle (in radians) to distribute the amplitude between the left and right speakers. As we vary the angle between 0° and 90°, the source appears to move along an arc, emulating the listener’s perception of spatial position.\nThis Pd patch shows the fundamental principles of stereophonic spatialization through intensity panning, implementing the mathematical framework for creating virtual sound source positioning between two speakers. By applying trigonometric amplitude control to left and right audio channels, the patch enables precise localization of monophonic sources within a 90-degree stereo field while maintaining constant total energy output. This approach forms the foundation for understanding spatial audio processing and serves as a building block for more complex multichannel spatialization systems.\nThe patch provides real-time control over angular positioning through intuitive slider interfaces, allowing users to experience the direct relationship between mathematical spatial calculations and perceptual sound localization. This implementation serves both educational and practical purposes, demonstrating how theoretical acoustic principles translate into functional audio processing tools.\n\n6.5.1 Patch Overview\nThe intensity panning system implements a complete stereophonic spatialization pipeline consisting of four primary processing stages:\n\nAngle Input and Conversion: Manages user input and degree-to-radian conversion\nTrigonometric Calculation: Computes sine and cosine values for amplitude control\nAudio Generation and Processing: Creates test signals and applies spatial positioning\nStereo Output: Delivers positioned audio to left and right channels\n\nThe patch demonstrates the essential mathematical relationship where spatial positioning is achieved through complementary amplitude control, ensuring that the virtual source maintains consistent perceived loudness while moving between speakers.\n\n6.5.1.1 Critical Processing Components\nDegree-to-Radian Conversion: The multiplication factor 0.0174533 represents the mathematical constant π/180, enabling conversion from user-friendly degree input to the radian values required by Pd’s trigonometric functions. This conversion ensures precise mathematical calculation while maintaining intuitive user control.\nTrigonometric Expression: The expr cos($f1) \\; sin($f1) object simultaneously calculates both amplitude coefficients needed for stereophonic positioning. The cosine output controls left channel amplitude, while sine controls right channel amplitude, creating the complementary relationship essential for constant-energy panning.\nAudio Expression Multiplier: The expr~ $v1*$v2 \\; $v1*$v3 object performs real-time multiplication of the audio signal with the calculated trigonometric values, applying the spatial positioning in the audio domain while maintaining sample-accurate timing.\n\n\n\n6.5.2 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Angle Input 0-90°] --&gt; B[Degree to Radian Conversion]\n    B --&gt; C[Trigonometric Calculation]\n    C --&gt; D[Sine/Cosine Values]\n    D --&gt; E[Audio Signal Generation]\n    E --&gt; F[Amplitude Modulation]\n    F --&gt; G[Left Channel Output]\n    F --&gt; H[Right Channel Output]\n    I[Test Oscillator] --&gt; E\n    \n    style A fill:#e1f5fe\n    style G fill:#f3e5f5\n    style H fill:#f3e5f5\n    style C fill:#fff3e0\n\n\n\n\n\n\nThe stereophonic intensity panning patch orchestrates a mathematical transformation pipeline that converts angular positioning information into spatially positioned audio output through systematic trigonometric processing. This implementation demonstrates how theoretical spatial audio principles translate into practical real-time audio processing systems that maintain both mathematical accuracy and perceptual coherence.\nThe angle input and preprocessing stage establishes the foundation for spatial positioning through user interface management and mathematical preparation. The horizontal slider (hsl) provides intuitive control over the desired angular position, accepting input values in the familiar degree scale from 0° to 90°, where 0° represents full left positioning and 90° represents full right positioning. This degree-based input undergoes immediate conversion to radians through multiplication by the constant 0.0174533 (π/180), transforming the user-friendly angular measurement into the mathematical format required by Pd’s trigonometric functions. The conversion process ensures that subsequent calculations maintain precision while preserving the intuitive relationship between slider position and spatial location. The radian values flow directly to the trigonometric processing stage, maintaining real-time responsiveness that enables immediate auditory feedback as users adjust the angular positioning control.\nThe trigonometric calculation stage represents the mathematical core of the spatialization process, where angular information becomes amplitude control data through sophisticated mathematical processing. The expr cos($f1) \\; sin($f1) object performs simultaneous calculation of both cosine and sine functions for the input angle, generating the complementary amplitude coefficients essential for stereophonic positioning. The cosine output provides the left channel amplitude coefficient, following the mathematical convention where cos(0°) = 1 and cos(90°) = 0, creating maximum left channel output when the angle is at 0° and minimum output at 90°. Conversely, the sine output generates the right channel amplitude coefficient, where sin(0°) = 0 and sin(90°) = 1, producing the inverse relationship necessary for right channel control. This mathematical relationship ensures that the fundamental trigonometric identity sin²(θ) + cos²(θ) = 1 is preserved, maintaining constant total energy across the stereo field regardless of angular position.\nThe audio signal and modulation stage transforms the calculated amplitude coefficients into actual spatial positioning through real-time audio processing. The phasor~ 220 object generates a continuous sawtooth wave at 220 Hz, providing a harmonically rich test signal that clearly demonstrates the spatial positioning effects across the frequency spectrum. This test signal enters the expr~ $v1*$v2 \\; $v1*$v3 audio expression object, where it undergoes simultaneous multiplication with both trigonometric amplitude coefficients. The first output (\\(v1*\\)v2) produces the left channel audio by multiplying the source signal with the cosine amplitude value, while the second output (\\(v1*\\)v3) creates the right channel audio through multiplication with the sine amplitude value. This dual multiplication process applies the spatial positioning in real-time while maintaining sample-accurate synchronization between channels, ensuring that the stereo image remains stable and coherent.\nThe feedback and output stage provides both visual monitoring and audio delivery systems that enable users to understand and experience the spatialization process. The vertical sliders (vsl) connected to the trigonometric outputs provide real-time visual feedback of the calculated amplitude values, allowing users to observe how the complementary relationship between left and right coefficients changes as the angle varies. This visual feedback reinforces the mathematical principles underlying the spatialization process and helps users understand the relationship between angular input and amplitude distribution. The output~ object delivers the final spatialized audio to the system’s audio interface, enabling immediate auditory perception of the spatial positioning effects. The stereo output maintains the precise amplitude relationships calculated by the trigonometric functions, ensuring that the perceived spatial position corresponds accurately to the input angle while preserving the constant energy principle essential for realistic spatial audio.\n\n\n\n\n\nflowchart LR\n    A[Angle Input] --&gt; B[Radian Conversion]\n    B --&gt; C[Trigonometric Calculation]\n    C --&gt; D[Audio Modulation]\n    D --&gt; E[Stereo Output]\n\n\n\n\n\n\n\n\n6.5.3 Processing Chain Details\nThe spatialization system maintains mathematical precision through several coordinated processing stages:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nAngle (0-90°)\nDegree to radian conversion\nRadian values\n\n\n2\nRadian angle\nTrigonometric calculation\nCosine and sine coefficients\n\n\n3\nTest audio + coefficients\nReal-time multiplication\nLeft and right audio channels\n\n\n4\nStereo channels\nAudio interface output\nSpatialized audio\n\n\n\n\n6.5.3.1 Mathematical Relationships\nThe patch implements the fundamental stereophonic panning equations:\n\n\n\n\n\n\n\n\nParameter\nMathematical Function\nAudio Application\n\n\n\n\nLeft Amplitude\n\\(A_L = \\cos(\\theta)\\)\nMultiply source signal by cosine value\n\n\nRight Amplitude\n\\(A_R = \\sin(\\theta)\\)\nMultiply source signal by sine value\n\n\nEnergy Conservation\n\\(A_L^2 + A_R^2 = 1\\)\nMaintains constant perceived loudness\n\n\nAngular Range\n\\(\\theta \\in [0°, 90°]\\)\nMaps to full left-right stereo field\n\n\n\n\n\n6.5.3.2 Control Parameter Mapping\n\n\n\n\n\n\n\n\n\n\n\nSlider Position\nAngle (Degrees)\nAngle (Radians)\nLeft Amplitude\nRight Amplitude\nPerceived Position\n\n\n\n\n0\n0°\n0.000\n1.000\n0.000\nFull Left\n\n\n0.25\n22.5°\n0.393\n0.924\n0.383\nLeft-Center\n\n\n0.5\n45°\n0.785\n0.707\n0.707\nCenter\n\n\n0.75\n67.5°\n1.178\n0.383\n0.924\nRight-Center\n\n\n1.0\n90°\n1.571\n0.000\n1.000\nFull Right\n\n\n\n\n\n6.5.3.3 Signal Processing Verification\nThe patch maintains signal integrity through:\n\nConstant Energy Preservation: Total amplitude squared remains unity across all positions\nPhase Coherence: Both channels maintain identical phase relationships\nAmplitude Linearity: Trigonometric functions provide smooth transitions between positions\nReal-time Responsiveness: Sample-accurate processing without latency or artifacts\n\n\n\n\n6.5.4 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Spatialization\n\n\n\n\n* 0.0174533\nConversion multiplier\nConverts degrees to radians for trigonometric functions\n\n\nexpr cos($f1) \\; sin($f1)\nTrigonometric processor\nCalculates complementary amplitude values\n\n\nphasor~ 220\nAudio oscillator\nGenerates test signal for spatialization demonstration\n\n\nexpr~ $v1*$v2 \\; $v1*$v3\nAudio multiplier\nApplies trigonometric amplitude control to audio\n\n\noutput~\nAudio output\nDelivers spatialized stereo audio\n\n\n\n\n\n6.5.5 Creative Applications\n\nInteractive Spatial Composition: Use MIDI controllers or OSC input to automate the angle parameter, creating dynamic spatial movement within musical compositions.\nBinaural Audio Production: Combine with headphone processing to create immersive spatial experiences for stereo headphone listeners.\nMulti-source Spatialization: Duplicate the patch architecture to control multiple independent sound sources within the same stereo field.\nSpatial Modulation Effects: Apply low-frequency oscillators (LFOs) to the angle parameter to create tremolo-like spatial movement effects.\nPerformance Control Systems: Integrate with physical controllers, sensors, or computer vision systems to map performer gestures to spatial positioning.\nEducational Demonstrations: Use as a teaching tool to illustrate psychoacoustic principles, trigonometric relationships, and spatial audio mathematics.\nSound Design Applications: Create spatial effects for film, game audio, or immersive media by precisely controlling source positioning.\nInstallation Art: Implement in gallery spaces where visitor proximity or movement controls the spatial positioning of ambient sounds.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatial Audio</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#simulating-distance",
    "href": "chapters/space.html#simulating-distance",
    "title": "6  Spatial Audio",
    "section": "6.6 Simulating Distance",
    "text": "6.6 Simulating Distance\nTo simulate distance, we must account for how amplitude diminishes as the source moves away from the listener. Unlike intensity, which decreases with the square of the distance, amplitude is inversely proportional to the distance:\n\\[\nA \\propto \\frac{1}{d}\n\\]\nThus, the amplitude perceived by the listener is:\n\\[\nA_{\\text{perceived}} = \\frac{A_{\\text{source}}}{d}\n\\]\nTo manage both the angle of lateralization and the distance to the source simultaneously, we use the slider2d object from the ELSE library. This object provides a two-dimensional matrix that captures mouse pointer movement and returns the corresponding (x, y) coordinates.\n\n\n\nDistance panning where increasing distance reduces amplitude signal\n\n\nWe map the horizontal axis (x) to the angle (0°–90°) and the vertical axis (y) to the distance (1-10). These values are then routed via [send] and [receive] objects for use throughout the patch. It’s crucial to prevent division by zero in distance calculations, so we define the minimum distance as 1—equivalent to the baseline distance between the listener and speakers. If speakers are placed 2 meters away, then a distance of \\(d = 1\\) equals 2 meters in real space, \\(d = 2\\) would correspond to 4 meters, and so on. In this system, distance is relative to the speaker-listener spacing.\nThis Pd patch extends the fundamental stereophonic panning system by introducing interactive two-dimensional spatial control through a visual interface. By combining angular positioning with distance modeling, the patch creates a more sophisticated spatialization system that simulates both the horizontal positioning and depth perception of virtual sound sources. The implementation demonstrates how distance affects amplitude through inverse-square law approximations while maintaining the trigonometric relationships essential for accurate stereo positioning.\nThe patch integrates real-time visual control through a 2D slider interface that enables intuitive manipulation of both angular position and source distance simultaneously. This approach provides immediate tactile feedback for spatial audio composition while implementing realistic amplitude scaling based on distance relationships that mirror natural acoustic behavior.\n\n6.6.1 Patch Overview\nThe interactive distance panning system implements a comprehensive spatial audio control interface consisting of five primary processing stages:\n\nTwo-Dimensional Input Control: Visual interface for simultaneous angle and distance manipulation\nCoordinate Conversion: Cartesian to polar coordinate transformation with parameter scaling\nDistance Modeling: Amplitude scaling based on acoustic distance relationships\nAngular Processing: Trigonometric calculation for stereophonic positioning\nAudio Synthesis and Output: Signal generation with spatial and distance-based amplitude control\n\nThe patch demonstrates advanced Pd interface design by combining multiple control paradigms within a single cohesive system that maintains both mathematical precision and intuitive user interaction.\n\n6.6.1.1 Critical Processing Components\nslider2d Interface: This object provides the primary user interaction mechanism, outputting normalized x,y coordinates in the range 0.0-1.0. The horizontal axis (x) controls angular positioning while the vertical axis (y) determines distance scaling, creating an intuitive two-dimensional spatial control surface.\nCoordinate Transformation: The unpack f f object separates the 2D coordinates, enabling independent processing of angular and distance parameters. The x-coordinate undergoes multiplication by 90 to generate the 0-90° angular range required for stereophonic panning.\nDistance Amplitude Expression: The expr~ $v1 / (($v2*9)+1) object implements a distance-based amplitude scaling algorithm that approximates acoustic distance attenuation. The multiplication by 9 and addition of 1 creates a distance range from 1 to 10, preventing division by zero while providing meaningful amplitude variation.\n\n\n\n6.6.2 Data Flow\n\n\n\n\n\nflowchart TD\n    A[2D Slider Interface] --&gt; B[Coordinate Unpacking]\n    B --&gt; C[Angle Calculation 0-90°]\n    B --&gt; D[Distance Extraction 1-10]\n    C --&gt; E[Radian Conversion]\n    E --&gt; F[Trigonometric Processing]\n    D --&gt; G[Distance Amplitude Scaling]\n    F --&gt; H[Spatial Audio Mixing]\n    G --&gt; H\n    I[Test Oscillator] --&gt; J[Distance Division]\n    J --&gt; H\n    H --&gt; K[Stereo Output]\n    \n    style A fill:#e1f5fe\n    style K fill:#f3e5f5\n    style F fill:#fff3e0\n    style G fill:#fff3e0\n\n\n\n\n\n\nThe interactive distance panning patch implements a multi-dimensional control system that transforms two-dimensional interface input into spatially accurate audio output through coordinated processing of angular positioning and distance modeling. This implementation demonstrates how visual interface design can be integrated with acoustic modeling to create intuitive spatial audio control systems that maintain both mathematical precision and perceptual accuracy.\nThe initial input coordination and parameter extraction stage establishes the foundation for spatial control through the comprehensive management of the two-dimensional slider interface. The slider2d object captures user interaction as normalized x,y coordinate pairs ranging from 0.0 to 1.0, where the horizontal axis represents angular positioning information and the vertical axis encodes distance relationship data. Upon initialization, the loadbang and range 0 1 message ensure proper scaling configuration, establishing the coordinate system boundaries that will govern subsequent parameter mapping. The coordinate unpacking process through unpack f f separates the combined spatial input into independent data streams that can be processed according to their specific roles in the spatialization algorithm. This separation enables parallel processing of angular and distance information while maintaining the temporal relationship between user input and system response that is essential for real-time spatial audio manipulation.\nThe angular conversion and trigonometric processing stage transforms the horizontal coordinate information into the angular parameters required for stereophonic positioning calculations. The x-coordinate from the 2D slider undergoes scaling multiplication by 90, converting the normalized 0.0-1.0 range into the 0-90° angular span that defines the complete left-right stereo field. This angular value proceeds through the established degree-to-radian conversion process via multiplication by 0.0174533, preparing the data for the trigonometric calculations performed by the expr cos($f1) \\; sin($f1) object. The trigonometric processing generates the complementary amplitude coefficients essential for stereophonic positioning, where the cosine output controls left channel amplitude and the sine output controls right channel amplitude. This processing stage maintains the mathematical relationships established in previous spatial audio implementations while adapting to the dynamic input provided by the interactive interface system.\nThe distance modeling and amplitude scaling stage implements acoustic distance relationships through sophisticated amplitude manipulation that simulates realistic sound propagation characteristics. The y-coordinate from the 2D slider interface undergoes processing through the send/receive system (s distance / r distance) that distributes the distance parameter to multiple processing locations within the patch. The distance value enters the expr~ $v1 / (($v2*9)+1) expression, which implements an approximation of inverse-square law amplitude scaling where increasing distance values produce proportionally decreased amplitude output. The mathematical relationship ($v2*9)+1 creates a distance scaling factor that ranges from 1 (minimum distance, maximum amplitude) to 10 (maximum distance, minimum amplitude), preventing division by zero while providing meaningful amplitude variation across the available control range. This distance-based amplitude scaling applies to the test oscillator signal before it undergoes spatial positioning through the trigonometric multiplication stage.\nThe audio synthesis and spatial integration stage combines the distance-scaled audio signal with the angular positioning coefficients to create the final spatialized output with realistic distance characteristics. The phasor~ 220 object generates a harmonically rich test signal that undergoes distance-based amplitude scaling through the division expression before entering the spatial processing chain. The expr~ $v1*$v2 \\; $v1*$v3 object performs the critical multiplication that applies both the distance attenuation and the trigonometric spatial positioning simultaneously, creating stereo output channels that reflect both the angular position and distance of the virtual sound source. The left channel output results from multiplication with the cosine amplitude coefficient modified by distance scaling, while the right channel output employs the sine amplitude coefficient similarly modified by distance attenuation. This dual processing approach ensures that the final audio output maintains both the spatial positioning accuracy established by the trigonometric relationships and the amplitude scaling appropriate for the simulated distance.\nThe visual feedback and system monitoring stage provides real-time indication of the calculated spatial parameters through integrated display elements that enable users to understand the relationship between interface input and audio processing parameters. The numerical display objects connected to various processing stages show the current angular values in both degrees and radians, the extracted distance parameter, and the calculated amplitude coefficients for both stereo channels. This visual feedback system enables users to develop understanding of the mathematical relationships underlying the spatial audio processing while providing confirmation that the interface input is correctly translated into audio processing parameters. The continuous visual update of these parameters during real-time interaction creates an educational environment where users can observe the direct correspondence between spatial positioning concepts and their implementation in digital audio processing systems.\n\n\n\n\n\nflowchart LR\n    A[2D Interface Input] --&gt; B[Coordinate Processing]\n    B --&gt; C[Angular Conversion]\n    B --&gt; D[Distance Scaling]\n    C --&gt; E[Trigonometric Calculation]\n    D --&gt; F[Amplitude Attenuation]\n    E --&gt; G[Spatial Audio Output]\n    F --&gt; G\n\n\n\n\n\n\n\n\n6.6.3 Processing Chain Details\nThe distance panning system maintains mathematical precision through several coordinated processing stages that integrate spatial positioning with acoustic distance modeling:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\n2D Coordinates (0-1)\nInterface capture and unpacking\nSeparated x,y values\n\n\n2\nX-coordinate\nMultiplication by 90\nAngular degrees (0-90°)\n\n\n3\nY-coordinate\nDistance parameter extraction\nDistance factor (0-1)\n\n\n4\nAngular degrees\nRadian conversion and trigonometry\nCosine/sine coefficients\n\n\n5\nDistance factor\nAmplitude scaling calculation\nDistance attenuation\n\n\n6\nAudio + coefficients + distance\nCombined spatial and distance processing\nSpatialized stereo output\n\n\n\n\n6.6.3.1 Mathematical Relationships\nThe patch implements integrated spatial and distance modeling through coordinated mathematical relationships:\n\n\n\n\n\n\n\n\nParameter\nMathematical Function\nAudio Application\n\n\n\n\nAngular Position\n\\(\\theta = x \\times 90°\\)\nMaps horizontal slider to stereo field\n\n\nLeft Amplitude\n\\(A_L = \\cos(\\theta)\\)\nSpatial positioning coefficient\n\n\nRight Amplitude\n\\(A_R = \\sin(\\theta)\\)\nSpatial positioning coefficient\n\n\nDistance Scaling\n\\(A_d = \\frac{1}{(d \\times 9) + 1}\\)\nInverse distance attenuation\n\n\nFinal Left Channel\n\\(L = \\text{signal} \\times A_d \\times A_L\\)\nCombined distance and spatial processing\n\n\nFinal Right Channel\n\\(R = \\text{signal} \\times A_d \\times A_R\\)\nCombined distance and spatial processing\n\n\n\n\n\n6.6.3.2 Control Parameter Mapping\n\n\n\n\n\n\n\n\n\n\n\nInterface Position\nAngle (Degrees)\nDistance Factor\nLeft Amplitude\nRight Amplitude\nDistance Attenuation\n\n\n\n\n(0.0, 0.0)\n0°\n1 (close)\n1.000\n0.000\n1.000\n\n\n(0.5, 0.0)\n45°\n1 (close)\n0.707\n0.707\n1.000\n\n\n(1.0, 0.0)\n90°\n1 (close)\n0.000\n1.000\n1.000\n\n\n(0.0, 1.0)\n0°\n10 (far)\n1.000\n0.000\n0.100\n\n\n(0.5, 1.0)\n45°\n10 (far)\n0.707\n0.707\n0.100\n\n\n(1.0, 1.0)\n90°\n10 (far)\n0.000\n1.000\n0.100\n\n\n\n\n\n6.6.3.3 Distance Modeling Considerations\nAcoustic Accuracy: The distance scaling approximates natural sound propagation where amplitude decreases with increasing distance, though the specific mathematical relationship can be adjusted for different acoustic environments or artistic preferences.\nControl Resolution: The 1-10 distance range provides meaningful amplitude variation while maintaining sufficient resolution for precise spatial positioning in musical and installation contexts.\nInterface Mapping: The vertical axis controlling distance creates intuitive correspondence where upward movement represents increased distance and reduced amplitude, matching visual expectations of spatial relationships.\n\n\n\n6.6.4 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Spatial Control\n\n\n\n\nslider2d\nTwo-dimensional interface\nProvides x,y coordinate input for angle and distance\n\n\nunpack f f\nCoordinate separation\nSplits 2D coordinates into independent x,y values\n\n\n* 90\nAngular scaling\nConverts normalized x-coordinate to 0-90° range\n\n\ns distance / r distance\nDistance communication\nDistributes distance parameter across patch\n\n\nexpr~ $v1 / (($v2*9)+1)\nDistance amplitude modeling\nApplies inverse relationship for distance attenuation\n\n\nexpr cos($f1) \\; sin($f1)\nTrigonometric processor\nMaintains stereophonic positioning calculations\n\n\nexpr~ $v1*$v2 \\; $v1*$v3\nSpatial audio multiplier\nApplies both distance and angular amplitude control\n\n\n\n\n\n6.6.5 Creative Applications\n\nInteractive Spatial Composition: Use tablet interfaces or touch controllers to manipulate multiple sound sources simultaneously within a virtual acoustic space with realistic distance modeling.\nArchitectural Space Simulation: Model specific room acoustics by adjusting the distance scaling factors to match measured reverberation and absorption characteristics of real spaces.\nImmersive Audio Storytelling: Create dynamic soundscapes for narrative applications where sound source proximity affects listener emotional engagement and attention focus.\nPerformance Interface Design: Develop custom controllers that map performer gestures to spatial positioning, enabling expressive control over both angle and distance through physical movement.\nCollaborative Music Systems: Enable multiple performers to control independent sound sources within a shared spatial audio environment.\nInstallation Art with Proximity Sensing: Connect distance parameters to actual physical proximity sensors that adjust audio based on visitor location within gallery spaces\nGaming Audio Implementation: Provide realistic spatial audio feedback for game environments where player-object relationships affect both direction and distance perception",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatial Audio</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#enhanced-distance-modeling-with-air-absorption-and-reverberation",
    "href": "chapters/space.html#enhanced-distance-modeling-with-air-absorption-and-reverberation",
    "title": "6  Spatial Audio",
    "section": "6.7 Enhanced Distance Modeling with Air Absorption and Reverberation",
    "text": "6.7 Enhanced Distance Modeling with Air Absorption and Reverberation\nTo further enhance the realism of our spatialization, we simulate air absorption, which—although minimal at short distances—contributes to the perceptual illusion of depth. As distance increases, high frequencies are attenuated, which we simulate using a low-pass filter object[lop~].\n\n\n\nEnhanced distance modeling with frequency-dependent air absorption through low-pass filtering\n\n\nWe control the cutoff frequency of this filter using a [expr] object ([expr (1 - $f1) * 7000 + 1000]). For example, we can scale a distance range of 1–10 to a cutoff frequency range of 8000–1000 Hz. As the source moves farther away, the filter reduces the brightness of the sound, mimicking atmospheric filtering.\nConsider a large environment with significant reverberation—such as a church or garage. When someone speaks nearby, the direct sound dominates, and reverberation is minimal. Conversely, at greater distances, the direct signal weakens while reverberation remains relatively constant. This is due to the fact that reverberation is a function of the environment and not as sensitive to the source’s location.\n\n\n\nEnhanced distance modeling combining amplitude attenuation, low-pass filtering, and dynamic reverberation balance.\n\n\nTo simulate this phenomenon, we introduce a reverberation chamber using [freeverb~]. The reverb level is controlled via a rotary dial ([knob] from the ELSE library). By configuring the knob for a low initial setting, we simulate the acoustic behavior where reverberation becomes more prominent as the source moves farther from the listener. As the source approaches, the increased amplitude of the direct signal masks the reverberation, completing our auditory illusion.\nThe patch extends the interactive distance panning system by incorporating realistic acoustic phenomena that occur in natural environments. The implementation adds frequency-dependent air absorption through low-pass filtering and environmental reverberation modeling, creating a comprehensive spatial audio simulation that mirrors how sound behaves across varying distances in real acoustic spaces. This enhanced system demonstrates how multiple acoustic cues work together to create convincing depth perception and environmental context.\nThe design integrates three primary acoustic distance cues: amplitude attenuation based on inverse distance relationships, high-frequency absorption that simulates atmospheric filtering, and reverberation balance that reflects how direct and reflected sound components change with source proximity. This multi-parameter approach creates significantly more realistic distance simulation than amplitude scaling alone.\n\n6.7.1 Patch Overview\nThe enhanced distance modeling system implements a comprehensive acoustic simulation pipeline consisting of six integrated processing stages:\n\nTwo-Dimensional Interface Control: Visual manipulation of angle and distance parameters\nDistance-Based Amplitude Scaling: Inverse relationship modeling for realistic volume attenuation\nFrequency-Dependent Air Absorption: Low-pass filtering that simulates atmospheric effects\nEnvironmental Reverberation Processing: Spatial acoustic context through algorithmic reverb\nDirect/Reverb Signal Balancing: Dynamic mixing based on distance relationships\nSpatial Audio Output: Final stereo positioning with enhanced depth cues\n\nThe system demonstrates how multiple acoustic phenomena combine to create convincing spatial audio environments that extend beyond simple amplitude panning.\n\n6.7.1.1 Critical Enhancement Components\nAir Absorption Modeling: The lop~ object implements frequency-dependent attenuation that simulates how air absorption affects sound transmission over distance. The cutoff frequency calculation (1 - $f1) * 7000 + 1000 creates an inverse relationship where increased distance produces lower cutoff frequencies, mimicking the natural tendency for high frequencies to attenuate more rapidly through atmospheric transmission.\nReverberation Processing: The freeverb~ object provides environmental acoustic context through algorithmic reverberation that simulates room acoustics. The reverb parameters (dry 0, wet 1, roomsize 0.95) create a large virtual space with minimal direct signal bleed-through.\nDynamic Signal Routing: The send/receive system (s~ dry, r~ dry) enables parallel processing where the same audio source feeds both the direct signal path and the reverberation processing chain, allowing independent control of each component.\n\n\n\n6.7.2 Data Flow\n\n\n\n\n\nflowchart TD\n    A[2D Slider Interface] --&gt; B[Distance Extraction]\n    B --&gt; C[Amplitude Scaling]\n    B --&gt; D[Air Absorption Filtering]\n    B --&gt; E[Reverb Level Control]\n    F[Audio Source] --&gt; C\n    C --&gt; G[Low-pass Filter]\n    D --&gt; G\n    G --&gt; H[Dry Signal Path]\n    G --&gt; I[Reverb Processing]\n    E --&gt; I\n    H --&gt; J[Spatial Mixing]\n    I --&gt; J\n    J --&gt; K[Enhanced Stereo Output]\n    \n    style A fill:#e1f5fe\n    style K fill:#f3e5f5\n    style G fill:#fff3e0\n    style I fill:#fff3e0\n\n\n\n\n\n\nThe enhanced distance modeling patch orchestrates a sophisticated multi-stage acoustic simulation that combines amplitude, frequency, and temporal cues to create realistic spatial audio experiences. This implementation demonstrates how multiple acoustic phenomena interact in natural environments and how these relationships can be modeled systematically to achieve convincing distance perception.\nThe enhanced distance parameter extraction stage builds upon the basic distance panning system by distributing the distance control signal to multiple processing destinations simultaneously. The y-coordinate from the 2D slider interface continues to provide the primary distance measurement, but this value now feeds three distinct processing pathways rather than a single amplitude control. The distance parameter reaches the amplitude scaling system for volume attenuation, the air absorption calculation system for frequency filtering, and the reverberation balance system for environmental processing. This parallel distribution ensures that all acoustic distance cues remain synchronized and proportional, maintaining the physical relationships that exist in natural acoustic environments.\nThe frequency-dependent air absorption processing stage introduces realistic high-frequency attenuation that simulates atmospheric filtering effects. The distance value undergoes transformation through the expression (1 - $f1) * 7000 + 1000, which creates an inverse relationship mapping distance to low-pass filter cutoff frequency. When the source appears close (distance near 0), the calculation yields a cutoff frequency approaching 8000 Hz, allowing the full frequency spectrum to pass through unaffected. As distance increases toward maximum (distance near 1), the cutoff frequency approaches 1000 Hz, creating significant high-frequency attenuation that simulates the natural absorption characteristics of air transmission. The lop~ object implements this filtering in real-time, processing the distance-scaled audio signal to create the spectral changes associated with increasing source distance. This filtering effect proves particularly noticeable with broad-spectrum audio sources, where the gradual loss of high-frequency content creates immediate perceptual cues about source proximity.\nThe environmental reverberation processing stage adds spatial acoustic context through sophisticated algorithmic processing that simulates how direct and reflected sound components change with source distance. The audio signal enters the freeverb~ object configured with parameters that create a large virtual acoustic space: dry level set to 0 eliminates direct signal bleed-through, wet level at 1 provides full reverberation output, and room size at 0.95 creates an expansive virtual environment. The reverberation level control through the knob interface enables real-time adjustment of the environmental contribution, simulating how reverberation prominence changes with source distance. In natural acoustic environments, nearby sources produce strong direct signals that mask reverberation, while distant sources generate weaker direct signals that allow reverberation to become more prominent in the overall acoustic balance.\nThe signal mixing and spatial integration stage combines the processed direct and reverberated audio components while maintaining the established spatial positioning relationships. The direct signal path carries the distance-attenuated and air-absorption-filtered audio through the send/receive system (s~ dry, r~ dry) to the spatial mixing stage, while the reverberation output provides environmental context that adds spatial depth and acoustic realism. The final spatial audio expression expr~ $v1*$v2 \\; $v3*$v4 applies the trigonometric amplitude coefficients to both the direct and environmental signal components, ensuring that the spatial positioning affects all aspects of the audio output. This integrated approach creates stereo output where both the direct sound and its environmental reflections maintain proper spatial relationships, contributing to convincing three-dimensional audio positioning that extends beyond simple left-right panning.\n\n\n\n\n\nflowchart LR\n    A[Distance Control] --&gt; B[Multi-Parameter Processing]\n    B --&gt; C[Enhanced Audio Output]\n\n\n\n\n\n\n\n\n6.7.3 Processing Chain Details\nThe enhanced distance modeling system integrates multiple acoustic simulation stages:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\n2D Interface\nDistance parameter extraction\nControl signals for multiple destinations\n\n\n2\nDistance Value\nAmplitude scaling calculation\nVolume attenuation\n\n\n3\nDistance Value\nAir absorption frequency mapping\nLow-pass filter cutoff\n\n\n4\nFiltered Audio\nEnvironmental reverberation\nSpatial acoustic context\n\n\n5\nDistance Value\nReverb balance calculation\nDirect/reverb mixing ratio\n\n\n6\nMultiple Audio Streams\nSpatial positioning integration\nEnhanced stereo output\n\n\n\n\n6.7.3.1 Acoustic Parameter Relationships\n\n\n\n\n\n\n\n\n\n\nDistance Setting\nAmplitude\nCutoff Frequency\nReverb Prominence\nPerceived Effect\n\n\n\n\n0.0 (Close)\n1.000\n8000 Hz\nLow\nBright, direct, intimate\n\n\n0.25\n0.692\n6250 Hz\nLow-Medium\nSlightly muffled, present\n\n\n0.5\n0.444\n4500 Hz\nMedium\nNoticeably filtered, balanced\n\n\n0.75\n0.308\n2750 Hz\nMedium-High\nDistant, reverberant\n\n\n1.0 (Far)\n0.200\n1000 Hz\nHigh\nVery distant, heavily filtered\n\n\n\n\n\n6.7.3.2 Air Absorption Simulation\nThe frequency mapping implements realistic atmospheric absorption characteristics:\n\nNear field (0-0.3): Minimal high-frequency loss, preserving audio brightness\nMid field (0.3-0.7): Progressive filtering that maintains intelligibility\nFar field (0.7-1.0): Significant high-frequency attenuation creating distant character\n\n\n\n\n6.7.4 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Enhanced Distance Modeling\n\n\n\n\nslider2d\nTwo-dimensional interface\nAngle and distance parameter input\n\n\nexpr~ $v1 / (($v2*9)+1)\nDistance amplitude scaling\nInverse distance attenuation\n\n\nlop~\nLow-pass filter\nFrequency-dependent air absorption simulation\n\n\nexpr (1 - $f1) * 7000 + 1000\nCutoff frequency calculation\nMaps distance to absorption characteristics\n\n\nfreeverb~\nAlgorithmic reverberation\nEnvironmental acoustic simulation\n\n\ns~ dry / r~ dry\nDry signal routing\nDirect sound path management\n\n\nexpr~ $v1*$v2 \\; $v3*$v4\nSpatial audio multiplier\nFinal stereo positioning with distance effects\n\n\n\n\n\n6.7.5 Creative Applications\n\nCinematic Soundscape Design: Create realistic environmental audio where sound sources exhibit authentic distance characteristics for film, gaming, or virtual reality applications.\nArchitectural Acoustic Simulation: Model specific room types by adjusting reverberation parameters to match different environmental contexts from intimate chambers to vast cathedrals.\nInteractive Audio Installations: Design responsive environments where visitor movement controls not just positioning but the complete acoustic experience including filtering and reverberation.\nNature Sound Recreation: Simulate outdoor acoustic environments where wildlife calls, weather sounds, and ambient textures exhibit realistic distance-dependent characteristics.\nPerformance Audio Processing: Process live instruments with dynamic distance effects that can be controlled by performers or automated through score-following systems.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatial Audio</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#simulation-through-quadraphonic-spatialization-the-chowning-model",
    "href": "chapters/space.html#simulation-through-quadraphonic-spatialization-the-chowning-model",
    "title": "6  Spatial Audio",
    "section": "6.8 Simulation through Quadraphonic Spatialization: The Chowning Model",
    "text": "6.8 Simulation through Quadraphonic Spatialization: The Chowning Model\nTo extend spatial simulation beyond the limitations of stereophony—typically constrained to a 90° field—or to achieve finer localization resolution, the intensity panning technique used with two loudspeakers can be generalized to multi-channel systems.\n\n\n\nChowning quadraphonic spatialization model with lookup table generation, circular interface control, and four-speaker amplitude distribution for 360-degree spatial audio positioning.\n\n\nBy placing four loudspeakers at the corners of a square, one can simulate a 360° horizontal sound field, allowing for a full circular distribution of a virtual source. Arranging six speakers in a hexagonal layout also provides 360° coverage, with improved localization accuracy. As a general rule, more channels yield higher resolution and spatial definition, though this comes at the cost of increased system complexity. From a practical standpoint, quadraphonic configurations have proven to offer a compelling balance between implementation feasibility and perceptual quality, and have thus gained wide acceptance in spatial sound simulation.\nOne of the most well-known models in this domain is the John Chowning quadraphonic model. It features:\n\nDirect sound simulation from a virtual source\nLocal reverberation, individually tailored for each speaker based on the source’s angle\nGlobal reverberation shared across all four speakers\n\n\n6.8.1 Amplitude and Distance Scaling\nThe audio input signal is first attenuated according to the inverse of the distance:\n\\[\nA_{\\text{direct}} = \\frac{1}{D}\n\\]\nThen, the signal is scaled by a coefficient specific to each speaker, depending on the virtual source’s position:\n\\[\nA_i = \\frac{1}{D} \\times C_i(\\theta)\n\\]\nwhere:\n\n\\(A_i\\) is the amplitude for speaker \\(i\\),\n\\(D\\) is the distance between the source and the listener,\n\\(C_i(\\theta)\\) is the coefficient for speaker \\(i\\) as a function of the source angle \\(\\theta \\in [0^\\circ, 360^\\circ]\\),\nand the speaker coefficients are labeled LRA, LFA, RFA, RRA (Left Rear, Left Front, Right Front, Right Rear).\n\n\n\n6.8.2 Gain Function and Phase Offsets\nTo avoid complex calculations of gain for each speaker, Chowning implements a predefined function, stored as a lookup table of 512 samples. The table’s function ranges between 0 and 1 and is designed to generate continuous 360° source motion through intensity panning.\nEach speaker reads the same function with a phase offset:\n\nThe first quarter of the function represents a sine wave from \\(0^\\circ\\) to \\(90^\\circ\\)\nThe second quarter is a cosine wave over the same angular range\nThe remaining half is silent (zero amplitude)\n\nAs the virtual source rotates, only two speakers are active at a time, crossfading between them. For instance, as the source moves from \\(theta = 90^\\circ\\) to \\(\\theta = 0^\\circ\\), the cosine-driven speaker fades out while the sine-driven speaker fades in, mimicking a continuous spatial trajectory.\n\n\n6.8.3 Reverberation Modeling\nIn the lower part of Chowning’s model (see again Figure G.8.9), reverberation is processed separately: - The reverberation signal is attenuated by the square root of the inverse of the distance:\n\\[\nA_{\\text{reverb}} = \\sqrt{\\frac{1}{D}} \\times \\text{PRV}\n\\]\nwhere PRV is an empirical parameter representing the reverberation percentage.\n\nA global reverb signal is scaled again by \\(\\frac{1}{D}\\)\nLocal reverberation is further attenuated by \\(1 - \\frac{1}{D}\\), then scaled using the same angle-dependent coefficients (LRA, LFA, RFA, RRA)\n\nTo increase realism, four independent reverberation units are used—one for each loudspeaker—ensuring spatial decorrelation and more convincing immersion.\nThis model not only exemplifies how mathematical reasoning and spatial design converge in sound synthesis, but also stands as a landmark in algorithmic composition and immersive audio. Through relatively simple mathematical operations and efficient data structures (such as phase-offset lookup tables), it offers a powerful foundation for dynamic spatial control in real-time systems such as Pd.\n\n\n6.8.4 Coordinate System\nThe circle object gives you Cartesian coordinates (x, y) in range [-1, 1], but you need:\n\nPolar coordinates (angle, radius)\nScaled values (0-359° for azimuth, 1-10 for distance)\n\n\n\n6.8.5 Mathematical Conversion\n\n6.8.5.1 Azimuth (Angle) Calculation\nWe need to convert (x, y) to angle using arctangent using:\n[expr atan2($f2, $f1) * 180 / 3.14159]\nKey points:\n\natan2(y, x) gives angle in radians (-π to π)\nMultiply by 180/π to convert to degrees (-180° to 180°)\nAdd 360° if negative to get 0-360° range:\n\n[expr (atan2($f2, $f1) * 180 / 3.14159 + 360) % 360]\n\n\n6.8.5.2 Distance Calculation\nConvert (x, y) to radius using Pythagorean theorem, then scale:\n[expr sqrt($f1*$f1 + $f2*$f2) * 9 + 1]\nLogic:\n\nsqrt(x² + y²) gives radius in range [0, 1]\nMultiply by 9 and add 1 to get range [1, 10]\n\n\n\n\n6.8.6 Patch Overview\nThis Pd patch implements John Chowning’s groundbreaking quadraphonic spatialization model, demonstrating how mathematical algorithms can create convincing 360-degree spatial audio movement using four strategically positioned speakers. The implementation features a predefined lookup table, distance-based amplitude scaling, and sophisticated reverberation modeling that simulates both local and global acoustic environments. This approach enables precise control over virtual sound source movement while maintaining perceptual coherence across the complete circular sound field.\nThe patch demonstrates the practical application of Chowning’s theoretical framework through real-time interface controls that manage azimuth positioning, distance simulation, and reverberation balance. By utilizing lookup tables and phase-offset techniques, the system achieves smooth spatial transitions while minimizing computational overhead, making it suitable for live performance and interactive applications.\nThe Chowning quadraphonic spatialization system implements a comprehensive spatial audio pipeline consisting of six primary processing stages:\n\nLookup Table Generation: Creates the fundamental sine/cosine function for spatial positioning\nInteractive Control Interface: Manages azimuth, distance, and reverberation parameters\nAmplitude Coefficient Calculation: Determines speaker-specific gain values using phase offsets\nDistance Modeling: Applies inverse distance relationships for realistic amplitude scaling\nReverberation Processing: Implements both local and global acoustic environment simulation\nQuadraphonic Output: Delivers positioned audio to four independent speaker channels\n\nThe system demonstrates how relatively simple mathematical operations and efficient data structures can create sophisticated spatial audio experiences suitable for both compositional and performance applications.\n\n6.8.6.1 Critical Processing Components\nChowning Lookup Table: The 512-sample array contains a complete sine function from 0° to 180° (samples 0-255) followed by silence (samples 256-511). This creates the characteristic amplitude envelope where only two adjacent speakers are active at any time during 360° rotation.\nAngle Objects: Each of the four speakers uses an angle object with specific phase offsets (0, 128, 256, 384) that correspond to 90° increments around the circular speaker arrangement. These offsets ensure proper spatial positioning as the virtual source rotates.\nDynamic Distance Processing: The patch implements both amplitude attenuation (/~ distance) and reverberation balance modification (sqrt and 1-1/distance calculations) to create realistic distance perception through multiple acoustic cues.\n\n\n\n6.8.7 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Lookup Table Generation] --&gt; B[Interface Controls]\n    B --&gt; C[Amplitude Calculation]\n    B --&gt; D[Distance Processing]\n    C --&gt; E[Audio Positioning]\n    D --&gt; E\n    E --&gt; F[Reverberation]\n    F --&gt; G[Quadraphonic Output LF RF RR LR]\n    \n    style A fill:#e1f5fe\n    style G fill:#f3e5f5\n    style E fill:#fff3e0\n    style F fill:#fff3e0\n\n\n\n\n\n\nThe Chowning quadraphonic spatialization patch orchestrates a sophisticated multi-stage spatial audio processing pipeline that transforms user interface input into convincing 360-degree sound source positioning through mathematical modeling of acoustic propagation and psychoacoustic principles. This implementation demonstrates how John Chowning’s theoretical framework translates into practical real-time audio processing systems that maintain both computational efficiency and perceptual accuracy.\nThe lookup table generation and initialization stage establishes the mathematical foundation for the entire spatialization system through automated creation of the characteristic Chowning function. Upon patch initialization, the loadbang object triggers a systematic process that populates the chowning-table array with 512 samples representing the fundamental spatial positioning function. The generation process employs an until loop that iterates through array indices 0-511, with each position calculated using the expression if($f1&lt;256, sin($f1*3.14159/256), 0). This creates a complete sine wave from 0° to 180° occupying the first half of the table (samples 0-255), followed by complete silence in the second half (samples 256-511). This specific function design ensures that during 360° spatial rotation, only two adjacent speakers remain active at any time, creating smooth amplitude transitions that maintain constant energy output while avoiding phase cancellation effects that could degrade spatial imaging.\nThe interactive control and parameter extraction stage manages real-time spatial positioning through the sophisticated interface provided by the circle object from the ELSE library. This two-dimensional control surface captures mouse interaction as normalized coordinate pairs that represent both angular position and distance information within a single intuitive interface. The coordinate unpacking process separates the combined spatial input into independent data streams: the x-coordinate undergoes conversion to azimuth values spanning 0-359° through the expression (atan2($f2, $f1) * 180 / 3.14159 + 360) % 360, while the y-coordinate transforms into distance values ranging 1-10 through sqrt($f1*$f1 + $f2*$f2) * 9 + 1. This dual parameter extraction enables simultaneous control over both horizontal positioning and depth perception within the virtual acoustic space, providing the essential control data for subsequent processing stages.\nThe amplitude coefficient calculation stage represents the core of Chowning’s spatialization algorithm, where angular position information becomes speaker-specific gain values through phase-offset table reading. Each of the four speakers employs an individual angle object configured with specific phase offsets: 0 samples for the front-left speaker, 128 samples for front-right, 256 samples for rear-right, and 384 samples for rear-left. These offsets correspond to 90° increments around the circular speaker arrangement, ensuring that the spatial positioning function provides appropriate amplitude coefficients for each speaker location. As the azimuth value changes, each angle object reads from its phase-shifted position within the lookup table, generating amplitude coefficients that create smooth transitions between adjacent speakers while maintaining the characteristic two-speaker activation pattern that defines the Chowning model.\nThe distance processing and amplitude scaling stage implements realistic sound propagation characteristics through multiple coordinated processing pathways that simulate both amplitude attenuation and acoustic environmental effects. The primary distance attenuation employs the expression expr~ $v1 / (($v2*9)+1) applied to the raw audio signal, creating inverse distance scaling where increasing distance produces proportionally decreased amplitude. This amplitude scaling affects all speakers equally, maintaining the spatial positioning relationships while adding realistic volume changes that correspond to source proximity. Simultaneously, the distance parameter influences reverberation processing through the sqrt calculation that modifies reverberation level based on distance relationships, simulating how nearby sources produce strong direct signals that mask environmental reflections while distant sources allow reverberation to become more prominent in the overall acoustic balance.\nThe reverberation processing and environmental simulation stage adds spatial acoustic context through dual-path processing that creates both local and global environmental effects. The audio signal path splits to feed both direct positioning processing and environmental reverberation systems implemented through freeverb~ objects configured for large virtual spaces. The reverberation balance control through the knob interface enables real-time adjustment of environmental contribution, while the distance-dependent reverberation scaling creates realistic acoustic behavior where reverberation prominence varies with source distance. The dual reverberation system provides both general environmental ambience and speaker-specific local reflections, creating convincing spatial acoustic contexts that enhance the positioning effects achieved through the amplitude coefficients.\n\n\n\n\n\nflowchart LR\n    A[User Input] --&gt; B[Parameter Processing]\n    B --&gt; C[Spatial Calculation]\n    C --&gt; D[Audio Output]\n\n\n\n\n\n\n\n\n6.8.8 Processing Chain Details\nThe quadraphonic spatialization system maintains mathematical precision and perceptual coherence through coordinated processing stages:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nPatch Initialization\nSine function generation\n512-sample lookup table\n\n\n2\nCircle Interface\nCoordinate conversion\nAzimuth (0-359°) and Distance (1-10)\n\n\n3\nAzimuth + Phase Offsets\nTable reading with interpolation\nSpeaker amplitude coefficients\n\n\n4\nDistance Value\nInverse scaling calculation\nAmplitude attenuation factor\n\n\n5\nAudio + Coefficients + Distance\nMultiplication and mixing\nPositioned audio signals\n\n\n6\nMultiple Audio Streams\nReverberation processing\nEnhanced quadraphonic output\n\n\n\n\n6.8.8.1 Speaker Configuration and Phase Offsets\n\n\n\n\n\n\n\n\n\nSpeaker Position\nPhase Offset (samples)\nAngle Equivalent\nFunction Region\n\n\n\n\nFront Left (LF)\n0\n0°\nSine ascending\n\n\nFront Right (RF)\n128\n90°\nSine descending\n\n\nRear Right (RR)\n256\n180°\nSilent region\n\n\nRear Left (LR)\n384\n270°\nSilent region\n\n\n\n\n\n6.8.8.2 Distance Processing Parameters\n\n\n\n\n\n\n\n\n\nDistance Value\nAmplitude Scaling\nReverberation Factor\nPerceived Effect\n\n\n\n\n1.0 (Close)\n1.000\n0.000\nImmediate, dry, present\n\n\n3.0\n0.250\n0.667\nModerate distance, some reverb\n\n\n6.0\n0.143\n0.833\nDistant, reverberant\n\n\n10.0 (Far)\n0.091\n0.900\nVery distant, highly reverberant\n\n\n\n\n\n6.8.8.3 Lookup Table Function Analysis\nThe Chowning table implements a specific mathematical relationship that ensures smooth spatial transitions:\n\nSamples 0-127: Ascending sine (0 to 1) - Controls fade-in transitions\nSamples 128-255: Descending sine (1 to 0) - Controls fade-out transitions\n\nSamples 256-511: Silent (0) - Ensures two-speaker limitation\n\nThis function design creates the characteristic “phantom source” effect where virtual positioning occurs between active speaker pairs while maintaining constant total energy output.\n\n\n\n6.8.9 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in Quadraphonic System\n\n\n\n\ntabwrite chowning-table\nLookup table creation\nStores 512-sample sine function for spatial coefficients\n\n\nexpr if($f1&lt;256, sin($f1*3.14159/256), 0)\nFunction generation\nCreates sine wave with silent half-cycle\n\n\ncircle\n2D position interface\nProvides x,y coordinates for azimuth and distance control\n\n\nangle\nPhase offset calculation\nImplements speaker-specific table reading positions\n\n\nfreeverb~\nAlgorithmic reverberation\nCreates environmental acoustic simulation\n\n\nexpr~ $v1 / (($v2*9)+1)\nDistance attenuation\nApplies inverse distance amplitude scaling\n\n\nsqrt\nReverberation scaling\nModifies reverb balance based on distance\n\n\n\n\n\n6.8.10 Creative Applications\n\n360-Degree Spatial Composition: Create immersive musical pieces where sound sources move in complete circles around the listener, utilizing the full quadraphonic field for dynamic spatial narratives.\nInteractive Spatial Performance: Connect external controllers or sensors to the azimuth and distance parameters, enabling performers to sculpt spatial positioning through gesture or movement.\nEnvironmental Sound Design: Simulate realistic acoustic environments by combining the Chowning positioning with distance-dependent reverberation for film, game, or installation applications.\nMulti-Source Spatial Mixing: Implement multiple instances of the Chowning model to position independent sound sources within the same quadraphonic field, creating complex spatial polyphony.\nAdaptive Concert Hall Simulation: Use real-time reverberation control to simulate different acoustic environments during performance, from intimate chambers to vast cathedrals.\nCollaborative Virtual Environments: Network multiple quadraphonic systems where remote participants can position sounds within shared virtual acoustic spaces for telepresence applications.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatial Audio</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#midside-ms-processing",
    "href": "chapters/space.html#midside-ms-processing",
    "title": "6  Spatial Audio",
    "section": "6.9 Mid/Side (M/S) Processing",
    "text": "6.9 Mid/Side (M/S) Processing\nMid/Side (M/S) processing is a stereo audio technique that transforms conventional left/right stereo signals into a representation consisting of mid (center) and side (stereo difference) components. This transformation enables precise independent manipulation of the central mono-compatible content and the peripheral stereo information, providing powerful creative control over stereo imaging and spatial characteristics. The technique finds extensive application in mixing, mastering, and sound design contexts where selective processing of spatial components is required.\nThe M/S approach offers unique advantages over conventional stereo processing by separating spatial information into functionally distinct components. The mid channel contains the sum of left and right signals, representing content that appears centered in the stereo field, while the side channel contains the difference between left and right signals, representing the spatial width and ambience information. This separation enables processing strategies that would be impossible with conventional L/R stereo signals.\n\n\n\nMid/side stereo processing with adjustable “width” control.\n\n\n\n6.9.1 Historical Context\nThe Mid/Side (M/S) microphone recording technique is not a recent innovation. It was originally conceived by Alan Blumlein, a British engineer working at EMI and a pivotal figure in the development of stereophonic and surround sound. Blumlein patented the M/S technique in 1933, and it was implemented in some of the earliest stereophonic recordings, laying the foundation for much of what would follow in multichannel audio. Over time, M/S recording has found widespread adoption in broadcasting due to its inherent mono compatibility—a critical requirement in transmission environments where listeners may receive a downmixed signal. Its adaptability has also made it a mainstay in studio environments, where precise control over stereo width is valued, as well as in live recording scenarios, where on-site flexibility is essential.\n\n\n6.9.2 Why Choose Mid/Side?\nOne of the key advantages of M/S over traditional XY stereo techniques lies in its post-production flexibility. In XY configurations, the stereo image is fixed at the moment of recording. Any adjustments to the spatial field must be made prior to—or during—recording, leaving limited room for change afterward. Additionally, summing XY recordings to mono can result in undesirable phase cancellations, compromising audio quality.\nIn contrast, the M/S technique allows for the stereo width to be manipulated even after the recording session. By adjusting the balance between the mid and side signals, engineers can widen, narrow, or even collapse the stereo image to mono without destructive artifacts. This makes M/S a powerful tool for both artistic control and technical resilience.\n\n\n\nMS recording technique: The Mid mic acts as the center channel, while the Side mic signal creates the stereo ambience.\n\n\nThe core concept of M/S recording is elegantly simple yet powerful. The Mid microphone captures the central, on-axis sound—essentially forming the mono-compatible core of the stereo image. The Side microphone, with its figure-8 pattern, captures ambient and directional information from either side of the soundstage.\nThe critical feature of the Side mic is that its two lobes are 180 degrees out of phase. This means that a pressure increase on one side of the diaphragm results in an equivalent pressure decrease on the other. Conventionally, the positive lobe (+) is oriented to the left, while the negative lobe (−) faces the right side of the soundstage. When decoded into stereo, this phase relationship creates lateral cues that define the width and spatial depth of the stereo field.\n\n\n6.9.3 Patch Overview\nThe mid-side-stereo.pd patch implements a complete M/S processing system that demonstrates both encoding and decoding stages within a single interface. The patch features a stereo audio file player (drum.wav) that feeds into an M/S encoder, followed by a stereo width control that manipulates the side channel amplitude, and finally an M/S decoder that reconstructs the stereo signal. This architecture enables real-time demonstration of how M/S processing affects stereo imaging while maintaining mathematical precision throughout the signal chain.\nThe system incorporates a sophisticated stereo width control mechanism that allows users to adjust the prominence of the side channel from complete mono (width = 0) to exaggerated stereo enhancement (width &gt; 1). This control provides immediate auditory feedback about how M/S processing affects spatial perception while demonstrating the mathematical relationships underlying stereo imaging manipulation.\nThe patch design emphasizes educational clarity by implementing the encoding and decoding stages as separate processing blocks with clear signal flow visualization. The mathematical operations are implemented using expr~ objects that execute the precise calculations required for M/S transformation while maintaining the amplitude relationships essential for accurate stereo reconstruction.\n\n\n6.9.4 Data Flow\n\n\n\n\n\nflowchart TD\n    A[Audio File Player] --&gt; B[Stereo Input L/R]\n    B --&gt; C[M/S Encoding]\n    C --&gt; D[Mid Channel M = L + R]\n    C --&gt; E[Side Channel S = L - R]\n    E --&gt; F[Stereo Width Control]\n    F --&gt; G[Modified Side Channel]\n    D --&gt; H[M/S Decoding]\n    G --&gt; H\n    H --&gt; I[Left Output L = M + S * 0.7071]\n    H --&gt; J[Right Output R = M - S * 0.7071]\n    I --&gt; K[Stereo Audio Output]\n    J --&gt; K\n    \n    style A fill:#e1f5fe\n    style K fill:#f3e5f5\n    style C fill:#fff3e0\n    style H fill:#fff3e0\n\n\n\n\n\n\nThe M/S processing patch orchestrates a comprehensive stereo transformation pipeline that demonstrates both the mathematical principles and practical applications of mid/side processing through systematic signal manipulation stages. This implementation provides real-time control over stereo imaging while maintaining the mathematical relationships essential for accurate audio reconstruction.\nThe audio source and initialization stage establishes the foundation for M/S processing through automated stereo audio file playback that provides consistent source material for demonstration purposes. The readsf~ 2 object loads and plays the stereo audio file drum.wav, providing left and right channel audio streams that serve as input to the M/S processing chain. The looping mechanism implemented through the bang connection from the third outlet of readsf~ to the s loop send object ensures continuous playback that enables sustained observation of the M/S processing effects. The manual playback control through the PLAY! button provides user initiation of the audio source, while the file opening message open drum.wav, 1 prepares the stereo audio file for processing. This source material selection proves particularly effective for M/S demonstration because drum recordings typically contain both centered elements (kick, snare) that appear prominently in the mid channel and peripheral elements (cymbals, room ambience) that contribute significantly to the side channel.\nThe M/S encoding stage implements the fundamental mathematical transformation that separates stereo spatial information into functionally distinct mid and side components through parallel expression processing. The stereo input from the audio file feeds into two separate expr~ objects that perform the essential M/S calculations: the mid channel calculation through expr~ ($v1 + $v2) * 0.7071 creates the sum of left and right channels with appropriate amplitude scaling, while the side channel calculation through expr~ ($v1 - $v2) * 0.7071 generates the difference between left and right channels similarly scaled. The scaling factor 0.7071 (1/√2) ensures proper amplitude relationships that prevent signal overflow while maintaining mathematical accuracy throughout the encoding process. This encoding stage transforms the conventional stereo signal into a representation where the mid channel contains all centered audio content and the side channel contains all spatial width information, enabling independent manipulation of these components.\nThe stereo width control and side channel modification stage provides real-time manipulation of the spatial characteristics through user interface control that affects the prominence of the side channel information. The horizontal slider (hsl) interface element provides intuitive control over stereo width, with values ranging from 0 (complete mono) to 1 (original stereo width) and beyond for enhanced stereo effects. The slider output connects to a float atom display that provides numerical feedback of the current width setting, enabling precise control over the spatial manipulation amount. The width control value feeds into the expr~ $v1 * $v2 expression that multiplies the side channel signal by the user-controlled width factor, directly affecting how much spatial information contributes to the final stereo reconstruction. This multiplication approach enables smooth transitions from mono (width = 0, side channel eliminated) through original stereo imaging (width = 1, side channel unaffected) to enhanced stereo width (width &gt; 1, side channel emphasized).\nThe M/S decoding and stereo reconstruction stage converts the processed mid and side components back into conventional left/right stereo format through mathematical operations that reverse the encoding transformation while incorporating the width modifications. The decoding process employs two expr~ objects that implement the inverse M/S transformation: the left channel reconstruction through expr~ ($v1 + $v2) * 0.7071 combines the mid channel with the width-modified side channel, while the right channel reconstruction through expr~ ($v1 - $v2) * 0.7071 combines the mid channel with the inverted width-modified side channel. These calculations ensure that centered audio content (prominent in the mid channel) appears equally in both output channels while spatial content (contained in the side channel) contributes to the left-right differences that create stereo imaging. The 0.7071 scaling factor applied during decoding maintains proper amplitude relationships and prevents the signal amplification that would otherwise result from the mathematical operations.\nThe output and monitoring stage delivers the processed stereo audio while providing visual feedback about the processing parameters through integrated display elements that enable users to understand the relationship between control input and audio output characteristics. The final stereo signal feeds into the output~ object that manages audio interface connection and level control for monitoring purposes. The numerical displays connected to various processing stages show the current width parameter value and provide visual indication of the signal processing activity through the connected bang objects. This monitoring system enables users to observe how changes to the stereo width control immediately affect the audio output while providing confirmation that the M/S processing maintains proper signal levels and stereo relationships.\n\n\n\n\n\nflowchart LR\n    A[Stereo Input] --&gt; B[M/S Encode]\n    B --&gt; C[Width Control]\n    C --&gt; D[M/S Decode]\n    D --&gt; E[Stereo Output]\n\n\n\n\n\n\n\n\n6.9.5 Processing Chain Details\nThe M/S processing system maintains mathematical precision through coordinated processing stages:\n\n\n\n\n\n\n\n\n\nStage\nInput\nProcess\nOutput\n\n\n\n\n1\nStereo Audio File\nFile loading and playback\nLeft and right audio channels\n\n\n2\nL/R Channels\nM/S encoding with scaling\nMid and side channels\n\n\n3\nSide Channel + Width Control\nAmplitude multiplication\nModified side channel\n\n\n4\nMid + Modified Side\nM/S decoding with scaling\nReconstructed L/R channels\n\n\n5\nProcessed Stereo\nAudio interface output\nFinal stereo audio\n\n\n\n\n\n6.9.6 Mathematical Relationships\nThe mathematical foundation of M/S processing is straightforward and relies on simple linear operations.\nTo encode a standard left/right (L/R) stereo signal into mid and side channels, the following equations are used:\n\\[\n\\begin{align*}\nM &= L + R \\\\\nS &= L - R\n\\end{align*}\n\\]\nHere, \\((M)\\) (mid) represents the sum of the left and right channels, capturing the content that is common to both (centered in the stereo field), while \\((S)\\) (side) represents the difference, capturing the elements that are unique to each channel (the stereo width).\nTo decode and return to the conventional stereo format, the inverse operation is performed:\n\\[\n\\begin{align*}\nL &= \\frac{M + S}{2} \\\\\nR &= \\frac{M - S}{2}\n\\end{align*}\n\\]\nThe division by 2 in the decoding equations is crucial. Without this scaling, the amplitude of the reconstructed left and right channels would be doubled, potentially causing clipping or an unnatural increase in loudness. This scaling ensures that the energy of the signal remains consistent before and after the M/S transformation.\nIn practice, instead of dividing by 2, it is common to apply a gain correction of \\((-3\\,\\mathrm{dB})\\), which corresponds to multiplying each channel by:\n\\[\n\\frac{1}{\\sqrt{2}} \\approx 0.7071\n\\]\nThis factor is used because when both channels are summed or differenced, their combined amplitude could reach up to twice the original value. By scaling each by (0.7071), the total energy is preserved:\n\\[\n2 \\times 0.7071 \\times 0.7071 \\approx 1\n\\]\nThis approach maintains amplitude consistency and prevents distortion, ensuring that a round-trip conversion (L/R \\(\\rightarrow\\) M/S \\(\\rightarrow\\) L/R) yields a signal with the same loudness as the original.\nM/S processing is widely used in mixing and mastering. It allows engineers to adjust the stereo width, enhance mono compatibility, or apply effects selectively to the mid or side components. For example, boosting the side channel can widen the stereo image, while attenuating it can make the mix more mono-compatible. Creative sound design often involves processing the mid and side channels differently, such as adding reverb only to the sides or compressing the mid for a punchier center.\nA notable feature of M/S processing is its symmetry: the same mathematical structure is used for both encoding and decoding, differing only by the scaling factor. This symmetry arises because the conversion matrix for M/S is its own inverse (up to scaling), which simplifies both implementation and modularity in digital audio environments like Pure Data, Max/MSP, or custom DSP code. This efficiency, combined with its creative flexibility, makes M/S processing a staple in modern audio production.\nThe patch implements the fundamental M/S transformation equations with proper amplitude scaling:\n\n\n\n\n\n\n\n\nParameter\nMathematical Function\nAudio Application\n\n\n\n\nMid Encoding\n\\(M = (L + R) \\times 0.7071\\)\nSum channels with amplitude correction\n\n\nSide Encoding\n\\(S = (L - R) \\times 0.7071\\)\nDifference channels with amplitude correction\n\n\nWidth Modification\n\\(S' = S \\times W\\)\nApply user-controlled width factor\n\n\nLeft Decoding\n\\(L' = (M + S') \\times 0.7071\\)\nReconstruct left with processed side\n\n\nRight Decoding\n\\(R' = (M - S') \\times 0.7071\\)\nReconstruct right with processed side\n\n\n\n\n\n6.9.7 Stereo Width Control Effects\n\n\n\n\n\n\n\n\n\nWidth Setting\nSide Channel Scaling\nStereo Effect\nPractical Application\n\n\n\n\n0.0\nComplete elimination\nMono (no stereo width)\nMono compatibility checking\n\n\n0.5\n50% reduction\nNarrowed stereo image\nSubtle width reduction\n\n\n1.0\nOriginal level\nUnchanged stereo width\nBypass/reference setting\n\n\n1.5\n50% enhancement\nWidened stereo image\nStereo enhancement\n\n\n2.0\nDouble amplitude\nSignificantly widened\nExtreme stereo effects\n\n\n\n\n\n6.9.8 Key Objects and Their Roles\n\n\n\n\n\n\n\n\nObject\nFunction\nRole in M/S Processing\n\n\n\n\nreadsf~ 2\nStereo file player\nProvides consistent stereo source material\n\n\nexpr~ ($v1 + $v2) * 0.7071\nMid channel encoder\nCalculates sum of L/R with amplitude scaling\n\n\nexpr~ ($v1 - $v2) * 0.7071\nSide channel encoder\nCalculates difference of L/R with amplitude scaling\n\n\nexpr~ $v1 * $v2\nWidth control multiplier\nApplies user-controlled scaling to side channel\n\n\nWidth control\nStereo width interface\nProvides real-time control over spatial width\n\n\noutput~\nStereo audio output\nDelivers processed stereo audio for monitoring\n\n\n\n\n\n6.9.9 Creative Applications\n\nStereo Width Control: Use for dynamic stereo imaging effects in music production, from subtle width adjustments to dramatic spatial effects that respond to musical dynamics or external control sources.\nMono Compatibility Optimization: Employ during mixing and mastering to check how stereo mixes translate to mono playback systems, ensuring critical elements remain audible when stereo width is eliminated.\nCreative Sound Design: Apply different processing to mid and side channels independently—for example, compress the mid channel for punchy center content while adding reverb only to the side channel for enhanced ambience.\nLive Performance Control: Connect the width parameter to MIDI controllers or OSC input for real-time spatial manipulation during live electronic music performances or DJ sets.\nMastering Enhancement: Implement as part of mastering chains where precise control over stereo imaging enables correction of mix balance issues or enhancement of spatial characteristics.\nInstallation Art: Create responsive audio environments where visitor proximity or movement controls stereo width, creating intimate mono experiences that expand to full stereo as people move through the space.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatial Audio</span>"
    ]
  },
  {
    "objectID": "chapters/space.html#references",
    "href": "chapters/space.html#references",
    "title": "6  Spatial Audio",
    "section": "References",
    "text": "References\n\n\n\n\nAbregú, E. L., E. R. Calcagno, and R. O. Vergara. 2012. “La Distancia Como Factor Estructural de La Música.” Revista Argentina de Musicología, no. 12-13: 379–400.\n\n\nBasso, Gustavo, Oscar Pablo Di Liscia, and Juan Pampin, eds. 2009. Música y Espacio: Ciencia, Tecnología y Estética. Buenos Aires: Editorial de la Universidad Nacional de Quilmes.\n\n\nKnudsen, V. O. 1932. Architectural Acoustics. J. Wiley y Sons.\n\n\nRisett, J. C. 1969. “An Introductory Catalogue of Computer Synthesized Sounds.” Nueva Jersey: Bell Telephone Laboratories.\n\n\nSchaeffer, Pierre. 2003. Tratado de Los Objetos Musicales. Translated by Araceli Cabezón de Diego. Alianza Editorial.\n\n\nStockhausen, Karlheinz. 1959. “Musik Im Raum.” Die Reihe, no. 5.\n\n\nWishart, Trevor. 1996. On Sonic Art. Amsterdam: Harwood Academic Publishers.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Spatial Audio</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html",
    "href": "chapters/conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "A Synthesis of Themes\nWriting this book has been, above all, an exercise in listening: listening to code, to sound, to problems waiting for solutions—and, perhaps most importantly, to the questions that emerge from the creative process itself. Code That Sounds was never intended to offer definitive answers or a closed system. Rather, it opens a space—a conceptual and practical territory—where sound, code, and imagination intersect. As such, this conclusion is not a full stop, but a resonance: a reflection on what we’ve learned, what remains unresolved, and what may yet be explored.\nThroughout these chapters, we moved between conceptual exploration and hands-on implementation. From recording and playback, to sequencing, synthesis, sonification, spatialization, and tool design, the book traced a path through fundamental aspects of sound-based creative coding. Each section aimed to present not only techniques, but the deeper questions and ideas that underpin them. We considered how sequences construct musical time, how synthesis reimagines the materiality of sound, how data becomes expressive when translated into sonic form, and how space can be composed just as much as pitch or rhythm.\nAcross these topics, we have tried to maintain a balance between rigor and experimentation. Pd was our instrument of choice—not because it is the only or best option, but because it invites a certain kind of thinking: modular, open, visual, and sonic. Yet the language itself was never the focus. What mattered more was the clarity of thought behind the algorithm, the elegance of the structure, and the potential for creative variation. This book does not teach a tool; it invites a way of thinking.",
    "crumbs": [
      "Conclusion"
    ]
  },
  {
    "objectID": "chapters/conclusion.html#sound-code-and-interaction",
    "href": "chapters/conclusion.html#sound-code-and-interaction",
    "title": "Conclusion",
    "section": "Sound, Code, and Interaction",
    "text": "Sound, Code, and Interaction\nSound is not just output—it is medium, metaphor, and message. Code is not just syntax—it is structure, process, and potential. And interaction is not merely control—it is participation, embodiment, and interpretation. This triad—sound, code, interaction—forms the expressive core of the practices we explore. Together, they offer a fertile ground for experimentation, performance, and critique.\nBy treating code as a creative material, we position it alongside other artistic practices. We sketch with it, compose with it, prototype with it. We revise, remix, and reimagine. And in doing so, we shift the relationship between maker and machine—from command to conversation. It is in this dynamic, live, improvisational space that creative coding finds its strongest voice.",
    "crumbs": [
      "Conclusion"
    ]
  },
  {
    "objectID": "chapters/conclusion.html#open-tools-open-futures",
    "href": "chapters/conclusion.html#open-tools-open-futures",
    "title": "Conclusion",
    "section": "Open Tools, Open Futures",
    "text": "Open Tools, Open Futures\nThis work would not exist without the open-source communities that sustain tools like Pd. The accessibility of such platforms democratizes creative technology, allowing artists, educators, and students to participate in practices that were once locked behind proprietary systems. But openness is more than a license—it is a philosophy. It implies transparency, generosity, and collective authorship. It values process as much as product.\nStill, working with open tools also reveals their limits. Documenting, teaching, and developing them is labor-intensive and often under-resourced. In this sense, writing a book like this one not only reveals the scope of creative coding—it also exposes its fragilities. But that, too, is part of the story: a story of building infrastructures that support exploration, criticality, and care.",
    "crumbs": [
      "Conclusion"
    ]
  },
  {
    "objectID": "chapters/conclusion.html#toward-what-comes-next",
    "href": "chapters/conclusion.html#toward-what-comes-next",
    "title": "Conclusion",
    "section": "Toward What Comes Next",
    "text": "Toward What Comes Next\nIf this book succeeds, it will not be because it taught you how to patch a synthesizer or connect a sensor. It will be because it left you with questions—about systems, about aesthetics, about perception and participation. The real value of each chapter lies in what comes after: the remix, the variation, the new problem posed. Creative coding is not just a way to make things; it is a way to think, to test, to sense.\nAlgorithmic thinking, in this context, is not about efficiency or automation. It is about strategy, iteration, and design. It invites us to map the terrain of a problem, to build a process that responds to constraints, and to remain open to surprise. This mindset can be applied well beyond the boundaries of sound or art: it is a tool for navigating complexity, for making the invisible audible, for composing with systems as much as with signals.\nIn the end, Code That Sounds is a beginning, not a culmination. I hope it sparks your own experiments, collaborations, and inquiries. I hope it gives you language—not only to describe your tools, but to invent new ones. And above all, we hope it helps you listen: to code, to sound, to the world you are helping to shape.",
    "crumbs": [
      "Conclusion"
    ]
  }
]