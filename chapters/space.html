<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Spatial Audio – Code that Sounds</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/conclusion.html" rel="next">
<link href="../chapters/sonification.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ef56b68f8fa1e9d2ba328e99e439f80.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-1e81d527365e20b3495763033adbcf98.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-8ab8b311eebd604b8865a9cbc6c08092.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-1e81d527365e20b3495763033adbcf98.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="mermaid-theme" content="neutral">
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/space.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Spatial Audio</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Code that Sounds</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/ezequielabregu/code-that-sounds" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/sequencing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sequencing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/synthesis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sound Synthesis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/sonification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sonification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/space.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Spatial Audio</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-music-sound-space" id="toc-the-music-sound-space" class="nav-link active" data-scroll-target="#the-music-sound-space"><span class="header-section-number">6.1</span> The Music Sound Space</a>
  <ul class="collapse">
  <li><a href="#spatial-distribution-and-the-evolution-of-listening-spaces" id="toc-spatial-distribution-and-the-evolution-of-listening-spaces" class="nav-link" data-scroll-target="#spatial-distribution-and-the-evolution-of-listening-spaces"><span class="header-section-number">6.1.1</span> Spatial Distribution and the Evolution of Listening Spaces</a></li>
  <li><a href="#spatial-recontextualization-and-auditory-expectation" id="toc-spatial-recontextualization-and-auditory-expectation" class="nav-link" data-scroll-target="#spatial-recontextualization-and-auditory-expectation"><span class="header-section-number">6.1.2</span> Spatial Recontextualization and Auditory Expectation</a></li>
  <li><a href="#the-expanded-spatial-palette-of-electroacoustic-music" id="toc-the-expanded-spatial-palette-of-electroacoustic-music" class="nav-link" data-scroll-target="#the-expanded-spatial-palette-of-electroacoustic-music"><span class="header-section-number">6.1.3</span> The Expanded Spatial Palette of Electroacoustic Music</a></li>
  </ul></li>
  <li><a href="#spatial-localization-of-sound" id="toc-spatial-localization-of-sound" class="nav-link" data-scroll-target="#spatial-localization-of-sound"><span class="header-section-number">6.2</span> Spatial Localization of Sound</a>
  <ul class="collapse">
  <li><a href="#psychoacoustic-factors-in-sound-localization" id="toc-psychoacoustic-factors-in-sound-localization" class="nav-link" data-scroll-target="#psychoacoustic-factors-in-sound-localization"><span class="header-section-number">6.2.1</span> Psychoacoustic Factors in Sound Localization</a></li>
  </ul></li>
  <li><a href="#hearing-in-enclosed-spaces" id="toc-hearing-in-enclosed-spaces" class="nav-link" data-scroll-target="#hearing-in-enclosed-spaces"><span class="header-section-number">6.3</span> Hearing in Enclosed Spaces</a></li>
  <li><a href="#virtual-source-simulation-via-stereophonic-techniques" id="toc-virtual-source-simulation-via-stereophonic-techniques" class="nav-link" data-scroll-target="#virtual-source-simulation-via-stereophonic-techniques"><span class="header-section-number">6.4</span> Virtual Source Simulation via Stereophonic Techniques</a></li>
  <li><a href="#implementing-a-stereo-spatialization-in-pure-data" id="toc-implementing-a-stereo-spatialization-in-pure-data" class="nav-link" data-scroll-target="#implementing-a-stereo-spatialization-in-pure-data"><span class="header-section-number">6.5</span> Implementing a Stereo Spatialization in Pure Data</a>
  <ul class="collapse">
  <li><a href="#patch-overview" id="toc-patch-overview" class="nav-link" data-scroll-target="#patch-overview"><span class="header-section-number">6.5.1</span> Patch Overview</a></li>
  <li><a href="#key-objects-and-their-roles" id="toc-key-objects-and-their-roles" class="nav-link" data-scroll-target="#key-objects-and-their-roles"><span class="header-section-number">6.5.2</span> Key Objects and Their Roles</a></li>
  <li><a href="#data-flow" id="toc-data-flow" class="nav-link" data-scroll-target="#data-flow"><span class="header-section-number">6.5.3</span> Data Flow</a></li>
  <li><a href="#processing-chain-details" id="toc-processing-chain-details" class="nav-link" data-scroll-target="#processing-chain-details"><span class="header-section-number">6.5.4</span> Processing Chain Details</a></li>
  <li><a href="#creative-applications" id="toc-creative-applications" class="nav-link" data-scroll-target="#creative-applications"><span class="header-section-number">6.5.5</span> Creative Applications</a></li>
  </ul></li>
  <li><a href="#simulating-distance" id="toc-simulating-distance" class="nav-link" data-scroll-target="#simulating-distance"><span class="header-section-number">6.6</span> Simulating Distance</a>
  <ul class="collapse">
  <li><a href="#patch-overview-1" id="toc-patch-overview-1" class="nav-link" data-scroll-target="#patch-overview-1"><span class="header-section-number">6.6.1</span> Patch Overview</a></li>
  <li><a href="#key-objects-and-their-roles-1" id="toc-key-objects-and-their-roles-1" class="nav-link" data-scroll-target="#key-objects-and-their-roles-1"><span class="header-section-number">6.6.2</span> Key Objects and Their Roles</a></li>
  <li><a href="#data-flow-1" id="toc-data-flow-1" class="nav-link" data-scroll-target="#data-flow-1"><span class="header-section-number">6.6.3</span> Data Flow</a></li>
  <li><a href="#processing-chain-details-1" id="toc-processing-chain-details-1" class="nav-link" data-scroll-target="#processing-chain-details-1"><span class="header-section-number">6.6.4</span> Processing Chain Details</a></li>
  <li><a href="#creative-applications-1" id="toc-creative-applications-1" class="nav-link" data-scroll-target="#creative-applications-1"><span class="header-section-number">6.6.5</span> Creative Applications</a></li>
  </ul></li>
  <li><a href="#enhanced-distance-modeling-with-air-absorption-and-reverberation" id="toc-enhanced-distance-modeling-with-air-absorption-and-reverberation" class="nav-link" data-scroll-target="#enhanced-distance-modeling-with-air-absorption-and-reverberation"><span class="header-section-number">6.7</span> Enhanced Distance Modeling with Air Absorption and Reverberation</a>
  <ul class="collapse">
  <li><a href="#patch-overview-2" id="toc-patch-overview-2" class="nav-link" data-scroll-target="#patch-overview-2"><span class="header-section-number">6.7.1</span> Patch Overview</a></li>
  <li><a href="#key-objects-and-their-roles-2" id="toc-key-objects-and-their-roles-2" class="nav-link" data-scroll-target="#key-objects-and-their-roles-2"><span class="header-section-number">6.7.2</span> Key Objects and Their Roles</a></li>
  <li><a href="#data-flow-2" id="toc-data-flow-2" class="nav-link" data-scroll-target="#data-flow-2"><span class="header-section-number">6.7.3</span> Data Flow</a></li>
  <li><a href="#processing-chain-details-2" id="toc-processing-chain-details-2" class="nav-link" data-scroll-target="#processing-chain-details-2"><span class="header-section-number">6.7.4</span> Processing Chain Details</a></li>
  <li><a href="#creative-applications-2" id="toc-creative-applications-2" class="nav-link" data-scroll-target="#creative-applications-2"><span class="header-section-number">6.7.5</span> Creative Applications</a></li>
  </ul></li>
  <li><a href="#simulation-through-quadraphonic-spatialization-the-chowning-model" id="toc-simulation-through-quadraphonic-spatialization-the-chowning-model" class="nav-link" data-scroll-target="#simulation-through-quadraphonic-spatialization-the-chowning-model"><span class="header-section-number">6.8</span> Simulation through Quadraphonic Spatialization: The Chowning Model</a>
  <ul class="collapse">
  <li><a href="#amplitude-and-distance-scaling" id="toc-amplitude-and-distance-scaling" class="nav-link" data-scroll-target="#amplitude-and-distance-scaling"><span class="header-section-number">6.8.1</span> Amplitude and Distance Scaling</a></li>
  <li><a href="#gain-function-and-phase-offsets" id="toc-gain-function-and-phase-offsets" class="nav-link" data-scroll-target="#gain-function-and-phase-offsets"><span class="header-section-number">6.8.2</span> Gain Function and Phase Offsets</a></li>
  <li><a href="#reverberation-modeling" id="toc-reverberation-modeling" class="nav-link" data-scroll-target="#reverberation-modeling"><span class="header-section-number">6.8.3</span> Reverberation Modeling</a></li>
  <li><a href="#coordinate-system" id="toc-coordinate-system" class="nav-link" data-scroll-target="#coordinate-system"><span class="header-section-number">6.8.4</span> Coordinate System</a></li>
  <li><a href="#mathematical-conversion" id="toc-mathematical-conversion" class="nav-link" data-scroll-target="#mathematical-conversion"><span class="header-section-number">6.8.5</span> Mathematical Conversion</a></li>
  <li><a href="#patch-overview-3" id="toc-patch-overview-3" class="nav-link" data-scroll-target="#patch-overview-3"><span class="header-section-number">6.8.6</span> Patch Overview</a></li>
  <li><a href="#key-objects-and-their-roles-3" id="toc-key-objects-and-their-roles-3" class="nav-link" data-scroll-target="#key-objects-and-their-roles-3"><span class="header-section-number">6.8.7</span> Key Objects and Their Roles</a></li>
  <li><a href="#data-flow-3" id="toc-data-flow-3" class="nav-link" data-scroll-target="#data-flow-3"><span class="header-section-number">6.8.8</span> Data Flow</a></li>
  <li><a href="#processing-chain-details-3" id="toc-processing-chain-details-3" class="nav-link" data-scroll-target="#processing-chain-details-3"><span class="header-section-number">6.8.9</span> Processing Chain Details</a></li>
  <li><a href="#creative-applications-3" id="toc-creative-applications-3" class="nav-link" data-scroll-target="#creative-applications-3"><span class="header-section-number">6.8.10</span> Creative Applications</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Spatial Audio</span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<p>In the realm of sound, the concept of space is often overlooked. Yet, it plays a crucial role in shaping our auditory experience. The spatial dimension of sound is not merely an acoustic phenomenon; it is a fundamental aspect of how we perceive sound. This chapter explores the multifaceted nature of sound space, its historical evolution, and its implications for contemporary electroacoustic music.</p>
<section id="the-music-sound-space" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="the-music-sound-space"><span class="header-section-number">6.1</span> The Music Sound Space</h2>
<p>Sound always unfolds within a specific time and space. From the very moment sound is generated in music, space is implicitly present, enabling the possibility of organizing, reconstructing, and shaping that space to influence musical form. Thus, the use of space in music responds to concerns that go far beyond mere acoustic considerations.</p>
<p>Conceiving space as a structural element in the construction of sonic discourse requires us to address a complex notion—one that touches on multiple dimensions of composition, performance, and perception. This idea rests on the argument that space is a composite musical element, one that can be integrated into a compositional structure and assume a significant role within the formal hierarchy of sound discourse.</p>
<p>Spatiality in music represents a compositional variable with a long historical lineage. The treatment of sound space on stage can be traced back to classical Greek theatre, where actors and chorus members used masks to amplify vocal resonance and enhance vocal directionality <span class="citation" data-cites="knudsen1932">(<a href="#ref-knudsen1932" role="doc-biblioref">Knudsen 1932</a>)</span>. However, while compositional techniques addressing spatial sound have been developing for centuries, it is not until the early 20th century that a systematic use of spatial dimensions becomes central to musical structure. Landmark works such as <em>Universe Symphony</em> (1911–51) by Charles Ives, <em>Déserts</em> (1954) and <em>Poème électronique</em> (1958) by Edgard Varèse, <em>Gruppen</em> (1955–57) by Karlheinz Stockhausen, and <em>Persephassa</em> (1969) by Iannis Xenakis approach spatiality as an independent and structural dimension. In these works, space is interrelated with other sonic parameters such as timbre, dynamics, duration, and pitch.</p>
<p>Particularly noteworthy is that these pieces do not merely establish sonic space as a new musical dimension—they also develop theoretical frameworks for the spatialization of sound. These theories consider physical, poetic, pictorial, and perceptual aspects of spatial sound. As a result, in recent decades the areas of research and application related to artistic-musical knowledge have expanded significantly, fostering interdisciplinary inquiry into sound space and its parameters.</p>
<p>Beyond its central role in music, in recent years spatial sound has also become a subject of study across a variety of disciplines. One example of this is found in research into sound spatialization techniques in both real and virtual acoustic environments. These techniques have largely evolved based on principles of psychoacoustics, cognitive modeling, and technological advancements, leading to the development of specialized hardware and software for spatial sound processing. In the scientific realm, theses and research articles have explored, in great detail, the cues necessary to create an acoustic image of a virtual source and how these cues must be simulated—whether by electronic circuitry or computational algorithms.</p>
<p>However, it must be noted that musical techniques for managing sound space have not yet fully integrated findings from perceptual science. As we will explore later, many artistic approaches rely on only a subset of the cues involved in auditory spatial perception, overlooking others that are essential for producing a convincing acoustic image.</p>
<p>In general terms, the spatial dimension of a musical composition can function as the primary expressive and communicative attribute for the listener. For instance, from the very first moment, the acoustic characteristics of a venue—real or virtual—directly affect how we perceive the sonic discourse. This dimension of musical space is further shaped by the placement of sound sources, whether instrumentalists or loudspeakers. A historical precedent for this can be found in the Renaissance period through the use of divided choirs in the Basilica of San Marco in Venice. This stylistic practice, known as antiphonal music, represents a clear antecedent of the deep connection between musical construction and architectural space <span class="citation" data-cites="stockhausen1959">(<a href="#ref-stockhausen1959" role="doc-biblioref">Stockhausen 1959</a>)</span>. The style of Venetian composers was strongly influenced by the acoustics and architectural features of San Marco, involving spatially separated choirs or instrumental groups performing in alternation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/images/space/fig-1-san-marcos.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure 1: Floor plan of the Basilica of San Marco in Venice (Italy)."><img src="../assets/images/space/fig-1-san-marcos.png" class="img-fluid figure-img" alt="Figure 1: Floor plan of the Basilica of San Marco in Venice (Italy)."></a></p>
<figcaption><strong>Figure 1</strong>: Floor plan of the Basilica of San Marco in Venice (Italy).</figcaption>
</figure>
</div>
<section id="spatial-distribution-and-the-evolution-of-listening-spaces" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="spatial-distribution-and-the-evolution-of-listening-spaces"><span class="header-section-number">6.1.1</span> Spatial Distribution and the Evolution of Listening Spaces</h3>
<p>During the European Classical and Romantic periods, the spatial distribution of musicians generally adhered to the French ideal inspired by 19th-century military band traditions. In this widely adopted setup, space was typically divided into two main zones: musicians arranged on a frontal stage and the audience seated facing them. This concert model—linear, frontal, and fixed—reflected a formalized, hierarchical approach to the musical experience.</p>
<p>By the 20th century, this front-facing paradigm began to be reexamined and reimagined. Composers and sound artists started to question the limitations of conventional concert hall layouts. A notable example is found in 1962, when German composer Karlheinz Stockhausen proposed a radical redesign of the concert space to accommodate the evolving needs of contemporary composition. His vision included a circular venue without a fixed podium, flexible seating arrangements, adaptable ceiling and wall surfaces for mounting loudspeakers and microphones, suspended balconies for musicians, and configurable acoustic properties. These directives were not merely architectural; they were compositional in nature, meant to transform the listening experience by embedding spatiality into the core of musical structure.</p>
<p>This reconceptualization of space enabled a new way of thinking about the sensations and experiences that could be elicited in the listener through the manipulation of spatial audio. The concert hall was no longer a passive container of sound, but an active participant in its articulation.</p>
<p>In parallel developments, particularly in the field of electroacoustic music during the mid-20th century, spatial concerns were often shaped by the available technology of the time. Spatiality lacked a clearly defined theoretical vocabulary and was not yet integrated systematically with other familiar musical parameters. Nonetheless, from its inception, electroacoustic music leveraged technology to expand musical boundaries—either through electronic signal processing or by interacting with traditional instruments.</p>
<p>However, the notion of space in electroacoustic music differs significantly from that of instrumental music. In this context, spatiality is broad, multidimensional, and inherently difficult to define. It encompasses not only the individual sonic identity of each source—typically reproduced through loudspeakers—but also the relationships between those sources and the acoustic space in which they are heard. Critically, the audible experience of space unfolds through the temporal evolution of the sonic discourse itself.</p>
<p>Loudspeakers in electroacoustic music afford a uniquely flexible means of organizing space as a structural element. The spatial potential enabled by electroacoustic technologies has been, and continues to be, highly significant. With a single device (see Fig. 2), it is possible to transport the listener into a wide range of virtual environments, expanding the scope of musical space beyond the physical limitations of the performance venue. This opens the door to sonic representations of distance, movement, and directionality—factors that can be fully integrated into the musical structure and treated as compositional dimensions in their own right.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/images/space/fig-2-multichannel-setup-AI.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure 2: Multichannel electroacoustic concert hall. "><img src="../assets/images/space/fig-2-multichannel-setup-AI.png" class="img-fluid figure-img" alt="Figure 2: Multichannel electroacoustic concert hall. "></a></p>
<figcaption><strong>Figure 2</strong>: Multichannel electroacoustic concert hall.<br>
</figcaption>
</figure>
</div>
</section>
<section id="spatial-recontextualization-and-auditory-expectation" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="spatial-recontextualization-and-auditory-expectation"><span class="header-section-number">6.1.2</span> Spatial Recontextualization and Auditory Expectation</h3>
<p>One of the defining characteristics of electroacoustic music is its capacity to recontextualize sound. Through virtual spatialization, a sound can acquire new meaning depending on how it functions within diverse contexts. For example, two sounds that would never naturally coexist—such as a thunderstorm and a mechanical engine in the same acoustic scene—can be juxtaposed to create a landscape that defies ecological realism. This deliberate disjunction can trigger a complex interaction between what is heard and the listener’s prior knowledge of those sound sources, drawn from personal experience.</p>
<p>Our ability to interpret spatial cues in sound is deeply shaped by the patterns of interpersonal communication we engage in, the lived experience of urban or rural environments, and the architectural features of our surroundings. These formative influences are so embedded in our perceptual systems that we are often unaware of their role in shaping how we understand sensory information. Spatial cues are constantly processed in our everyday auditory experience and play a vital role in shaping our listening behaviors.</p>
<p>In the context of electroacoustic music, environmental cues may not only suggest associations with a physical location but also inform more abstract properties of the spatial discourse articulated by the piece. Jean-Claude Risset <span class="citation" data-cites="risett1969">(<a href="#ref-risett1969" role="doc-biblioref">Risett 1969</a>)</span> noted a tension in using environmental recordings in electroacoustic composition: the recognizable identity of “natural” sounds resists transformation without diminishing their spatial content. Nevertheless, identifiable sounds remain central to electroacoustic works, particularly those aligned with the tradition of musique concrète.</p>
<p>Under normal listening conditions, perception of a singular sound source is shaped by what Pierre Schaeffer <span class="citation" data-cites="schaeffer2003">(<a href="#ref-schaeffer2003" role="doc-biblioref">Schaeffer 2003</a>)</span> defined as the “sound object”—a sonic phenomenon perceived as a coherent whole, grasped through a form of reduced listening that focuses on the sound itself, independent of its origin or meaning. Our auditory system has evolved to identify and locate such sound objects in space by mapping them according to the physical attributes of their sources. A barking sound is understood as coming from a dog; a voice is mapped to a human speaker. As a result, the spatial interpretation of sound is closely linked to the listener’s expectations, shaped not only by the inherent acoustic characteristics of the signal but also by the listener’s accumulated learning and experience.</p>
<p>Unlike incidental listening in everyday life, attentive listening in electroacoustic music generally occurs from a fixed position. Apart from minor head movements, the spatial information available to the listener depends entirely on the acoustic cues encoded in the sound. While spatial hearing has been extensively researched in the field of psychoacoustics, its compositional potential as a carrier of musical form remains underexplored. In particular, the dimension of distance has received little attention compared to azimuth (horizontal angle) and elevation (vertical angle). These two spatial dimensions have been rigorously studied and well-documented, forming the basis of many standard models of spatial perception.</p>
<p>By contrast, the auditory perception of distance remains one of the most enigmatic topics in both music and psychophysics. It involves a complex array of cues—many of which are still not fully understood or integrated into compositional practice. As such, distance remains an open and promising field of inquiry for creative and scientific exploration alike <span class="citation" data-cites="abregu2012">(<a href="#ref-abregu2012" role="doc-biblioref">Abregú, Calcagno, and Vergara 2012</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/images/space/fig-3-space-xyz-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure 3: Schematic representation of the azimuth, elevation, and distance axes. "><img src="../assets/images/space/fig-3-space-xyz-2.png" class="img-fluid figure-img" alt="Figure 3: Schematic representation of the azimuth, elevation, and distance axes. "></a></p>
<figcaption><strong>Figure 3</strong>: Schematic representation of the azimuth, elevation, and distance axes.<br>
</figcaption>
</figure>
</div>
</section>
<section id="the-expanded-spatial-palette-of-electroacoustic-music" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="the-expanded-spatial-palette-of-electroacoustic-music"><span class="header-section-number">6.1.3</span> The Expanded Spatial Palette of Electroacoustic Music</h3>
<p>Unlike instrumental music, which relies on the physicality of pre-existing sound sources, electroacoustic music is not constrained by such limitations. This fundamental distinction allows composers not only to design entirely new sounds tailored to their spatial intentions but also to explore acoustic environments that defy conventional physical logic. While a performance of acoustic music offers visual and sonic cues grounded in the material world—a stage, a hall, visible instruments—electroacoustic music invites a more abstract engagement, where the listener navigates a virtual sound world.</p>
<p>In acoustic settings, physical space provides the listener with consistent spatial information. In contrast, electroacoustic environments—particularly those utilizing multichannel reproduction—radically extend the spatial canvas. These environments challenge and expand our auditory expectations, offering a broader spectrum of spatial strategies shaped by the interaction between sound diffusion technologies and the perceptual mechanisms of the listener.</p>
<p>Yet, this very potential also brings complications. The ephemeral and immaterial nature of loudspeaker-based sound makes it difficult to generate a spatial image as vivid or intuitive as that encountered in the physical world. Despite technological precision, virtual spatial environments often lack the tactile immediacy of real acoustic spaces.</p>
<p>Research in the field of computer music typically falls into two major domains: the study of spatial hearing and cognition, and the development of sound reproduction technologies. One of the advantages in electroacoustic contexts is the ability to isolate and manipulate spatial cues independently—a task nearly impossible with traditional acoustic instruments. This level of control opens up possibilities for treating space as a fully sculptable compositional parameter. However, practical results vary greatly, and many perceptual questions remain unresolved. It is within this intersection—where psychophysics, spatialization technologies, and compositional imagination meet—that the most fertile ground for innovation lies.</p>
<p>A classic example is John Chowning’s <em>Turenas</em> (1972), which employed quadraphonic speaker arrangements and a mathematical model to simulate virtual spatial motion. Chowning implemented a distance-dependent intensity multiplier according to the inverse square law, applied independently to each channel. The outcome was a virtual “phantom source” whose movement through space was dynamically shaped and perceptually engaging. Interestingly, Chowning discovered that simple, well-structured paths—such as basic geometric curves—produced the most convincing spatial gestures. In <em>On Sonic Art</em> <span class="citation" data-cites="wishart1996">(<a href="#ref-wishart1996" role="doc-biblioref">Wishart 1996</a>)</span>, one finds multiple visual representations of two-dimensional spatial trajectories; however, these diagrams often fail to translate into equally perceptible auditory experiences. For instance, while Lissajous curves are visually complex and elegant, their perceptual distinctiveness can be minimal. Clarity, it turns out, is key to audible spatial expression.</p>
<p>Since the mid-20th century, various spatialization systems have emerged to simulate acoustic spaces through loudspeaker arrays. Among the most notable are Intensity Panning, Ambisonics, and Dolby 5.1. Although a detailed technical comparison of these systems lies beyond the scope of this section (see <span class="citation" data-cites="basso2009">(<a href="#ref-basso2009" role="doc-biblioref">Basso, Di Liscia, and Pampin 2009</a>)</span> for an extensive overview), it is worth noting that electroacoustic space only becomes meaningful if its structural characteristics—such as reverberation—can be clearly perceived. In orchestral music, spatial features are often inherent to the instrument layout and performance context. With fixed instrument positions, the spatial configuration is relatively stable. In virtual space, however, reverberation must be explicitly created and shaped to define the acoustic character of the environment in which sonic events unfold.</p>
<p>Unlike the relatively fixed reverberation properties of physical rooms, virtual spaces offer remarkable plasticity. Composers can craft distinct acoustic identities for different sections of a piece, allowing each space to function as a compositional agent in its own right. This flexibility implies that in electroacoustic music, space must often be invented—not merely inherited from the sounding objects, as is typically the case with acoustic instruments.</p>
<p>It becomes clear, then, that spatial design can be a powerful structural element in music. Sound space carries poetic, aesthetic, and expressive dimensions. At the same time, convincing spatial construction often depends on our embodied experience of real-world acoustics. This dual grounding—in perceptual realism and creative abstraction—opens new theoretical and practical avenues for artists. The potential to reconceptualize spatiality through an expanded compositional lens can empower creators to engage more fully with the multidimensionality of sonic art. In doing so, it enables the development of richer analytical and taxonomical criteria that extend beyond sound itself, embracing broader cultural, perceptual, and technological contexts.</p>
</section>
</section>
<section id="spatial-localization-of-sound" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="spatial-localization-of-sound"><span class="header-section-number">6.2</span> Spatial Localization of Sound</h2>
<section id="psychoacoustic-factors-in-sound-localization" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="psychoacoustic-factors-in-sound-localization"><span class="header-section-number">6.2.1</span> Psychoacoustic Factors in Sound Localization</h3>
<p>When we perceive a sound event, we rely on a variety of auditory cues to infer the position of its source. These cues are associated with both <strong>direction</strong> and <strong>distance</strong>, and are typically classified into two main categories: <strong>binaural cues</strong>, which arise from comparisons between the two ears, and <strong>monaural cues</strong>, which are perceived by a single ear and are still critical for spatial localization.</p>
<p>To illustrate the binaural mechanism, consider a point sound source moving in a horizontal circular path around a listener’s head. At any given moment, the distance from the source to each ear will differ depending on the angular position of the source. This spatial configuration results in <strong>time and intensity differences</strong> between the two ears. These discrepancies are key to our spatial perception of sound.</p>
<p>When a sound wave reaches the right ear before the left, the slight delay in arrival time is known as the <strong>Interaural Time Difference (ITD)</strong>, while the difference in loudness is called the <strong>Interaural Level Difference (ILD)</strong>. These cues are extremely subtle—at 90°, the maximum ITD is around <strong>630 microseconds</strong>—yet the brain is remarkably adept at interpreting them to infer the lateral position of the source.</p>
<p>The ILD is largely caused by the <strong>acoustic shadowing effect of the head</strong>. For higher frequencies (with wavelengths shorter than the head’s diameter), the sound cannot diffract around the head, leading to more pronounced differences in intensity between the ears, depending on the angle of incidence.</p>
<p>These binaural cues work together most effectively between <strong>800 Hz and 1600 Hz</strong>. Below 800 Hz, ITD is the dominant cue; above 1600 Hz, ILD becomes more effective. The distance between the listener and the sound source also affects these cues, with ILD being particularly sensitive to distance changes, while ITD remains relatively stable.</p>
<p>Binaural localization does not require prior knowledge of the sound source. In contrast, <strong>monaural localization</strong>—when only one ear is used—often depends on recognizing the sound in advance. One key monaural cue is the <strong>Spectral Cue (also called the Monaural Spectral Cue)</strong>, which arises from changes in the sound spectrum caused by interactions with the outer ear (the pinna).</p>
<p>Even slight modifications in the signals reaching the auditory system can lead to significant shifts in the perceived spatial image. From an acoustic perspective, the <strong>pinna acts as a directional filter</strong>, primarily affecting high frequencies. It introduces spectral and temporal modifications to the signal depending on its angle of incidence and distance. These effects help the listener distinguish between sounds originating in front versus behind, and also assist in determining the <strong>elevation</strong> of the source.</p>
<p>Historically, the role of the pinnae in spatial hearing was underestimated, often seen merely as protective structures. Contemporary research, however, confirms their essential role in spatial perception and in reducing environmental noise, such as wind.</p>
<p>When it comes to estimating <strong>source distance</strong>, familiarity with the sound plays a significant role. For example, spoken voice at a normal volume provides a relatively accurate cue for distance perception. Like light intensity or visual size, <strong>sound intensity decreases with distance</strong>, following the <strong>inverse square law</strong>. But intensity is not the only factor: <strong>air acts as a low-pass filter</strong>, attenuating high frequencies over distance. This effect is clearly noticeable in scenarios like an approaching airplane, where the sound not only grows louder but also becomes brighter and richer in frequency content.</p>
<p>In sum, sound localization is a complex perceptual process informed by a combination of <strong>binaural and monaural cues</strong>, <strong>spectral filtering by the outer ear</strong>, and <strong>familiarity with the sound source</strong>. These mechanisms interact continuously to allow listeners to orient themselves in a dynamic acoustic environment, whether real or simulated.</p>
</section>
</section>
<section id="hearing-in-enclosed-spaces" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="hearing-in-enclosed-spaces"><span class="header-section-number">6.3</span> Hearing in Enclosed Spaces</h2>
<p>When we experience sound in an enclosed environment, the first auditory cue that reaches our ears is known as the <strong>direct sound</strong>—the signal that travels straight from the source without interacting with any obstacles. This is followed by the <strong>first-order reflections</strong>, which occur when the sound bounces off a single surface—such as a wall—before reaching the listener. These are then succeeded by <strong>second-order reflections</strong>, involving two bounces, and progressively higher orders of reflection. Eventually, this cascade of reflections gives rise to a <strong>diffuse auditory sensation</strong> known as <strong>reverberation</strong>.</p>
<p>In a typical empty room—consider one with four walls, a ceiling, and a floor—the six primary surfaces contribute to the first-order reflections. These early reflections are particularly significant when the sound has a sharp or impulsive attack, as they assist the auditory system in identifying the <strong>spatial position of the sound source</strong>. Reverberation, by contrast, carries information about the <strong>acoustic properties</strong> and <strong>dimensions</strong> of the room. It plays a key role in shaping the overall perception of the space’s material and geometry.</p>
<p>A critical metric used in room acoustics is the <strong>reverberation time</strong>, commonly denoted as <em>T₆₀</em>. This refers to the time it takes for the reverberant sound energy to decay by <strong>60 decibels</strong> after the original sound source has ceased. A widely used formula for estimating this value is the <strong>Sabine equation</strong>: <span class="math display">\[T_{60} = 0.16 \times \frac{V}{A}\]</span></p>
<p>Here, <strong>V</strong> represents the volume of the room in cubic meters, and <strong>A</strong> is the total <strong>equivalent absorption area</strong>, calculated as the sum of the products of each surface’s area and its corresponding <strong>absorption coefficient</strong>. Importantly, absorption coefficients are <strong>frequency-dependent</strong>, meaning that the acoustic behavior of a room varies with different sound frequencies.</p>
<p>To account for this, acousticians often use a metric known as the <strong>Noise Reduction Coefficient (NRC)</strong>, which averages absorption values across key frequencies—specifically 250 Hz, 500 Hz, 1000 Hz, and 2000 Hz. This provides a more practical estimate of how a room absorbs sound across the human auditory spectrum.</p>
<p>A graphical representation of room acoustics typically illustrates the <strong>temporal distribution of reflections</strong> produced by an impulse response within the space. In such plots, the <strong>height of each line</strong> corresponds to the <strong>relative amplitude</strong> of each reflection. These visualizations help researchers and designers understand how sound energy evolves over time in an architectural space, offering insight into both <strong>clarity</strong> and <strong>spatial impression</strong>.</p>
<p>Understanding how direct sound, early reflections, and reverberation interact is essential for both technical and artistic applications in sound design. Whether composing for multichannel electroacoustic environments or designing spatial sound installations, these acoustic principles are fundamental to crafting immersive and intelligible auditory experiences.</p>
</section>
<section id="virtual-source-simulation-via-stereophonic-techniques" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="virtual-source-simulation-via-stereophonic-techniques"><span class="header-section-number">6.4</span> Virtual Source Simulation via Stereophonic Techniques</h2>
<p>Stereophonic systems enable the simulation of virtual sound source localization using just two loudspeakers positioned to the left and right of the listener. By manipulating the amplitude of each signal sent to the speakers, we can generate the perceptual illusion of a sound source moving laterally across the auditory field. Additional transformations also allow us to simulate depth and even create the sensation of a source emanating from behind the speakers, constructing an illusory auditory space.</p>
<p>The perception of a virtual source positioned between two speakers arises from the system’s ability to artificially generate an <strong>Interaural Level Difference (ILD)</strong>. If the signal emitted from the right speaker is louder than that from the left, the right ear perceives a higher intensity, prompting the auditory system to localize the source toward the right. The degree of perceived lateral displacement is directly proportional to the intensity difference.</p>
<p>In typical stereo setups, the speakers are arranged with a 60° separation. However, for purposes of spatial expansion and compatibility with quadraphonic systems (involving four speakers surrounding the listener), we assume a 90° separation. We define 0° as the location of the left speaker and 90° as the right. The objective is to calculate the appropriate amplitude for each speaker to position the virtual source at a desired angle θ within this range.</p>
<p>To ensure the virtual source appears equidistant from the listener during its movement, the <strong>total sound intensity</strong> emitted by both speakers must remain constant. That is:</p>
<p><span class="math display">\[
I_L + I_R = k
\]</span></p>
<p>Assuming intensity is normalized between 0 and 1, we can simplify this to:</p>
<p><span class="math display">\[
I_L + I_R = 1
\]</span></p>
<p>Here, <span class="math inline">\(I_L\)</span> and <span class="math inline">\(I_R\)</span> represent the intensities emitted by the left and right speakers, respectively. When the source is fully localized at the left, <span class="math inline">\(I_L = 1\)</span> and <span class="math inline">\(I_R = 0\)</span>; when at the right, <span class="math inline">\(I_L = 0\)</span> and <span class="math inline">\(I_R = 1\)</span>; and when centered, both are <span class="math inline">\(0.5\)</span>. However, in most digital audio systems we operate on <strong>amplitude</strong>, not intensity. Since <strong>intensity is proportional to the square of amplitude</strong>, we use the relation:</p>
<p><span class="math display">\[
I \propto A^2
\]</span></p>
<p>Thus, the condition of constant intensity becomes:</p>
<p><span class="math display">\[
A_L^2 + A_R^2 = 1
\]</span></p>
<p>A common misconception is to linearly scale amplitudes (e.g., <span class="math inline">\(A_L = A_R = 0.5\)</span> when centered), but this would yield:</p>
<p><span class="math display">\[
0.5^2 + 0.5^2 = 0.25 + 0.25 = 0.5
\]</span></p>
<p>—only half the intended total intensity, which could be mistakenly perceived as an increase in distance. To preserve consistent intensity across the stereo field, we rely on a trigonometric identity:</p>
<p><span class="math display">\[
\sin^2(\theta) + \cos^2(\theta) = 1
\]</span></p>
<p>Using this, we can define the <strong>amplitude assignments</strong> as:</p>
<p><span class="math display">\[
A_R = \sin(\theta) \\
A_L = \cos(\theta)
\]</span></p>
<p>Where θ ranges from 0° (left) to 90° (right). For example, at 45°, both <span class="math inline">\(sin(45^\circ)\)</span> and <span class="math inline">\(cos(45^\circ)\)</span> equal approximately 0.707. Squaring and summing these values confirms that the total intensity remains 1:</p>
<p><span class="math display">\[
A_R^2 + A_L^2 = \sin^2(45^\circ) + \cos^2(45^\circ) = 0.5 + 0.5 = 1
\]</span></p>
<p>Thus, this method preserves the <strong>perceptual coherence</strong> of the virtual sound source’s position while maintaining a <strong>constant energy output</strong>, a critical aspect of spatial audio realism.</p>
<p>In summary, for any virtual angle <span class="math inline">\(\theta \in [0^\circ, 90^\circ]\)</span>, we can accurately simulate spatial position with consistent intensity using:</p>
<p><span class="math display">\[
A_R = \sin(\theta) \\
A_L = \cos(\theta)
\]</span></p>
<p>This technique forms the basis for further expansion into quadraphonic and multichannel spatialization systems.</p>
</section>
<section id="implementing-a-stereo-spatialization-in-pure-data" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="implementing-a-stereo-spatialization-in-pure-data"><span class="header-section-number">6.5</span> Implementing a Stereo Spatialization in Pure Data</h2>
<p>Before diving into the implementation of a stereophonic spatialization system in Pure Data (Pd), it is essential to note that the objects used to compute trigonometric functions such as sine and cosine require the input angle to be expressed in <strong>radians</strong>, not degrees. Recall that one radian is the angle formed when the length of the arc of a circle equals the radius. From this definition, we derive the following equivalences:</p>
<ul>
<li>360° = <span class="math inline">\(2\pi\)</span> radians<br>
</li>
<li>180° = <span class="math inline">\(\pi\)</span> radians<br>
</li>
<li>90° = <span class="math inline">\(\frac{\pi}{2}\)</span> radians</li>
</ul>
<p>To convert degrees to radians, we multiply the angle in degrees by the ratio (<span class="math inline">\(\frac{2\pi}{360}\)</span>). To optimize performance and avoid redundant calculations, we can precompute this factor:</p>
<p><span class="math display">\[
\text{Angle}_{\text{rad}} = \text{Angle}_{\text{deg}} \times \frac{2\pi}{360} = \text{Angle}_{\text{deg}} \times 0.0174533
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/screenshots/space/intenssity-panning.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Fig."><img src="../assets/screenshots/space/intenssity-panning.png" class="img-fluid figure-img" style="width:50.0%" alt="Fig."></a></p>
<figcaption>Fig.</figcaption>
</figure>
</div>
<p>The patch illustrated below simulates a virtual sound source using stereophonic panning. This technique, commonly referred to as <em>intensity panning</em>, adjusts the balance between two loudspeakers. The source signa is split into two channels, each multiplied by the cosine and sine of the specified angle (in radians) to distribute the amplitude between the left and right speakers. As we vary the angle between 0° and 90°, the source appears to move along an arc, emulating the listener’s perception of spatial position.</p>
<p>This Pure Data patch demonstrates the fundamental principles of stereophonic spatialization through intensity panning, implementing the mathematical framework for creating virtual sound source positioning between two speakers. By applying trigonometric amplitude control to left and right audio channels, the patch enables precise localization of monophonic sources within a 90-degree stereo field while maintaining constant total energy output. This approach forms the foundation for understanding spatial audio processing and serves as a building block for more complex multichannel spatialization systems.</p>
<p>The patch provides real-time control over angular positioning through intuitive slider interfaces, allowing users to experience the direct relationship between mathematical spatial calculations and perceptual sound localization. This implementation serves both educational and practical purposes, demonstrating how theoretical acoustic principles translate into functional audio processing tools.</p>
<section id="patch-overview" class="level3" data-number="6.5.1">
<h3 data-number="6.5.1" class="anchored" data-anchor-id="patch-overview"><span class="header-section-number">6.5.1</span> Patch Overview</h3>
<p>The intensity panning system implements a complete stereophonic spatialization pipeline consisting of four primary processing stages:</p>
<ol type="1">
<li><strong>Angle Input and Conversion</strong>: Manages user input and degree-to-radian conversion</li>
<li><strong>Trigonometric Calculation</strong>: Computes sine and cosine values for amplitude control</li>
<li><strong>Audio Generation and Processing</strong>: Creates test signals and applies spatial positioning</li>
<li><strong>Stereo Output</strong>: Delivers positioned audio to left and right channels</li>
</ol>
<p>The patch demonstrates the essential mathematical relationship where spatial positioning is achieved through complementary amplitude control, ensuring that the virtual source maintains consistent perceived loudness while moving between speakers.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A[Angle Input 0-90°] --&gt; B[Degree to Radian Conversion]
    B --&gt; C[Trigonometric Calculation]
    C --&gt; D[Sine/Cosine Values]
    D --&gt; E[Audio Signal Generation]
    E --&gt; F[Amplitude Modulation]
    F --&gt; G[Left Channel Output]
    F --&gt; H[Right Channel Output]
    I[Test Oscillator] --&gt; E
    
    style A fill:#e1f5fe
    style G fill:#f3e5f5
    style H fill:#f3e5f5
    style C fill:#fff3e0
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="key-objects-and-their-roles" class="level3" data-number="6.5.2">
<h3 data-number="6.5.2" class="anchored" data-anchor-id="key-objects-and-their-roles"><span class="header-section-number">6.5.2</span> Key Objects and Their Roles</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 25%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th>Object</th>
<th>Function</th>
<th>Role in Spatialization</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>hsl</code></td>
<td>Horizontal slider</td>
<td>User interface for angle input (0-90 degrees)</td>
</tr>
<tr class="even">
<td><code>* 0.0174533</code></td>
<td>Conversion multiplier</td>
<td>Converts degrees to radians for trigonometric functions</td>
</tr>
<tr class="odd">
<td><code>expr cos($f1) \; sin($f1)</code></td>
<td>Trigonometric processor</td>
<td>Calculates complementary amplitude values</td>
</tr>
<tr class="even">
<td><code>phasor~ 220</code></td>
<td>Audio oscillator</td>
<td>Generates test signal for spatialization demonstration</td>
</tr>
<tr class="odd">
<td><code>expr~ $v1*$v2 \; $v1*$v3</code></td>
<td>Audio multiplier</td>
<td>Applies trigonometric amplitude control to audio</td>
</tr>
<tr class="even">
<td><code>vsl</code></td>
<td>Vertical slider</td>
<td>Visual feedback for calculated amplitude values</td>
</tr>
<tr class="odd">
<td><code>output~</code></td>
<td>Audio output</td>
<td>Delivers spatialized stereo audio</td>
</tr>
</tbody>
</table>
<section id="critical-processing-components" class="level4" data-number="6.5.2.1">
<h4 data-number="6.5.2.1" class="anchored" data-anchor-id="critical-processing-components"><span class="header-section-number">6.5.2.1</span> Critical Processing Components</h4>
<p><strong>Degree-to-Radian Conversion</strong>: The multiplication factor <code>0.0174533</code> represents the mathematical constant π/180, enabling conversion from user-friendly degree input to the radian values required by Pure Data’s trigonometric functions. This conversion ensures precise mathematical calculation while maintaining intuitive user control.</p>
<p><strong>Trigonometric Expression</strong>: The <code>expr cos($f1) \; sin($f1)</code> object simultaneously calculates both amplitude coefficients needed for stereophonic positioning. The cosine output controls left channel amplitude, while sine controls right channel amplitude, creating the complementary relationship essential for constant-energy panning.</p>
<p><strong>Audio Expression Multiplier</strong>: The <code>expr~ $v1*$v2 \; $v1*$v3</code> object performs real-time multiplication of the audio signal with the calculated trigonometric values, applying the spatial positioning in the audio domain while maintaining sample-accurate timing.</p>
</section>
</section>
<section id="data-flow" class="level3" data-number="6.5.3">
<h3 data-number="6.5.3" class="anchored" data-anchor-id="data-flow"><span class="header-section-number">6.5.3</span> Data Flow</h3>
<p>The stereophonic intensity panning patch orchestrates a precise mathematical transformation pipeline that converts angular positioning information into spatially positioned audio output through systematic trigonometric processing. This implementation demonstrates how theoretical spatial audio principles translate into practical real-time audio processing systems that maintain both mathematical accuracy and perceptual coherence.</p>
<p>The angle input and preprocessing stage establishes the foundation for spatial positioning through user interface management and mathematical preparation. The horizontal slider (<code>hsl</code>) provides intuitive control over the desired angular position, accepting input values in the familiar degree scale from 0° to 90°, where 0° represents full left positioning and 90° represents full right positioning. This degree-based input undergoes immediate conversion to radians through multiplication by the constant 0.0174533 (π/180), transforming the user-friendly angular measurement into the mathematical format required by Pure Data’s trigonometric functions. The conversion process ensures that subsequent calculations maintain precision while preserving the intuitive relationship between slider position and spatial location. The radian values flow directly to the trigonometric processing stage, maintaining real-time responsiveness that enables immediate auditory feedback as users adjust the angular positioning control.</p>
<p>The trigonometric calculation stage represents the mathematical core of the spatialization process, where angular information becomes amplitude control data through sophisticated mathematical processing. The <code>expr cos($f1) \; sin($f1)</code> object performs simultaneous calculation of both cosine and sine functions for the input angle, generating the complementary amplitude coefficients essential for stereophonic positioning. The cosine output provides the left channel amplitude coefficient, following the mathematical convention where cos(0°) = 1 and cos(90°) = 0, creating maximum left channel output when the angle is at 0° and minimum output at 90°. Conversely, the sine output generates the right channel amplitude coefficient, where sin(0°) = 0 and sin(90°) = 1, producing the inverse relationship necessary for right channel control. This mathematical relationship ensures that the fundamental trigonometric identity sin²(θ) + cos²(θ) = 1 is preserved, maintaining constant total energy across the stereo field regardless of angular position.</p>
<p>The audio signal generation and modulation stage transforms the calculated amplitude coefficients into actual spatial positioning through real-time audio processing. The <code>phasor~ 220</code> object generates a continuous sawtooth wave at 220 Hz, providing a harmonically rich test signal that clearly demonstrates the spatial positioning effects across the frequency spectrum. This test signal enters the <code>expr~ $v1*$v2 \; $v1*$v3</code> audio expression object, where it undergoes simultaneous multiplication with both trigonometric amplitude coefficients. The first output (<span class="math inline">\(v1*\)</span>v2) produces the left channel audio by multiplying the source signal with the cosine amplitude value, while the second output (<span class="math inline">\(v1*\)</span>v3) creates the right channel audio through multiplication with the sine amplitude value. This dual multiplication process applies the spatial positioning in real-time while maintaining sample-accurate synchronization between channels, ensuring that the stereo image remains stable and coherent.</p>
<p>The feedback and output stage provides both visual monitoring and audio delivery systems that enable users to understand and experience the spatialization process. The vertical sliders (<code>vsl</code>) connected to the trigonometric outputs provide real-time visual feedback of the calculated amplitude values, allowing users to observe how the complementary relationship between left and right coefficients changes as the angle varies. This visual feedback reinforces the mathematical principles underlying the spatialization process and helps users understand the relationship between angular input and amplitude distribution. The <code>output~</code> object delivers the final spatialized audio to the system’s audio interface, enabling immediate auditory perception of the spatial positioning effects. The stereo output maintains the precise amplitude relationships calculated by the trigonometric functions, ensuring that the perceived spatial position corresponds accurately to the input angle while preserving the constant energy principle essential for realistic spatial audio.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    A[Angle Input] --&gt; B[Radian Conversion]
    B --&gt; C[Trigonometric Calculation]
    C --&gt; D[Audio Modulation]
    D --&gt; E[Stereo Output]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="processing-chain-details" class="level3" data-number="6.5.4">
<h3 data-number="6.5.4" class="anchored" data-anchor-id="processing-chain-details"><span class="header-section-number">6.5.4</span> Processing Chain Details</h3>
<p>The spatialization system maintains mathematical precision through several coordinated processing stages:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 29%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>Input</th>
<th>Process</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Angle (0-90°)</td>
<td>Degree to radian conversion</td>
<td>Radian values</td>
</tr>
<tr class="even">
<td>2</td>
<td>Radian angle</td>
<td>Trigonometric calculation</td>
<td>Cosine and sine coefficients</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Test audio + coefficients</td>
<td>Real-time multiplication</td>
<td>Left and right audio channels</td>
</tr>
<tr class="even">
<td>4</td>
<td>Stereo channels</td>
<td>Audio interface output</td>
<td>Spatialized audio</td>
</tr>
</tbody>
</table>
<section id="mathematical-relationships" class="level4" data-number="6.5.4.1">
<h4 data-number="6.5.4.1" class="anchored" data-anchor-id="mathematical-relationships"><span class="header-section-number">6.5.4.1</span> Mathematical Relationships</h4>
<p>The patch implements the fundamental stereophonic panning equations:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 42%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Mathematical Function</th>
<th>Audio Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Left Amplitude</td>
<td><span class="math inline">\(A_L = \cos(\theta)\)</span></td>
<td>Multiply source signal by cosine value</td>
</tr>
<tr class="even">
<td>Right Amplitude</td>
<td><span class="math inline">\(A_R = \sin(\theta)\)</span></td>
<td>Multiply source signal by sine value</td>
</tr>
<tr class="odd">
<td>Energy Conservation</td>
<td><span class="math inline">\(A_L^2 + A_R^2 = 1\)</span></td>
<td>Maintains constant perceived loudness</td>
</tr>
<tr class="even">
<td>Angular Range</td>
<td><span class="math inline">\(\theta \in [0°, 90°]\)</span></td>
<td>Maps to full left-right stereo field</td>
</tr>
</tbody>
</table>
</section>
<section id="control-parameter-mapping" class="level4" data-number="6.5.4.2">
<h4 data-number="6.5.4.2" class="anchored" data-anchor-id="control-parameter-mapping"><span class="header-section-number">6.5.4.2</span> Control Parameter Mapping</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 15%">
<col style="width: 16%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Slider Position</th>
<th>Angle (Degrees)</th>
<th>Angle (Radians)</th>
<th>Left Amplitude</th>
<th>Right Amplitude</th>
<th>Perceived Position</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0°</td>
<td>0.000</td>
<td>1.000</td>
<td>0.000</td>
<td>Full Left</td>
</tr>
<tr class="even">
<td>0.25</td>
<td>22.5°</td>
<td>0.393</td>
<td>0.924</td>
<td>0.383</td>
<td>Left-Center</td>
</tr>
<tr class="odd">
<td>0.5</td>
<td>45°</td>
<td>0.785</td>
<td>0.707</td>
<td>0.707</td>
<td>Center</td>
</tr>
<tr class="even">
<td>0.75</td>
<td>67.5°</td>
<td>1.178</td>
<td>0.383</td>
<td>0.924</td>
<td>Right-Center</td>
</tr>
<tr class="odd">
<td>1.0</td>
<td>90°</td>
<td>1.571</td>
<td>0.000</td>
<td>1.000</td>
<td>Full Right</td>
</tr>
</tbody>
</table>
</section>
<section id="signal-processing-verification" class="level4" data-number="6.5.4.3">
<h4 data-number="6.5.4.3" class="anchored" data-anchor-id="signal-processing-verification"><span class="header-section-number">6.5.4.3</span> Signal Processing Verification</h4>
<p>The patch maintains signal integrity through:</p>
<ul>
<li><strong>Constant Energy Preservation</strong>: Total amplitude squared remains unity across all positions</li>
<li><strong>Phase Coherence</strong>: Both channels maintain identical phase relationships</li>
<li><strong>Amplitude Linearity</strong>: Trigonometric functions provide smooth transitions between positions</li>
<li><strong>Real-time Responsiveness</strong>: Sample-accurate processing without latency or artifacts</li>
</ul>
</section>
</section>
<section id="creative-applications" class="level3" data-number="6.5.5">
<h3 data-number="6.5.5" class="anchored" data-anchor-id="creative-applications"><span class="header-section-number">6.5.5</span> Creative Applications</h3>
<ul>
<li><strong>Interactive Spatial Composition</strong>: Use MIDI controllers or OSC input to automate the angle parameter, creating dynamic spatial movement within musical compositions.</li>
<li><strong>Binaural Audio Production</strong>: Combine with headphone processing to create immersive spatial experiences for stereo headphone listeners.</li>
<li><strong>Multi-source Spatialization</strong>: Duplicate the patch architecture to control multiple independent sound sources within the same stereo field.</li>
<li><strong>Spatial Modulation Effects</strong>: Apply low-frequency oscillators (LFOs) to the angle parameter to create tremolo-like spatial movement effects.</li>
<li><strong>Performance Control Systems</strong>: Integrate with physical controllers, sensors, or computer vision systems to map performer gestures to spatial positioning.</li>
<li><strong>Educational Demonstrations</strong>: Use as a teaching tool to illustrate psychoacoustic principles, trigonometric relationships, and spatial audio mathematics.</li>
<li><strong>Sound Design Applications</strong>: Create spatial effects for film, game audio, or immersive media by precisely controlling source positioning.</li>
<li><strong>Installation Art</strong>: Implement in gallery spaces where visitor proximity or movement controls the spatial positioning of ambient sounds.</li>
</ul>
</section>
</section>
<section id="simulating-distance" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="simulating-distance"><span class="header-section-number">6.6</span> Simulating Distance</h2>
<p>To simulate distance, we must account for how amplitude diminishes as the source moves away from the listener. Unlike intensity, which decreases with the square of the distance, amplitude is inversely proportional to the distance:</p>
<p><span class="math display">\[
A \propto \frac{1}{d}
\]</span></p>
<p>Thus, the amplitude perceived by the listener is:</p>
<p><span class="math display">\[
A_{\text{perceived}} = \frac{A_{\text{source}}}{d}
\]</span></p>
<p>To manage both the <strong>angle of lateralization</strong> and the <strong>distance to the source</strong> simultaneously, we use the <code>slider2d</code> object from the <code>ELSE</code> library. This object provides a two-dimensional matrix that captures mouse pointer movement and returns the corresponding (x, y) coordinates.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/screenshots/space/distance-panning.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Fig."><img src="../assets/screenshots/space/distance-panning.png" class="img-fluid figure-img" style="width:80.0%" alt="Fig."></a></p>
<figcaption>Fig.</figcaption>
</figure>
</div>
<p>We map the horizontal axis (x) to the angle (0°–90°) and the vertical axis (y) to the distance (1-10). These values are then routed via <code>[send]</code> and <code>[receive]</code> objects for use throughout the patch. It’s crucial to prevent division by zero in distance calculations, so we define the minimum distance as 1—equivalent to the baseline distance between the listener and speakers. If speakers are placed 2 meters away, then a distance of <span class="math inline">\(d = 1\)</span> equals 2 meters in real space, <span class="math inline">\(d = 2\)</span> would correspond to 4 meters, and so on. In this system, <strong>distance is relative to the speaker-listener spacing</strong>.</p>
<p>This Pure Data patch extends the fundamental stereophonic panning system by introducing interactive two-dimensional spatial control through a visual interface. By combining angular positioning with distance modeling, the patch creates a more sophisticated spatialization system that simulates both the horizontal positioning and depth perception of virtual sound sources. The implementation demonstrates how distance affects amplitude through inverse-square law approximations while maintaining the trigonometric relationships essential for accurate stereo positioning.</p>
<p>The patch integrates real-time visual control through a 2D slider interface that enables intuitive manipulation of both angular position and source distance simultaneously. This approach provides immediate tactile feedback for spatial audio composition while implementing realistic amplitude scaling based on distance relationships that mirror natural acoustic behavior.</p>
<section id="patch-overview-1" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="patch-overview-1"><span class="header-section-number">6.6.1</span> Patch Overview</h3>
<p>The interactive distance panning system implements a comprehensive spatial audio control interface consisting of five primary processing stages:</p>
<ol type="1">
<li><strong>Two-Dimensional Input Control</strong>: Visual interface for simultaneous angle and distance manipulation</li>
<li><strong>Coordinate Conversion</strong>: Cartesian to polar coordinate transformation with parameter scaling</li>
<li><strong>Distance Modeling</strong>: Amplitude scaling based on acoustic distance relationships</li>
<li><strong>Angular Processing</strong>: Trigonometric calculation for stereophonic positioning</li>
<li><strong>Audio Synthesis and Output</strong>: Signal generation with spatial and distance-based amplitude control</li>
</ol>
<p>The patch demonstrates advanced Pure Data interface design by combining multiple control paradigms within a single cohesive system that maintains both mathematical precision and intuitive user interaction.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A[2D Slider Interface] --&gt; B[Coordinate Unpacking]
    B --&gt; C[Angle Calculation 0-90°]
    B --&gt; D[Distance Extraction 1-10]
    C --&gt; E[Radian Conversion]
    E --&gt; F[Trigonometric Processing]
    D --&gt; G[Distance Amplitude Scaling]
    F --&gt; H[Spatial Audio Mixing]
    G --&gt; H
    I[Test Oscillator] --&gt; J[Distance Division]
    J --&gt; H
    H --&gt; K[Stereo Output]
    
    style A fill:#e1f5fe
    style K fill:#f3e5f5
    style F fill:#fff3e0
    style G fill:#fff3e0
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="key-objects-and-their-roles-1" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="key-objects-and-their-roles-1"><span class="header-section-number">6.6.2</span> Key Objects and Their Roles</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 23%">
<col style="width: 57%">
</colgroup>
<thead>
<tr class="header">
<th>Object</th>
<th>Function</th>
<th>Role in Spatial Control</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>slider2d</code></td>
<td>Two-dimensional interface</td>
<td>Provides x,y coordinate input for angle and distance</td>
</tr>
<tr class="even">
<td><code>unpack f f</code></td>
<td>Coordinate separation</td>
<td>Splits 2D coordinates into independent x,y values</td>
</tr>
<tr class="odd">
<td><code>* 90</code></td>
<td>Angular scaling</td>
<td>Converts normalized x-coordinate to 0-90° range</td>
</tr>
<tr class="even">
<td><code>s distance</code> / <code>r distance</code></td>
<td>Distance communication</td>
<td>Distributes distance parameter across patch</td>
</tr>
<tr class="odd">
<td><code>expr~ $v1 / (($v2*9)+1)</code></td>
<td>Distance amplitude modeling</td>
<td>Applies inverse relationship for distance attenuation</td>
</tr>
<tr class="even">
<td><code>expr cos($f1) \; sin($f1)</code></td>
<td>Trigonometric processor</td>
<td>Maintains stereophonic positioning calculations</td>
</tr>
<tr class="odd">
<td><code>expr~ $v1*$v2 \; $v1*$v3</code></td>
<td>Spatial audio multiplier</td>
<td>Applies both distance and angular amplitude control</td>
</tr>
</tbody>
</table>
<section id="critical-processing-components-1" class="level4" data-number="6.6.2.1">
<h4 data-number="6.6.2.1" class="anchored" data-anchor-id="critical-processing-components-1"><span class="header-section-number">6.6.2.1</span> Critical Processing Components</h4>
<p><strong>slider2d Interface</strong>: This object provides the primary user interaction mechanism, outputting normalized x,y coordinates in the range 0.0-1.0. The horizontal axis (x) controls angular positioning while the vertical axis (y) determines distance scaling, creating an intuitive two-dimensional spatial control surface.</p>
<p><strong>Coordinate Transformation</strong>: The <code>unpack f f</code> object separates the 2D coordinates, enabling independent processing of angular and distance parameters. The x-coordinate undergoes multiplication by 90 to generate the 0-90° angular range required for stereophonic panning.</p>
<p><strong>Distance Amplitude Expression</strong>: The <code>expr~ $v1 / (($v2*9)+1)</code> object implements a distance-based amplitude scaling algorithm that approximates acoustic distance attenuation. The multiplication by 9 and addition of 1 creates a distance range from 1 to 10, preventing division by zero while providing meaningful amplitude variation.</p>
</section>
</section>
<section id="data-flow-1" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="data-flow-1"><span class="header-section-number">6.6.3</span> Data Flow</h3>
<p>The interactive distance panning patch orchestrates a sophisticated multi-dimensional control system that transforms two-dimensional interface input into spatially accurate audio output through coordinated processing of angular positioning and distance modeling. This implementation demonstrates how visual interface design can be integrated with acoustic modeling to create intuitive spatial audio control systems that maintain both mathematical precision and perceptual accuracy.</p>
<p>The initial input coordination and parameter extraction stage establishes the foundation for spatial control through the comprehensive management of the two-dimensional slider interface. The <code>slider2d</code> object captures user interaction as normalized x,y coordinate pairs ranging from 0.0 to 1.0, where the horizontal axis represents angular positioning information and the vertical axis encodes distance relationship data. Upon initialization, the <code>loadbang</code> and <code>range 0 1</code> message ensure proper scaling configuration, establishing the coordinate system boundaries that will govern subsequent parameter mapping. The coordinate unpacking process through <code>unpack f f</code> separates the combined spatial input into independent data streams that can be processed according to their specific roles in the spatialization algorithm. This separation enables parallel processing of angular and distance information while maintaining the temporal relationship between user input and system response that is essential for real-time spatial audio manipulation.</p>
<p>The angular conversion and trigonometric processing stage transforms the horizontal coordinate information into the angular parameters required for stereophonic positioning calculations. The x-coordinate from the 2D slider undergoes scaling multiplication by 90, converting the normalized 0.0-1.0 range into the 0-90° angular span that defines the complete left-right stereo field. This angular value proceeds through the established degree-to-radian conversion process via multiplication by 0.0174533, preparing the data for the trigonometric calculations performed by the <code>expr cos($f1) \; sin($f1)</code> object. The trigonometric processing generates the complementary amplitude coefficients essential for stereophonic positioning, where the cosine output controls left channel amplitude and the sine output controls right channel amplitude. This processing stage maintains the mathematical relationships established in previous spatial audio implementations while adapting to the dynamic input provided by the interactive interface system.</p>
<p>The distance modeling and amplitude scaling stage implements acoustic distance relationships through sophisticated amplitude manipulation that simulates realistic sound propagation characteristics. The y-coordinate from the 2D slider interface undergoes processing through the send/receive system (<code>s distance</code> / <code>r distance</code>) that distributes the distance parameter to multiple processing locations within the patch. The distance value enters the <code>expr~ $v1 / (($v2*9)+1)</code> expression, which implements an approximation of inverse-square law amplitude scaling where increasing distance values produce proportionally decreased amplitude output. The mathematical relationship <code>($v2*9)+1</code> creates a distance scaling factor that ranges from 1 (minimum distance, maximum amplitude) to 10 (maximum distance, minimum amplitude), preventing division by zero while providing meaningful amplitude variation across the available control range. This distance-based amplitude scaling applies to the test oscillator signal before it undergoes spatial positioning through the trigonometric multiplication stage.</p>
<p>The audio synthesis and spatial integration stage combines the distance-scaled audio signal with the angular positioning coefficients to create the final spatialized output with realistic distance characteristics. The <code>phasor~ 220</code> object generates a harmonically rich test signal that undergoes distance-based amplitude scaling through the division expression before entering the spatial processing chain. The <code>expr~ $v1*$v2 \; $v1*$v3</code> object performs the critical multiplication that applies both the distance attenuation and the trigonometric spatial positioning simultaneously, creating stereo output channels that reflect both the angular position and distance of the virtual sound source. The left channel output results from multiplication with the cosine amplitude coefficient modified by distance scaling, while the right channel output employs the sine amplitude coefficient similarly modified by distance attenuation. This dual processing approach ensures that the final audio output maintains both the spatial positioning accuracy established by the trigonometric relationships and the amplitude scaling appropriate for the simulated distance.</p>
<p>The visual feedback and system monitoring stage provides real-time indication of the calculated spatial parameters through integrated display elements that enable users to understand the relationship between interface input and audio processing parameters. The numerical display objects connected to various processing stages show the current angular values in both degrees and radians, the extracted distance parameter, and the calculated amplitude coefficients for both stereo channels. This visual feedback system enables users to develop understanding of the mathematical relationships underlying the spatial audio processing while providing confirmation that the interface input is correctly translated into audio processing parameters. The continuous visual update of these parameters during real-time interaction creates an educational environment where users can observe the direct correspondence between spatial positioning concepts and their implementation in digital audio processing systems.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    A[2D Interface Input] --&gt; B[Coordinate Processing]
    B --&gt; C[Angular Conversion]
    B --&gt; D[Distance Scaling]
    C --&gt; E[Trigonometric Calculation]
    D --&gt; F[Amplitude Attenuation]
    E --&gt; G[Spatial Audio Output]
    F --&gt; G
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="processing-chain-details-1" class="level3" data-number="6.6.4">
<h3 data-number="6.6.4" class="anchored" data-anchor-id="processing-chain-details-1"><span class="header-section-number">6.6.4</span> Processing Chain Details</h3>
<p>The distance panning system maintains mathematical precision through several coordinated processing stages that integrate spatial positioning with acoustic distance modeling:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 29%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>Input</th>
<th>Process</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2D Coordinates (0-1)</td>
<td>Interface capture and unpacking</td>
<td>Separated x,y values</td>
</tr>
<tr class="even">
<td>2</td>
<td>X-coordinate</td>
<td>Multiplication by 90</td>
<td>Angular degrees (0-90°)</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Y-coordinate</td>
<td>Distance parameter extraction</td>
<td>Distance factor (0-1)</td>
</tr>
<tr class="even">
<td>4</td>
<td>Angular degrees</td>
<td>Radian conversion and trigonometry</td>
<td>Cosine/sine coefficients</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Distance factor</td>
<td>Amplitude scaling calculation</td>
<td>Distance attenuation</td>
</tr>
<tr class="even">
<td>6</td>
<td>Audio + coefficients + distance</td>
<td>Combined spatial and distance processing</td>
<td>Spatialized stereo output</td>
</tr>
</tbody>
</table>
<section id="mathematical-relationships-1" class="level4" data-number="6.6.4.1">
<h4 data-number="6.6.4.1" class="anchored" data-anchor-id="mathematical-relationships-1"><span class="header-section-number">6.6.4.1</span> Mathematical Relationships</h4>
<p>The patch implements integrated spatial and distance modeling through coordinated mathematical relationships:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 42%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Mathematical Function</th>
<th>Audio Application</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Angular Position</td>
<td><span class="math inline">\(\theta = x \times 90°\)</span></td>
<td>Maps horizontal slider to stereo field</td>
</tr>
<tr class="even">
<td>Left Amplitude</td>
<td><span class="math inline">\(A_L = \cos(\theta)\)</span></td>
<td>Spatial positioning coefficient</td>
</tr>
<tr class="odd">
<td>Right Amplitude</td>
<td><span class="math inline">\(A_R = \sin(\theta)\)</span></td>
<td>Spatial positioning coefficient</td>
</tr>
<tr class="even">
<td>Distance Scaling</td>
<td><span class="math inline">\(A_d = \frac{1}{(d \times 9) + 1}\)</span></td>
<td>Inverse distance attenuation</td>
</tr>
<tr class="odd">
<td>Final Left Channel</td>
<td><span class="math inline">\(L = \text{signal} \times A_d \times A_L\)</span></td>
<td>Combined distance and spatial processing</td>
</tr>
<tr class="even">
<td>Final Right Channel</td>
<td><span class="math inline">\(R = \text{signal} \times A_d \times A_R\)</span></td>
<td>Combined distance and spatial processing</td>
</tr>
</tbody>
</table>
</section>
<section id="control-parameter-mapping-1" class="level4" data-number="6.6.4.2">
<h4 data-number="6.6.4.2" class="anchored" data-anchor-id="control-parameter-mapping-1"><span class="header-section-number">6.6.4.2</span> Control Parameter Mapping</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 14%">
<col style="width: 15%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Interface Position</th>
<th>Angle (Degrees)</th>
<th>Distance Factor</th>
<th>Left Amplitude</th>
<th>Right Amplitude</th>
<th>Distance Attenuation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(0.0, 0.0)</td>
<td>0°</td>
<td>1 (close)</td>
<td>1.000</td>
<td>0.000</td>
<td>1.000</td>
</tr>
<tr class="even">
<td>(0.5, 0.0)</td>
<td>45°</td>
<td>1 (close)</td>
<td>0.707</td>
<td>0.707</td>
<td>1.000</td>
</tr>
<tr class="odd">
<td>(1.0, 0.0)</td>
<td>90°</td>
<td>1 (close)</td>
<td>0.000</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr class="even">
<td>(0.0, 1.0)</td>
<td>0°</td>
<td>10 (far)</td>
<td>1.000</td>
<td>0.000</td>
<td>0.100</td>
</tr>
<tr class="odd">
<td>(0.5, 1.0)</td>
<td>45°</td>
<td>10 (far)</td>
<td>0.707</td>
<td>0.707</td>
<td>0.100</td>
</tr>
<tr class="even">
<td>(1.0, 1.0)</td>
<td>90°</td>
<td>10 (far)</td>
<td>0.000</td>
<td>1.000</td>
<td>0.100</td>
</tr>
</tbody>
</table>
</section>
<section id="distance-modeling-considerations" class="level4" data-number="6.6.4.3">
<h4 data-number="6.6.4.3" class="anchored" data-anchor-id="distance-modeling-considerations"><span class="header-section-number">6.6.4.3</span> Distance Modeling Considerations</h4>
<p><strong>Acoustic Accuracy</strong>: The distance scaling approximates natural sound propagation where amplitude decreases with increasing distance, though the specific mathematical relationship can be adjusted for different acoustic environments or artistic preferences.</p>
<p><strong>Control Resolution</strong>: The 1-10 distance range provides meaningful amplitude variation while maintaining sufficient resolution for precise spatial positioning in musical and installation contexts.</p>
<p><strong>Interface Mapping</strong>: The vertical axis controlling distance creates intuitive correspondence where upward movement represents increased distance and reduced amplitude, matching visual expectations of spatial relationships.</p>
</section>
</section>
<section id="creative-applications-1" class="level3" data-number="6.6.5">
<h3 data-number="6.6.5" class="anchored" data-anchor-id="creative-applications-1"><span class="header-section-number">6.6.5</span> Creative Applications</h3>
<ul>
<li><strong>Interactive Spatial Composition</strong>: Use tablet interfaces or touch controllers to manipulate multiple sound sources simultaneously within a virtual acoustic space with realistic distance modeling.</li>
<li><strong>Architectural Space Simulation</strong>: Model specific room acoustics by adjusting the distance scaling factors to match measured reverberation and absorption characteristics of real spaces.</li>
<li><strong>Immersive Audio Storytelling</strong>: Create dynamic soundscapes for narrative applications where sound source proximity affects listener emotional engagement and attention focus.</li>
<li><strong>Performance Interface Design</strong>: Develop custom controllers that map performer gestures to spatial positioning, enabling expressive control over both angle and distance through physical movement.</li>
<li><strong>Collaborative Music Systems</strong>: Enable multiple performers to control independent sound sources within a shared spatial audio environment.</li>
<li><strong>Installation Art with Proximity Sensing</strong>: Connect distance parameters to actual physical proximity sensors that adjust audio based on visitor location within gallery spaces</li>
<li><strong>Gaming Audio Implementation</strong>: Provide realistic spatial audio feedback for game environments where player-object relationships affect both direction and distance perception</li>
</ul>
</section>
</section>
<section id="enhanced-distance-modeling-with-air-absorption-and-reverberation" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="enhanced-distance-modeling-with-air-absorption-and-reverberation"><span class="header-section-number">6.7</span> Enhanced Distance Modeling with Air Absorption and Reverberation</h2>
<p>To further enhance the realism of our spatialization, we simulate <strong>air absorption</strong>, which—although minimal at short distances—contributes to the perceptual illusion of depth. As distance increases, high frequencies are attenuated, which we simulate using a <strong>low-pass filter</strong> object<code>[lop~]</code>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/screenshots/space/distance-panning-lop.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Fig."><img src="../assets/screenshots/space/distance-panning-lop.png" class="img-fluid figure-img" alt="Fig."></a></p>
<figcaption>Fig.</figcaption>
</figure>
</div>
<p>We control the cutoff frequency of this filter using a <code>[expr]</code> object (<code>[expr (1 - $f1) * 7000 + 1000]</code>). For example, we can scale a distance range of 1–10 to a cutoff frequency range of 8000–1000 Hz. As the source moves farther away, the filter reduces the brightness of the sound, mimicking atmospheric filtering.</p>
<p>Consider a large environment with significant reverberation—such as a church or garage. When someone speaks nearby, the <strong>direct sound</strong> dominates, and reverberation is minimal. Conversely, at greater distances, the direct signal weakens while reverberation remains relatively constant. This is due to the fact that reverberation is a function of the environment and not as sensitive to the source’s location.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/screenshots/space/distance-panning-lop-reverb.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Fig."><img src="../assets/screenshots/space/distance-panning-lop-reverb.png" class="img-fluid figure-img" alt="Fig."></a></p>
<figcaption>Fig.</figcaption>
</figure>
</div>
<p>To simulate this phenomenon, we introduce a <strong>reverberation chamber</strong> using <code>[freeverb~]</code>. The reverb level is controlled via a rotary dial (<code>[knob]</code> from the <code>ELSE</code> library). By configuring the knob for a low initial setting, we simulate the acoustic behavior where reverberation becomes more prominent as the source moves farther from the listener. As the source approaches, the increased amplitude of the direct signal masks the reverberation, completing our auditory illusion.</p>
<p>The patch extends the interactive distance panning system by incorporating realistic acoustic phenomena that occur in natural environments. The implementation adds frequency-dependent air absorption through low-pass filtering and environmental reverberation modeling, creating a comprehensive spatial audio simulation that mirrors how sound behaves across varying distances in real acoustic spaces. This enhanced system demonstrates how multiple acoustic cues work together to create convincing depth perception and environmental context.</p>
<p>The design integrates three primary acoustic distance cues: amplitude attenuation based on inverse distance relationships, high-frequency absorption that simulates atmospheric filtering, and reverberation balance that reflects how direct and reflected sound components change with source proximity. This multi-parameter approach creates significantly more realistic distance simulation than amplitude scaling alone.</p>
<section id="patch-overview-2" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="patch-overview-2"><span class="header-section-number">6.7.1</span> Patch Overview</h3>
<p>The enhanced distance modeling system implements a comprehensive acoustic simulation pipeline consisting of six integrated processing stages:</p>
<ol type="1">
<li><strong>Two-Dimensional Interface Control</strong>: Visual manipulation of angle and distance parameters</li>
<li><strong>Distance-Based Amplitude Scaling</strong>: Inverse relationship modeling for realistic volume attenuation</li>
<li><strong>Frequency-Dependent Air Absorption</strong>: Low-pass filtering that simulates atmospheric effects</li>
<li><strong>Environmental Reverberation Processing</strong>: Spatial acoustic context through algorithmic reverb</li>
<li><strong>Direct/Reverb Signal Balancing</strong>: Dynamic mixing based on distance relationships</li>
<li><strong>Spatial Audio Output</strong>: Final stereo positioning with enhanced depth cues</li>
</ol>
<p>The system demonstrates how multiple acoustic phenomena combine to create convincing spatial audio environments that extend beyond simple amplitude panning.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A[2D Slider Interface] --&gt; B[Distance Extraction]
    B --&gt; C[Amplitude Scaling]
    B --&gt; D[Air Absorption Filtering]
    B --&gt; E[Reverb Level Control]
    F[Audio Source] --&gt; C
    C --&gt; G[Low-pass Filter]
    D --&gt; G
    G --&gt; H[Dry Signal Path]
    G --&gt; I[Reverb Processing]
    E --&gt; I
    H --&gt; J[Spatial Mixing]
    I --&gt; J
    J --&gt; K[Enhanced Stereo Output]
    
    style A fill:#e1f5fe
    style K fill:#f3e5f5
    style G fill:#fff3e0
    style I fill:#fff3e0
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="key-objects-and-their-roles-2" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="key-objects-and-their-roles-2"><span class="header-section-number">6.7.2</span> Key Objects and Their Roles</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 18%">
<col style="width: 66%">
</colgroup>
<thead>
<tr class="header">
<th>Object</th>
<th>Function</th>
<th>Role in Enhanced Distance Modeling</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>slider2d</code></td>
<td>Two-dimensional interface</td>
<td>Angle and distance parameter input</td>
</tr>
<tr class="even">
<td><code>expr~ $v1 / (($v2*9)+1)</code></td>
<td>Distance amplitude scaling</td>
<td>Inverse distance attenuation</td>
</tr>
<tr class="odd">
<td><code>lop~</code></td>
<td>Low-pass filter</td>
<td>Frequency-dependent air absorption simulation</td>
</tr>
<tr class="even">
<td><code>expr (1 - $f1) * 7000 + 1000</code></td>
<td>Cutoff frequency calculation</td>
<td>Maps distance to absorption characteristics</td>
</tr>
<tr class="odd">
<td><code>freeverb~</code></td>
<td>Algorithmic reverberation</td>
<td>Environmental acoustic simulation</td>
</tr>
<tr class="even">
<td><code>knob</code></td>
<td>Reverb level control</td>
<td>Interactive reverberation amount adjustment</td>
</tr>
<tr class="odd">
<td><code>s~ dry</code> / <code>r~ dry</code></td>
<td>Dry signal routing</td>
<td>Direct sound path management</td>
</tr>
<tr class="even">
<td><code>expr~ $v1*$v2 \; $v3*$v4</code></td>
<td>Spatial audio multiplier</td>
<td>Final stereo positioning with distance effects</td>
</tr>
</tbody>
</table>
<section id="critical-enhancement-components" class="level4" data-number="6.7.2.1">
<h4 data-number="6.7.2.1" class="anchored" data-anchor-id="critical-enhancement-components"><span class="header-section-number">6.7.2.1</span> Critical Enhancement Components</h4>
<p><strong>Air Absorption Modeling</strong>: The <code>lop~</code> object implements frequency-dependent attenuation that simulates how air absorption affects sound transmission over distance. The cutoff frequency calculation <code>(1 - $f1) * 7000 + 1000</code> creates an inverse relationship where increased distance produces lower cutoff frequencies, mimicking the natural tendency for high frequencies to attenuate more rapidly through atmospheric transmission.</p>
<p><strong>Reverberation Processing</strong>: The <code>freeverb~</code> object provides environmental acoustic context through algorithmic reverberation that simulates room acoustics. The reverb parameters (dry 0, wet 1, roomsize 0.95) create a large virtual space with minimal direct signal bleed-through.</p>
<p><strong>Dynamic Signal Routing</strong>: The send/receive system (<code>s~ dry</code>, <code>r~ dry</code>) enables parallel processing where the same audio source feeds both the direct signal path and the reverberation processing chain, allowing independent control of each component.</p>
</section>
</section>
<section id="data-flow-2" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="data-flow-2"><span class="header-section-number">6.7.3</span> Data Flow</h3>
<p>The enhanced distance modeling patch orchestrates a sophisticated multi-stage acoustic simulation that combines amplitude, frequency, and temporal cues to create realistic spatial audio experiences. This implementation demonstrates how multiple acoustic phenomena interact in natural environments and how these relationships can be modeled systematically to achieve convincing distance perception.</p>
<p>The enhanced distance parameter extraction stage builds upon the basic distance panning system by distributing the distance control signal to multiple processing destinations simultaneously. The y-coordinate from the 2D slider interface continues to provide the primary distance measurement, but this value now feeds three distinct processing pathways rather than a single amplitude control. The distance parameter reaches the amplitude scaling system for volume attenuation, the air absorption calculation system for frequency filtering, and the reverberation balance system for environmental processing. This parallel distribution ensures that all acoustic distance cues remain synchronized and proportional, maintaining the physical relationships that exist in natural acoustic environments.</p>
<p>The frequency-dependent air absorption processing stage introduces realistic high-frequency attenuation that simulates atmospheric filtering effects. The distance value undergoes transformation through the expression <code>(1 - $f1) * 7000 + 1000</code>, which creates an inverse relationship mapping distance to low-pass filter cutoff frequency. When the source appears close (distance near 0), the calculation yields a cutoff frequency approaching 8000 Hz, allowing the full frequency spectrum to pass through unaffected. As distance increases toward maximum (distance near 1), the cutoff frequency approaches 1000 Hz, creating significant high-frequency attenuation that simulates the natural absorption characteristics of air transmission. The <code>lop~</code> object implements this filtering in real-time, processing the distance-scaled audio signal to create the spectral changes associated with increasing source distance. This filtering effect proves particularly noticeable with broad-spectrum audio sources, where the gradual loss of high-frequency content creates immediate perceptual cues about source proximity.</p>
<p>The environmental reverberation processing stage adds spatial acoustic context through sophisticated algorithmic processing that simulates how direct and reflected sound components change with source distance. The audio signal enters the <code>freeverb~</code> object configured with parameters that create a large virtual acoustic space: dry level set to 0 eliminates direct signal bleed-through, wet level at 1 provides full reverberation output, and room size at 0.95 creates an expansive virtual environment. The reverberation level control through the <code>knob</code> interface enables real-time adjustment of the environmental contribution, simulating how reverberation prominence changes with source distance. In natural acoustic environments, nearby sources produce strong direct signals that mask reverberation, while distant sources generate weaker direct signals that allow reverberation to become more prominent in the overall acoustic balance.</p>
<p>The signal mixing and spatial integration stage combines the processed direct and reverberated audio components while maintaining the established spatial positioning relationships. The direct signal path carries the distance-attenuated and air-absorption-filtered audio through the send/receive system (<code>s~ dry</code>, <code>r~ dry</code>) to the spatial mixing stage, while the reverberation output provides environmental context that adds spatial depth and acoustic realism. The final spatial audio expression <code>expr~ $v1*$v2 \; $v3*$v4</code> applies the trigonometric amplitude coefficients to both the direct and environmental signal components, ensuring that the spatial positioning affects all aspects of the audio output. This integrated approach creates stereo output where both the direct sound and its environmental reflections maintain proper spatial relationships, contributing to convincing three-dimensional audio positioning that extends beyond simple left-right panning.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    A[Distance Control] --&gt; B[Multi-Parameter Processing]
    B --&gt; C[Enhanced Audio Output]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="processing-chain-details-2" class="level3" data-number="6.7.4">
<h3 data-number="6.7.4" class="anchored" data-anchor-id="processing-chain-details-2"><span class="header-section-number">6.7.4</span> Processing Chain Details</h3>
<p>The enhanced distance modeling system integrates multiple acoustic simulation stages:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 29%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>Input</th>
<th>Process</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2D Interface</td>
<td>Distance parameter extraction</td>
<td>Control signals for multiple destinations</td>
</tr>
<tr class="even">
<td>2</td>
<td>Distance Value</td>
<td>Amplitude scaling calculation</td>
<td>Volume attenuation</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Distance Value</td>
<td>Air absorption frequency mapping</td>
<td>Low-pass filter cutoff</td>
</tr>
<tr class="even">
<td>4</td>
<td>Filtered Audio</td>
<td>Environmental reverberation</td>
<td>Spatial acoustic context</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Distance Value</td>
<td>Reverb balance calculation</td>
<td>Direct/reverb mixing ratio</td>
</tr>
<tr class="even">
<td>6</td>
<td>Multiple Audio Streams</td>
<td>Spatial positioning integration</td>
<td>Enhanced stereo output</td>
</tr>
</tbody>
</table>
<section id="acoustic-parameter-relationships" class="level4" data-number="6.7.4.1">
<h4 data-number="6.7.4.1" class="anchored" data-anchor-id="acoustic-parameter-relationships"><span class="header-section-number">6.7.4.1</span> Acoustic Parameter Relationships</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 13%">
<col style="width: 21%">
<col style="width: 22%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Distance Setting</th>
<th>Amplitude</th>
<th>Cutoff Frequency</th>
<th>Reverb Prominence</th>
<th>Perceived Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.0 (Close)</td>
<td>1.000</td>
<td>8000 Hz</td>
<td>Low</td>
<td>Bright, direct, intimate</td>
</tr>
<tr class="even">
<td>0.25</td>
<td>0.692</td>
<td>6250 Hz</td>
<td>Low-Medium</td>
<td>Slightly muffled, present</td>
</tr>
<tr class="odd">
<td>0.5</td>
<td>0.444</td>
<td>4500 Hz</td>
<td>Medium</td>
<td>Noticeably filtered, balanced</td>
</tr>
<tr class="even">
<td>0.75</td>
<td>0.308</td>
<td>2750 Hz</td>
<td>Medium-High</td>
<td>Distant, reverberant</td>
</tr>
<tr class="odd">
<td>1.0 (Far)</td>
<td>0.200</td>
<td>1000 Hz</td>
<td>High</td>
<td>Very distant, heavily filtered</td>
</tr>
</tbody>
</table>
</section>
<section id="air-absorption-simulation" class="level4" data-number="6.7.4.2">
<h4 data-number="6.7.4.2" class="anchored" data-anchor-id="air-absorption-simulation"><span class="header-section-number">6.7.4.2</span> Air Absorption Simulation</h4>
<p>The frequency mapping implements realistic atmospheric absorption characteristics:</p>
<ul>
<li><strong>Near field (0-0.3)</strong>: Minimal high-frequency loss, preserving audio brightness</li>
<li><strong>Mid field (0.3-0.7)</strong>: Progressive filtering that maintains intelligibility</li>
<li><strong>Far field (0.7-1.0)</strong>: Significant high-frequency attenuation creating distant character</li>
</ul>
</section>
</section>
<section id="creative-applications-2" class="level3" data-number="6.7.5">
<h3 data-number="6.7.5" class="anchored" data-anchor-id="creative-applications-2"><span class="header-section-number">6.7.5</span> Creative Applications</h3>
<ul>
<li><strong>Cinematic Soundscape Design</strong>: Create realistic environmental audio where sound sources exhibit authentic distance characteristics for film, gaming, or virtual reality applications.</li>
<li><strong>Architectural Acoustic Simulation</strong>: Model specific room types by adjusting reverberation parameters to match different environmental contexts from intimate chambers to vast cathedrals.</li>
<li><strong>Interactive Audio Installations</strong>: Design responsive environments where visitor movement controls not just positioning but the complete acoustic experience including filtering and reverberation.</li>
<li><strong>Nature Sound Recreation</strong>: Simulate outdoor acoustic environments where wildlife calls, weather sounds, and ambient textures exhibit realistic distance-dependent characteristics.</li>
<li><strong>Performance Audio Processing</strong>: Process live instruments with dynamic distance effects that can be controlled by performers or automated through score-following systems.</li>
</ul>
</section>
</section>
<section id="simulation-through-quadraphonic-spatialization-the-chowning-model" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="simulation-through-quadraphonic-spatialization-the-chowning-model"><span class="header-section-number">6.8</span> Simulation through Quadraphonic Spatialization: The Chowning Model</h2>
<p>To extend spatial simulation beyond the limitations of stereophony—typically constrained to a 90° field—or to achieve finer localization resolution, the <em>intensity panning</em> technique used with two loudspeakers can be generalized to multi-channel systems.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/screenshots/space/quadra-chowning.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Fig."><img src="../assets/screenshots/space/quadra-chowning.png" class="img-fluid figure-img" alt="Fig."></a></p>
<figcaption>Fig.</figcaption>
</figure>
</div>
<p>By placing four loudspeakers at the corners of a square, one can simulate a 360° horizontal sound field, allowing for a full circular distribution of a virtual source. Arranging six speakers in a hexagonal layout also provides 360° coverage, with improved localization accuracy. As a general rule, more channels yield higher resolution and spatial definition, though this comes at the cost of increased system complexity. From a practical standpoint, quadraphonic configurations have proven to offer a compelling balance between implementation feasibility and perceptual quality, and have thus gained wide acceptance in spatial sound simulation.</p>
<p>One of the most well-known models in this domain is the <strong>John Chowning quadraphonic model</strong>. It features:</p>
<ul>
<li>Direct sound simulation from a virtual source</li>
<li>Local reverberation, individually tailored for each speaker based on the source’s angle</li>
<li>Global reverberation shared across all four speakers</li>
</ul>
<section id="amplitude-and-distance-scaling" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="amplitude-and-distance-scaling"><span class="header-section-number">6.8.1</span> Amplitude and Distance Scaling</h3>
<p>The audio input signal is first attenuated according to the <strong>inverse of the distance</strong>:</p>
<p><span class="math display">\[
A_{\text{direct}} = \frac{1}{D}
\]</span></p>
<p>Then, the signal is <strong>scaled by a coefficient specific to each speaker</strong>, depending on the virtual source’s position:</p>
<p><span class="math display">\[
A_i = \frac{1}{D} \times C_i(\theta)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(A_i\)</span> is the amplitude for speaker <span class="math inline">\(i\)</span>,</li>
<li><span class="math inline">\(D\)</span> is the distance between the source and the listener,</li>
<li><span class="math inline">\(C_i(\theta)\)</span> is the coefficient for speaker <span class="math inline">\(i\)</span> as a function of the source angle <span class="math inline">\(\theta \in [0^\circ, 360^\circ]\)</span>,</li>
<li>and the speaker coefficients are labeled LRA, LFA, RFA, RRA (Left Rear, Left Front, Right Front, Right Rear).</li>
</ul>
</section>
<section id="gain-function-and-phase-offsets" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="gain-function-and-phase-offsets"><span class="header-section-number">6.8.2</span> Gain Function and Phase Offsets</h3>
<p>To avoid complex calculations of gain for each speaker, Chowning implements a <strong>predefined function</strong>, stored as a lookup table of 512 samples. The table’s function ranges between 0 and 1 and is designed to generate continuous 360° source motion through intensity panning.</p>
<p>Each speaker reads the same function with a <strong>phase offset</strong>:</p>
<ul>
<li>The <strong>first quarter</strong> of the function represents a sine wave from <span class="math inline">\(0^\circ\)</span> to <span class="math inline">\(90^\circ\)</span></li>
<li>The <strong>second quarter</strong> is a cosine wave over the same angular range</li>
<li>The <strong>remaining half</strong> is silent (zero amplitude)</li>
</ul>
<p>As the virtual source rotates, only <strong>two speakers are active</strong> at a time, crossfading between them. For instance, as the source moves from <span class="math inline">\(theta = 90^\circ\)</span> to <span class="math inline">\(\theta = 0^\circ\)</span>, the cosine-driven speaker fades out while the sine-driven speaker fades in, mimicking a continuous spatial trajectory.</p>
</section>
<section id="reverberation-modeling" class="level3" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="reverberation-modeling"><span class="header-section-number">6.8.3</span> Reverberation Modeling</h3>
<p>In the lower part of Chowning’s model (see again Figure G.8.9), <strong>reverberation is processed separately</strong>: - The reverberation signal is attenuated by the <strong>square root of the inverse of the distance</strong>:</p>
<p><span class="math display">\[
A_{\text{reverb}} = \sqrt{\frac{1}{D}} \times \text{PRV}
\]</span></p>
<p>where <strong>PRV</strong> is an empirical parameter representing the <strong>reverberation percentage</strong>.</p>
<ul>
<li>A <strong>global reverb</strong> signal is scaled again by <span class="math inline">\(\frac{1}{D}\)</span></li>
<li><strong>Local reverberation</strong> is further attenuated by <span class="math inline">\(1 - \frac{1}{D}\)</span>, then scaled using the same angle-dependent coefficients (LRA, LFA, RFA, RRA)</li>
</ul>
<p>To increase realism, <strong>four independent reverberation units</strong> are used—one for each loudspeaker—ensuring spatial decorrelation and more convincing immersion.</p>
<p>This model not only exemplifies how mathematical reasoning and spatial design converge in sound synthesis, but also stands as a landmark in algorithmic composition and immersive audio. Through relatively simple mathematical operations and efficient data structures (such as phase-offset lookup tables), it offers a powerful foundation for dynamic spatial control in real-time systems such as Pure Data.</p>
<p>Here’s how to think about the solution:</p>
</section>
<section id="coordinate-system" class="level3" data-number="6.8.4">
<h3 data-number="6.8.4" class="anchored" data-anchor-id="coordinate-system"><span class="header-section-number">6.8.4</span> Coordinate System</h3>
<p>The <code>circle</code> object gives you <strong>Cartesian coordinates</strong> (x, y) in range [-1, 1], but you need:</p>
<ol type="1">
<li><strong>Polar coordinates</strong> (angle, radius)</li>
<li><strong>Scaled values</strong> (0-359° for azimuth, 1-10 for distance)</li>
</ol>
</section>
<section id="mathematical-conversion" class="level3" data-number="6.8.5">
<h3 data-number="6.8.5" class="anchored" data-anchor-id="mathematical-conversion"><span class="header-section-number">6.8.5</span> Mathematical Conversion</h3>
<section id="azimuth-angle-calculation" class="level4" data-number="6.8.5.1">
<h4 data-number="6.8.5.1" class="anchored" data-anchor-id="azimuth-angle-calculation"><span class="header-section-number">6.8.5.1</span> Azimuth (Angle) Calculation</h4>
<p>We need to convert (x, y) to angle using <strong>arctangent</strong> using:</p>
<p><code>[expr atan2($f2, $f1) * 180 / 3.14159]</code></p>
<p><strong>Key points:</strong></p>
<ul>
<li><code>atan2(y, x)</code> gives angle in radians (-π to π)</li>
<li>Multiply by <code>180/π</code> to convert to degrees (-180° to 180°)</li>
<li>Add 360° if negative to get 0-360° range:</li>
</ul>
<p><code>[expr (atan2($f2, $f1) * 180 / 3.14159 + 360) % 360]</code></p>
</section>
<section id="distance-calculation" class="level4" data-number="6.8.5.2">
<h4 data-number="6.8.5.2" class="anchored" data-anchor-id="distance-calculation"><span class="header-section-number">6.8.5.2</span> Distance Calculation</h4>
<p>Convert (x, y) to radius using <strong>Pythagorean theorem</strong>, then scale:</p>
<p><code>[expr sqrt($f1*$f1 + $f2*$f2) * 9 + 1]</code></p>
<p><strong>Logic:</strong></p>
<ul>
<li><code>sqrt(x² + y²)</code> gives radius in range [0, 1]</li>
<li>Multiply by 9 and add 1 to get range [1, 10]</li>
</ul>
</section>
</section>
<section id="patch-overview-3" class="level3" data-number="6.8.6">
<h3 data-number="6.8.6" class="anchored" data-anchor-id="patch-overview-3"><span class="header-section-number">6.8.6</span> Patch Overview</h3>
<p>This Pure Data patch implements John Chowning’s groundbreaking quadraphonic spatialization model, demonstrating how mathematical algorithms can create convincing 360-degree spatial audio movement using four strategically positioned speakers. The implementation features a predefined lookup table, distance-based amplitude scaling, and sophisticated reverberation modeling that simulates both local and global acoustic environments. This approach enables precise control over virtual sound source movement while maintaining perceptual coherence across the complete circular sound field.</p>
<p>The patch demonstrates the practical application of Chowning’s theoretical framework through real-time interface controls that manage azimuth positioning, distance simulation, and reverberation balance. By utilizing lookup tables and phase-offset techniques, the system achieves smooth spatial transitions while minimizing computational overhead, making it suitable for live performance and interactive applications.</p>
<p>The Chowning quadraphonic spatialization system implements a comprehensive spatial audio pipeline consisting of six primary processing stages:</p>
<ol type="1">
<li><strong>Lookup Table Generation</strong>: Creates the fundamental sine/cosine function for spatial positioning</li>
<li><strong>Interactive Control Interface</strong>: Manages azimuth, distance, and reverberation parameters</li>
<li><strong>Amplitude Coefficient Calculation</strong>: Determines speaker-specific gain values using phase offsets</li>
<li><strong>Distance Modeling</strong>: Applies inverse distance relationships for realistic amplitude scaling</li>
<li><strong>Reverberation Processing</strong>: Implements both local and global acoustic environment simulation</li>
<li><strong>Quadraphonic Output</strong>: Delivers positioned audio to four independent speaker channels</li>
</ol>
<p>The system demonstrates how relatively simple mathematical operations and efficient data structures can create sophisticated spatial audio experiences suitable for both compositional and performance applications.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A[Lookup Table Generation] --&gt; B[Interface Controls]
    B --&gt; C[Amplitude Calculation]
    B --&gt; D[Distance Processing]
    C --&gt; E[Audio Positioning]
    D --&gt; E
    E --&gt; F[Reverberation]
    F --&gt; G[Quadraphonic Output LF RF RR LR]
    
    style A fill:#e1f5fe
    style G fill:#f3e5f5
    style E fill:#fff3e0
    style F fill:#fff3e0
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="key-objects-and-their-roles-3" class="level3" data-number="6.8.7">
<h3 data-number="6.8.7" class="anchored" data-anchor-id="key-objects-and-their-roles-3"><span class="header-section-number">6.8.7</span> Key Objects and Their Roles</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 21%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th>Object</th>
<th>Function</th>
<th>Role in Quadraphonic System</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>tabwrite chowning-table</code></td>
<td>Lookup table creation</td>
<td>Stores 512-sample sine function for spatial coefficients</td>
</tr>
<tr class="even">
<td><code>expr if($f1&lt;256, sin($f1*3.14159/256), 0)</code></td>
<td>Function generation</td>
<td>Creates sine wave with silent half-cycle</td>
</tr>
<tr class="odd">
<td><code>circle</code></td>
<td>2D position interface</td>
<td>Provides x,y coordinates for azimuth and distance control</td>
</tr>
<tr class="even">
<td><code>angle</code></td>
<td>Phase offset calculation</td>
<td>Implements speaker-specific table reading positions</td>
</tr>
<tr class="odd">
<td><code>freeverb~</code></td>
<td>Algorithmic reverberation</td>
<td>Creates environmental acoustic simulation</td>
</tr>
<tr class="even">
<td><code>expr~ $v1 / (($v2*9)+1)</code></td>
<td>Distance attenuation</td>
<td>Applies inverse distance amplitude scaling</td>
</tr>
<tr class="odd">
<td><code>sqrt</code></td>
<td>Reverberation scaling</td>
<td>Modifies reverb balance based on distance</td>
</tr>
</tbody>
</table>
<section id="critical-processing-components-2" class="level4" data-number="6.8.7.1">
<h4 data-number="6.8.7.1" class="anchored" data-anchor-id="critical-processing-components-2"><span class="header-section-number">6.8.7.1</span> Critical Processing Components</h4>
<p><strong>Chowning Lookup Table</strong>: The 512-sample array contains a complete sine function from 0° to 180° (samples 0-255) followed by silence (samples 256-511). This creates the characteristic amplitude envelope where only two adjacent speakers are active at any time during 360° rotation.</p>
<p><strong>Angle Objects</strong>: Each of the four speakers uses an <code>angle</code> object with specific phase offsets (0, 128, 256, 384) that correspond to 90° increments around the circular speaker arrangement. These offsets ensure proper spatial positioning as the virtual source rotates.</p>
<p><strong>Dynamic Distance Processing</strong>: The patch implements both amplitude attenuation (<code>/~ distance</code>) and reverberation balance modification (<code>sqrt</code> and <code>1-1/distance</code> calculations) to create realistic distance perception through multiple acoustic cues.</p>
</section>
</section>
<section id="data-flow-3" class="level3" data-number="6.8.8">
<h3 data-number="6.8.8" class="anchored" data-anchor-id="data-flow-3"><span class="header-section-number">6.8.8</span> Data Flow</h3>
<p>The Chowning quadraphonic spatialization patch orchestrates a sophisticated multi-stage spatial audio processing pipeline that transforms user interface input into convincing 360-degree sound source positioning through mathematical modeling of acoustic propagation and psychoacoustic principles. This implementation demonstrates how John Chowning’s theoretical framework translates into practical real-time audio processing systems that maintain both computational efficiency and perceptual accuracy.</p>
<p>The lookup table generation and initialization stage establishes the mathematical foundation for the entire spatialization system through automated creation of the characteristic Chowning function. Upon patch initialization, the <code>loadbang</code> object triggers a systematic process that populates the <code>chowning-table</code> array with 512 samples representing the fundamental spatial positioning function. The generation process employs an <code>until</code> loop that iterates through array indices 0-511, with each position calculated using the expression <code>if($f1&lt;256, sin($f1*3.14159/256), 0)</code>. This creates a complete sine wave from 0° to 180° occupying the first half of the table (samples 0-255), followed by complete silence in the second half (samples 256-511). This specific function design ensures that during 360° spatial rotation, only two adjacent speakers remain active at any time, creating smooth amplitude transitions that maintain constant energy output while avoiding phase cancellation effects that could degrade spatial imaging.</p>
<p>The interactive control and parameter extraction stage manages real-time spatial positioning through the sophisticated interface provided by the <code>circle</code> object from the ELSE library. This two-dimensional control surface captures mouse interaction as normalized coordinate pairs that represent both angular position and distance information within a single intuitive interface. The coordinate unpacking process separates the combined spatial input into independent data streams: the x-coordinate undergoes conversion to azimuth values spanning 0-359° through the expression <code>(atan2($f2, $f1) * 180 / 3.14159 + 360) % 360</code>, while the y-coordinate transforms into distance values ranging 1-10 through <code>sqrt($f1*$f1 + $f2*$f2) * 9 + 1</code>. This dual parameter extraction enables simultaneous control over both horizontal positioning and depth perception within the virtual acoustic space, providing the essential control data for subsequent processing stages.</p>
<p>The amplitude coefficient calculation stage represents the core of Chowning’s spatialization algorithm, where angular position information becomes speaker-specific gain values through phase-offset table reading. Each of the four speakers employs an individual <code>angle</code> object configured with specific phase offsets: 0 samples for the front-left speaker, 128 samples for front-right, 256 samples for rear-right, and 384 samples for rear-left. These offsets correspond to 90° increments around the circular speaker arrangement, ensuring that the spatial positioning function provides appropriate amplitude coefficients for each speaker location. As the azimuth value changes, each <code>angle</code> object reads from its phase-shifted position within the lookup table, generating amplitude coefficients that create smooth transitions between adjacent speakers while maintaining the characteristic two-speaker activation pattern that defines the Chowning model.</p>
<p>The distance processing and amplitude scaling stage implements realistic sound propagation characteristics through multiple coordinated processing pathways that simulate both amplitude attenuation and acoustic environmental effects. The primary distance attenuation employs the expression <code>expr~ $v1 / (($v2*9)+1)</code> applied to the raw audio signal, creating inverse distance scaling where increasing distance produces proportionally decreased amplitude. This amplitude scaling affects all speakers equally, maintaining the spatial positioning relationships while adding realistic volume changes that correspond to source proximity. Simultaneously, the distance parameter influences reverberation processing through the <code>sqrt</code> calculation that modifies reverberation level based on distance relationships, simulating how nearby sources produce strong direct signals that mask environmental reflections while distant sources allow reverberation to become more prominent in the overall acoustic balance.</p>
<p>The reverberation processing and environmental simulation stage adds spatial acoustic context through dual-path processing that creates both local and global environmental effects. The audio signal path splits to feed both direct positioning processing and environmental reverberation systems implemented through <code>freeverb~</code> objects configured for large virtual spaces. The reverberation balance control through the <code>knob</code> interface enables real-time adjustment of environmental contribution, while the distance-dependent reverberation scaling creates realistic acoustic behavior where reverberation prominence varies with source distance. The dual reverberation system provides both general environmental ambience and speaker-specific local reflections, creating convincing spatial acoustic contexts that enhance the positioning effects achieved through the amplitude coefficients.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    A[User Input] --&gt; B[Parameter Processing]
    B --&gt; C[Spatial Calculation]
    C --&gt; D[Audio Output]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="processing-chain-details-3" class="level3" data-number="6.8.9">
<h3 data-number="6.8.9" class="anchored" data-anchor-id="processing-chain-details-3"><span class="header-section-number">6.8.9</span> Processing Chain Details</h3>
<p>The quadraphonic spatialization system maintains mathematical precision and perceptual coherence through coordinated processing stages:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 29%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>Input</th>
<th>Process</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Patch Initialization</td>
<td>Sine function generation</td>
<td>512-sample lookup table</td>
</tr>
<tr class="even">
<td>2</td>
<td>Circle Interface</td>
<td>Coordinate conversion</td>
<td>Azimuth (0-359°) and Distance (1-10)</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Azimuth + Phase Offsets</td>
<td>Table reading with interpolation</td>
<td>Speaker amplitude coefficients</td>
</tr>
<tr class="even">
<td>4</td>
<td>Distance Value</td>
<td>Inverse scaling calculation</td>
<td>Amplitude attenuation factor</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Audio + Coefficients + Distance</td>
<td>Multiplication and mixing</td>
<td>Positioned audio signals</td>
</tr>
<tr class="even">
<td>6</td>
<td>Multiple Audio Streams</td>
<td>Reverberation processing</td>
<td>Enhanced quadraphonic output</td>
</tr>
</tbody>
</table>
<section id="speaker-configuration-and-phase-offsets" class="level4" data-number="6.8.9.1">
<h4 data-number="6.8.9.1" class="anchored" data-anchor-id="speaker-configuration-and-phase-offsets"><span class="header-section-number">6.8.9.1</span> Speaker Configuration and Phase Offsets</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 29%">
<col style="width: 24%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Speaker Position</th>
<th>Phase Offset (samples)</th>
<th>Angle Equivalent</th>
<th>Function Region</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Front Left (LF)</td>
<td>0</td>
<td>0°</td>
<td>Sine ascending</td>
</tr>
<tr class="even">
<td>Front Right (RF)</td>
<td>128</td>
<td>90°</td>
<td>Sine descending</td>
</tr>
<tr class="odd">
<td>Rear Right (RR)</td>
<td>256</td>
<td>180°</td>
<td>Silent region</td>
</tr>
<tr class="even">
<td>Rear Left (LR)</td>
<td>384</td>
<td>270°</td>
<td>Silent region</td>
</tr>
</tbody>
</table>
</section>
<section id="distance-processing-parameters" class="level4" data-number="6.8.9.2">
<h4 data-number="6.8.9.2" class="anchored" data-anchor-id="distance-processing-parameters"><span class="header-section-number">6.8.9.2</span> Distance Processing Parameters</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 25%">
<col style="width: 28%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Distance Value</th>
<th>Amplitude Scaling</th>
<th>Reverberation Factor</th>
<th>Perceived Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.0 (Close)</td>
<td>1.000</td>
<td>0.000</td>
<td>Immediate, dry, present</td>
</tr>
<tr class="even">
<td>3.0</td>
<td>0.250</td>
<td>0.667</td>
<td>Moderate distance, some reverb</td>
</tr>
<tr class="odd">
<td>6.0</td>
<td>0.143</td>
<td>0.833</td>
<td>Distant, reverberant</td>
</tr>
<tr class="even">
<td>10.0 (Far)</td>
<td>0.091</td>
<td>0.900</td>
<td>Very distant, highly reverberant</td>
</tr>
</tbody>
</table>
</section>
<section id="lookup-table-function-analysis" class="level4" data-number="6.8.9.3">
<h4 data-number="6.8.9.3" class="anchored" data-anchor-id="lookup-table-function-analysis"><span class="header-section-number">6.8.9.3</span> Lookup Table Function Analysis</h4>
<p>The Chowning table implements a specific mathematical relationship that ensures smooth spatial transitions:</p>
<ul>
<li><strong>Samples 0-127</strong>: Ascending sine (0 to 1) - Controls fade-in transitions</li>
<li><strong>Samples 128-255</strong>: Descending sine (1 to 0) - Controls fade-out transitions<br>
</li>
<li><strong>Samples 256-511</strong>: Silent (0) - Ensures two-speaker limitation</li>
</ul>
<p>This function design creates the characteristic “phantom source” effect where virtual positioning occurs between active speaker pairs while maintaining constant total energy output.</p>
</section>
</section>
<section id="creative-applications-3" class="level3" data-number="6.8.10">
<h3 data-number="6.8.10" class="anchored" data-anchor-id="creative-applications-3"><span class="header-section-number">6.8.10</span> Creative Applications</h3>
<ul>
<li><strong>360-Degree Spatial Composition</strong>: Create immersive musical pieces where sound sources move in complete circles around the listener, utilizing the full quadraphonic field for dynamic spatial narratives.</li>
<li><strong>Interactive Spatial Performance</strong>: Connect external controllers or sensors to the azimuth and distance parameters, enabling performers to sculpt spatial positioning through gesture or movement.</li>
<li><strong>Environmental Sound Design</strong>: Simulate realistic acoustic environments by combining the Chowning positioning with distance-dependent reverberation for film, game, or installation applications.</li>
<li><strong>Multi-Source Spatial Mixing</strong>: Implement multiple instances of the Chowning model to position independent sound sources within the same quadraphonic field, creating complex spatial polyphony.</li>
<li><strong>Adaptive Concert Hall Simulation</strong>: Use real-time reverberation control to simulate different acoustic environments during performance, from intimate chambers to vast cathedrals.</li>
<li><strong>Collaborative Virtual Environments</strong>: Network multiple quadraphonic systems where remote participants can position sounds within shared virtual acoustic spaces for telepresence applications.</li>
</ul>
</section>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-abregu2012" class="csl-entry" role="listitem">
Abregú, E. L., E. R. Calcagno, and R. O. Vergara. 2012. <span>“La Distancia Como Factor Estructural de La Música.”</span> <em>Revista Argentina de Musicología</em>, no. 12-13: 379–400.
</div>
<div id="ref-basso2009" class="csl-entry" role="listitem">
Basso, Gustavo, Oscar Pablo Di Liscia, and Juan Pampin, eds. 2009. <em>Música y Espacio: Ciencia, Tecnología y Estética</em>. Buenos Aires: Editorial de la Universidad Nacional de Quilmes.
</div>
<div id="ref-knudsen1932" class="csl-entry" role="listitem">
Knudsen, V. O. 1932. <em>Architectural Acoustics</em>. J. Wiley y Sons.
</div>
<div id="ref-risett1969" class="csl-entry" role="listitem">
Risett, J. C. 1969. <span>“An Introductory Catalogue of Computer Synthesized Sounds.”</span> Nueva Jersey: Bell Telephone Laboratories.
</div>
<div id="ref-schaeffer2003" class="csl-entry" role="listitem">
Schaeffer, Pierre. 2003. <em>Tratado de Los Objetos Musicales</em>. Translated by Araceli Cabezón de Diego. Alianza Editorial.
</div>
<div id="ref-stockhausen1959" class="csl-entry" role="listitem">
Stockhausen, Karlheinz. 1959. <span>“Musik Im Raum.”</span> <em>Die Reihe</em>, no. 5.
</div>
<div id="ref-wishart1996" class="csl-entry" role="listitem">
Wishart, Trevor. 1996. <em>On Sonic Art</em>. Amsterdam: Harwood Academic Publishers.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/sonification.html" class="pagination-link" aria-label="Sonification">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sonification</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/conclusion.html" class="pagination-link" aria-label="Conclusion">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Conclusion</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Code that Sounds - Ezequiel Abregú
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>