# Space

In the realm of sound, the concept of space is often overlooked. Yet, it plays a crucial role in shaping our auditory experience. The spatial dimension of sound is not merely an acoustic phenomenon; it is a fundamental aspect of how we perceive sound. This chapter explores the multifaceted nature of sound space, its historical evolution, and its implications for contemporary electroacoustic music.

## The Music Sound Space 

Sound always unfolds within a specific time and space. From the very moment sound is generated in music, space is implicitly present, enabling the possibility of organizing, reconstructing, and shaping that space to influence musical form. Thus, the use of space in music responds to concerns that go far beyond mere acoustic considerations.

Conceiving space as a structural element in the construction of sonic discourse requires us to address a complex notion—one that touches on multiple dimensions of composition, performance, and perception. This idea rests on the argument that space is a composite musical element, one that can be integrated into a compositional structure and assume a significant role within the formal hierarchy of sound discourse.

Spatiality in music represents a compositional variable with a long historical lineage. The treatment of sound space on stage can be traced back to classical Greek theatre, where actors and chorus members used masks to amplify vocal resonance and enhance vocal directionality [@knudsen1932]. However, while compositional techniques addressing spatial sound have been developing for centuries, it is not until the early 20th century that a systematic use of spatial dimensions becomes central to musical structure. Landmark works such as *Universe Symphony* (1911–51) by Charles Ives, *Déserts* (1954) and *Poème électronique* (1958) by Edgard Varèse, *Gruppen* (1955–57) by Karlheinz Stockhausen, and *Persephassa* (1969) by Iannis Xenakis approach spatiality as an independent and structural dimension. In these works, space is interrelated with other sonic parameters such as timbre, dynamics, duration, and pitch.

Particularly noteworthy is that these pieces do not merely establish sonic space as a new musical dimension—they also develop theoretical frameworks for the spatialization of sound. These theories consider physical, poetic, pictorial, and perceptual aspects of spatial sound. As a result, in recent decades the areas of research and application related to artistic-musical knowledge have expanded significantly, fostering interdisciplinary inquiry into sound space and its parameters.

Beyond its central role in music, in recent years spatial sound has also become a subject of study across a variety of disciplines. One example of this is found in research into sound spatialization techniques in both real and virtual acoustic environments. These techniques have largely evolved based on principles of psychoacoustics, cognitive modeling, and technological advancements, leading to the development of specialized hardware and software for spatial sound processing. In the scientific realm, theses and research articles have explored, in great detail, the cues necessary to create an acoustic image of a virtual source and how these cues must be simulated—whether by electronic circuitry or computational algorithms.

However, it must be noted that musical techniques for managing sound space have not yet fully integrated findings from perceptual science. As we will explore later, many artistic approaches rely on only a subset of the cues involved in auditory spatial perception, overlooking others that are essential for producing a convincing acoustic image.

In general terms, the spatial dimension of a musical composition can function as the primary expressive and communicative attribute for the listener. For instance, from the very first moment, the acoustic characteristics of a venue—real or virtual—directly affect how we perceive the sonic discourse. This dimension of musical space is further shaped by the placement of sound sources, whether instrumentalists or loudspeakers. A historical precedent for this can be found in the Renaissance period through the use of divided choirs in the Basilica of San Marco in Venice. This stylistic practice, known as antiphonal music, represents a clear antecedent of the deep connection between musical construction and architectural space [@stockhausen1959]. The style of Venetian composers was strongly influenced by the acoustics and architectural features of San Marco, involving spatially separated choirs or instrumental groups performing in alternation.


![**Figure 1**: Floor plan of the Basilica of San Marco in Venice (Italy).](/assets/images/space/fig-1-san-marcos.png)


### Spatial Distribution and the Evolution of Listening Spaces

During the European Classical and Romantic periods, the spatial distribution of musicians generally adhered to the French ideal inspired by 19th-century military band traditions. In this widely adopted setup, space was typically divided into two main zones: musicians arranged on a frontal stage and the audience seated facing them. This concert model—linear, frontal, and fixed—reflected a formalized, hierarchical approach to the musical experience.

By the 20th century, this front-facing paradigm began to be reexamined and reimagined. Composers and sound artists started to question the limitations of conventional concert hall layouts. A notable example is found in 1962, when German composer Karlheinz Stockhausen proposed a radical redesign of the concert space to accommodate the evolving needs of contemporary composition. His vision included a circular venue without a fixed podium, flexible seating arrangements, adaptable ceiling and wall surfaces for mounting loudspeakers and microphones, suspended balconies for musicians, and configurable acoustic properties. These directives were not merely architectural; they were compositional in nature, meant to transform the listening experience by embedding spatiality into the core of musical structure.

This reconceptualization of space enabled a new way of thinking about the sensations and experiences that could be elicited in the listener through the manipulation of spatial audio. The concert hall was no longer a passive container of sound, but an active participant in its articulation.

In parallel developments, particularly in the field of electroacoustic music during the mid-20th century, spatial concerns were often shaped by the available technology of the time. Spatiality lacked a clearly defined theoretical vocabulary and was not yet integrated systematically with other familiar musical parameters. Nonetheless, from its inception, electroacoustic music leveraged technology to expand musical boundaries—either through electronic signal processing or by interacting with traditional instruments.

However, the notion of space in electroacoustic music differs significantly from that of instrumental music. In this context, spatiality is broad, multidimensional, and inherently difficult to define. It encompasses not only the individual sonic identity of each source—typically reproduced through loudspeakers—but also the relationships between those sources and the acoustic space in which they are heard. Critically, the audible experience of space unfolds through the temporal evolution of the sonic discourse itself.

Loudspeakers in electroacoustic music afford a uniquely flexible means of organizing space as a structural element. The spatial potential enabled by electroacoustic technologies has been, and continues to be, highly significant. With a single device (see Fig. 2), it is possible to transport the listener into a wide range of virtual environments, expanding the scope of musical space beyond the physical limitations of the performance venue. This opens the door to sonic representations of distance, movement, and directionality—factors that can be fully integrated into the musical structure and treated as compositional dimensions in their own right.

![**Figure 2**: Multichannel electroacoustic concert hall.  
](/assets/images/space/fig-2-multichannel-setup-AI.png)


### Spatial Recontextualization and Auditory Expectation

One of the defining characteristics of electroacoustic music is its capacity to recontextualize sound. Through virtual spatialization, a sound can acquire new meaning depending on how it functions within diverse contexts. For example, two sounds that would never naturally coexist—such as a thunderstorm and a mechanical engine in the same acoustic scene—can be juxtaposed to create a landscape that defies ecological realism. This deliberate disjunction can trigger a complex interaction between what is heard and the listener’s prior knowledge of those sound sources, drawn from personal experience.

Our ability to interpret spatial cues in sound is deeply shaped by the patterns of interpersonal communication we engage in, the lived experience of urban or rural environments, and the architectural features of our surroundings. These formative influences are so embedded in our perceptual systems that we are often unaware of their role in shaping how we understand sensory information. Spatial cues are constantly processed in our everyday auditory experience and play a vital role in shaping our listening behaviors.

In the context of electroacoustic music, environmental cues may not only suggest associations with a physical location but also inform more abstract properties of the spatial discourse articulated by the piece. Jean-Claude Risset [@risett1969] noted a tension in using environmental recordings in electroacoustic composition: the recognizable identity of “natural” sounds resists transformation without diminishing their spatial content. Nevertheless, identifiable sounds remain central to electroacoustic works, particularly those aligned with the tradition of musique concrète.

Under normal listening conditions, perception of a singular sound source is shaped by what Pierre Schaeffer [@schaeffer2003] defined as the “sound object”—a sonic phenomenon perceived as a coherent whole, grasped through a form of reduced listening that focuses on the sound itself, independent of its origin or meaning. Our auditory system has evolved to identify and locate such sound objects in space by mapping them according to the physical attributes of their sources. A barking sound is understood as coming from a dog; a voice is mapped to a human speaker. As a result, the spatial interpretation of sound is closely linked to the listener’s expectations, shaped not only by the inherent acoustic characteristics of the signal but also by the listener’s accumulated learning and experience.

Unlike incidental listening in everyday life, attentive listening in electroacoustic music generally occurs from a fixed position. Apart from minor head movements, the spatial information available to the listener depends entirely on the acoustic cues encoded in the sound. While spatial hearing has been extensively researched in the field of psychoacoustics, its compositional potential as a carrier of musical form remains underexplored. In particular, the dimension of distance has received little attention compared to azimuth (horizontal angle) and elevation (vertical angle). These two spatial dimensions have been rigorously studied and well-documented, forming the basis of many standard models of spatial perception.

By contrast, the auditory perception of distance remains one of the most enigmatic topics in both music and psychophysics. It involves a complex array of cues—many of which are still not fully understood or integrated into compositional practice. As such, distance remains an open and promising field of inquiry for creative and scientific exploration alike [@abregu2012].

![**Figure 3**: Schematic representation of the azimuth, elevation, and distance axes.  
](/assets/images/space/fig-3-space-xyz-2.png)

### The Expanded Spatial Palette of Electroacoustic Music

Unlike instrumental music, which relies on the physicality of pre-existing sound sources, electroacoustic music is not constrained by such limitations. This fundamental distinction allows composers not only to design entirely new sounds tailored to their spatial intentions but also to explore acoustic environments that defy conventional physical logic. While a performance of acoustic music offers visual and sonic cues grounded in the material world—a stage, a hall, visible instruments—electroacoustic music invites a more abstract engagement, where the listener navigates a virtual sound world.

In acoustic settings, physical space provides the listener with consistent spatial information. In contrast, electroacoustic environments—particularly those utilizing multichannel reproduction—radically extend the spatial canvas. These environments challenge and expand our auditory expectations, offering a broader spectrum of spatial strategies shaped by the interaction between sound diffusion technologies and the perceptual mechanisms of the listener.

Yet, this very potential also brings complications. The ephemeral and immaterial nature of loudspeaker-based sound makes it difficult to generate a spatial image as vivid or intuitive as that encountered in the physical world. Despite technological precision, virtual spatial environments often lack the tactile immediacy of real acoustic spaces.

Research in the field of computer music typically falls into two major domains: the study of spatial hearing and cognition, and the development of sound reproduction technologies. One of the advantages in electroacoustic contexts is the ability to isolate and manipulate spatial cues independently—a task nearly impossible with traditional acoustic instruments. This level of control opens up possibilities for treating space as a fully sculptable compositional parameter. However, practical results vary greatly, and many perceptual questions remain unresolved. It is within this intersection—where psychophysics, spatialization technologies, and compositional imagination meet—that the most fertile ground for innovation lies.

A classic example is John Chowning’s *Turenas* (1972), which employed quadraphonic speaker arrangements and a mathematical model to simulate virtual spatial motion. Chowning implemented a distance-dependent intensity multiplier according to the inverse square law, applied independently to each channel. The outcome was a virtual “phantom source” whose movement through space was dynamically shaped and perceptually engaging. Interestingly, Chowning discovered that simple, well-structured paths—such as basic geometric curves—produced the most convincing spatial gestures. In *On Sonic Art* [@wishart1996], one finds multiple visual representations of two-dimensional spatial trajectories; however, these diagrams often fail to translate into equally perceptible auditory experiences. For instance, while Lissajous curves are visually complex and elegant, their perceptual distinctiveness can be minimal. Clarity, it turns out, is key to audible spatial expression.

Since the mid-20th century, various spatialization systems have emerged to simulate acoustic spaces through loudspeaker arrays. Among the most notable are Intensity Panning, Ambisonics, and Dolby 5.1. Although a detailed technical comparison of these systems lies beyond the scope of this section (see [@basso2009] for an extensive overview), it is worth noting that electroacoustic space only becomes meaningful if its structural characteristics—such as reverberation—can be clearly perceived. In orchestral music, spatial features are often inherent to the instrument layout and performance context. With fixed instrument positions, the spatial configuration is relatively stable. In virtual space, however, reverberation must be explicitly created and shaped to define the acoustic character of the environment in which sonic events unfold.

Unlike the relatively fixed reverberation properties of physical rooms, virtual spaces offer remarkable plasticity. Composers can craft distinct acoustic identities for different sections of a piece, allowing each space to function as a compositional agent in its own right. This flexibility implies that in electroacoustic music, space must often be invented—not merely inherited from the sounding objects, as is typically the case with acoustic instruments.

It becomes clear, then, that spatial design can be a powerful structural element in music. Sound space carries poetic, aesthetic, and expressive dimensions. At the same time, convincing spatial construction often depends on our embodied experience of real-world acoustics. This dual grounding—in perceptual realism and creative abstraction—opens new theoretical and practical avenues for artists. The potential to reconceptualize spatiality through an expanded compositional lens can empower creators to engage more fully with the multidimensionality of sonic art. In doing so, it enables the development of richer analytical and taxonomical criteria that extend beyond sound itself, embracing broader cultural, perceptual, and technological contexts.


## Spatial Localization of Sound


### Psychoacoustic Factors in Sound Localization

When we perceive a sound event, we rely on a variety of auditory cues to infer the position of its source. These cues are associated with both **direction** and **distance**, and are typically classified into two main categories: **binaural cues**, which arise from comparisons between the two ears, and **monaural cues**, which are perceived by a single ear and are still critical for spatial localization.

To illustrate the binaural mechanism, consider a point sound source moving in a horizontal circular path around a listener's head. At any given moment, the distance from the source to each ear will differ depending on the angular position of the source. This spatial configuration results in **time and intensity differences** between the two ears. These discrepancies are key to our spatial perception of sound.

When a sound wave reaches the right ear before the left, the slight delay in arrival time is known as the **Interaural Time Difference (ITD)**, while the difference in loudness is called the **Interaural Level Difference (ILD)**. These cues are extremely subtle—at 90°, the maximum ITD is around **630 microseconds**—yet the brain is remarkably adept at interpreting them to infer the lateral position of the source.

The ILD is largely caused by the **acoustic shadowing effect of the head**. For higher frequencies (with wavelengths shorter than the head's diameter), the sound cannot diffract around the head, leading to more pronounced differences in intensity between the ears, depending on the angle of incidence.

These binaural cues work together most effectively between **800 Hz and 1600 Hz**. Below 800 Hz, ITD is the dominant cue; above 1600 Hz, ILD becomes more effective. The distance between the listener and the sound source also affects these cues, with ILD being particularly sensitive to distance changes, while ITD remains relatively stable.

Binaural localization does not require prior knowledge of the sound source. In contrast, **monaural localization**—when only one ear is used—often depends on recognizing the sound in advance. One key monaural cue is the **Spectral Cue (also called the Monaural Spectral Cue)**, which arises from changes in the sound spectrum caused by interactions with the outer ear (the pinna).

Even slight modifications in the signals reaching the auditory system can lead to significant shifts in the perceived spatial image. From an acoustic perspective, the **pinna acts as a directional filter**, primarily affecting high frequencies. It introduces spectral and temporal modifications to the signal depending on its angle of incidence and distance. These effects help the listener distinguish between sounds originating in front versus behind, and also assist in determining the **elevation** of the source.

Historically, the role of the pinnae in spatial hearing was underestimated, often seen merely as protective structures. Contemporary research, however, confirms their essential role in spatial perception and in reducing environmental noise, such as wind.

When it comes to estimating **source distance**, familiarity with the sound plays a significant role. For example, spoken voice at a normal volume provides a relatively accurate cue for distance perception. Like light intensity or visual size, **sound intensity decreases with distance**, following the **inverse square law**. But intensity is not the only factor: **air acts as a low-pass filter**, attenuating high frequencies over distance. This effect is clearly noticeable in scenarios like an approaching airplane, where the sound not only grows louder but also becomes brighter and richer in frequency content.

In sum, sound localization is a complex perceptual process informed by a combination of **binaural and monaural cues**, **spectral filtering by the outer ear**, and **familiarity with the sound source**. These mechanisms interact continuously to allow listeners to orient themselves in a dynamic acoustic environment, whether real or simulated.

## Hearing in Enclosed Spaces

When we experience sound in an enclosed environment, the first auditory cue that reaches our ears is known as the **direct sound**—the signal that travels straight from the source without interacting with any obstacles. This is followed by the **first-order reflections**, which occur when the sound bounces off a single surface—such as a wall—before reaching the listener. These are then succeeded by **second-order reflections**, involving two bounces, and progressively higher orders of reflection. Eventually, this cascade of reflections gives rise to a **diffuse auditory sensation** known as **reverberation**.

In a typical empty room—consider one with four walls, a ceiling, and a floor—the six primary surfaces contribute to the first-order reflections. These early reflections are particularly significant when the sound has a sharp or impulsive attack, as they assist the auditory system in identifying the **spatial position of the sound source**. Reverberation, by contrast, carries information about the **acoustic properties** and **dimensions** of the room. It plays a key role in shaping the overall perception of the space's material and geometry.

A critical metric used in room acoustics is the **reverberation time**, commonly denoted as *T₆₀*. This refers to the time it takes for the reverberant sound energy to decay by **60 decibels** after the original sound source has ceased. A widely used formula for estimating this value is the **Sabine equation**:
$$T_{60} = 0.16 \times \frac{V}{A}$$

Here, **V** represents the volume of the room in cubic meters, and **A** is the total **equivalent absorption area**, calculated as the sum of the products of each surface's area and its corresponding **absorption coefficient**. Importantly, absorption coefficients are **frequency-dependent**, meaning that the acoustic behavior of a room varies with different sound frequencies.

To account for this, acousticians often use a metric known as the **Noise Reduction Coefficient (NRC)**, which averages absorption values across key frequencies—specifically 250 Hz, 500 Hz, 1000 Hz, and 2000 Hz. This provides a more practical estimate of how a room absorbs sound across the human auditory spectrum.

A graphical representation of room acoustics typically illustrates the **temporal distribution of reflections** produced by an impulse response within the space. In such plots, the **height of each line** corresponds to the **relative amplitude** of each reflection. These visualizations help researchers and designers understand how sound energy evolves over time in an architectural space, offering insight into both **clarity** and **spatial impression**.

Understanding how direct sound, early reflections, and reverberation interact is essential for both technical and artistic applications in sound design. Whether composing for multichannel electroacoustic environments or designing spatial sound installations, these acoustic principles are fundamental to crafting immersive and intelligible auditory experiences.

## Virtual Source Simulation via Stereophonic Techniques

Stereophonic systems enable the simulation of virtual sound source localization using just two loudspeakers positioned to the left and right of the listener. By manipulating the amplitude of each signal sent to the speakers, we can generate the perceptual illusion of a sound source moving laterally across the auditory field. Additional transformations also allow us to simulate depth and even create the sensation of a source emanating from behind the speakers, constructing an illusory auditory space.

The perception of a virtual source positioned between two speakers arises from the system’s ability to artificially generate an **Interaural Level Difference (ILD)**. If the signal emitted from the right speaker is louder than that from the left, the right ear perceives a higher intensity, prompting the auditory system to localize the source toward the right. The degree of perceived lateral displacement is directly proportional to the intensity difference.

In typical stereo setups, the speakers are arranged with a 60° separation. However, for purposes of spatial expansion and compatibility with quadraphonic systems (involving four speakers surrounding the listener), we assume a 90° separation. We define 0° as the location of the left speaker and 90° as the right. The objective is to calculate the appropriate amplitude for each speaker to position the virtual source at a desired angle θ within this range.

To ensure the virtual source appears equidistant from the listener during its movement, the **total sound intensity** emitted by both speakers must remain constant. That is:

$$
I_L + I_R = k
$$

Assuming intensity is normalized between 0 and 1, we can simplify this to:

$$
I_L + I_R = 1
$$

Here, \( I_L \) and \( I_R \) represent the intensities emitted by the left and right speakers, respectively. When the source is fully localized at the left, \( I_L = 1 \) and \( I_R = 0 \); when at the right, \( I_L = 0 \) and \( I_R = 1 \); and when centered, both are \( 0.5 \). However, in most digital audio systems we operate on **amplitude**, not intensity. Since **intensity is proportional to the square of amplitude**, we use the relation:

$$
I \propto A^2
$$

Thus, the condition of constant intensity becomes:

$$
A_L^2 + A_R^2 = 1
$$

A common misconception is to linearly scale amplitudes (e.g., \( A_L = A_R = 0.5 \) when centered), but this would yield:

$$
0.5^2 + 0.5^2 = 0.25 + 0.25 = 0.5
$$

—only half the intended total intensity, which could be mistakenly perceived as an increase in distance. To preserve consistent intensity across the stereo field, we rely on a trigonometric identity:

$$
\sin^2(\theta) + \cos^2(\theta) = 1
$$

Using this, we can define the **amplitude assignments** as:

$$
A_R = \sin(\theta) \\
A_L = \cos(\theta)
$$

Where θ ranges from 0° (left) to 90° (right). For example, at 45°, both \( \sin(45^\circ) \) and \( \cos(45^\circ) \) equal approximately 0.707. Squaring and summing these values confirms that the total intensity remains 1:

$$
A_R^2 + A_L^2 = \sin^2(45^\circ) + \cos^2(45^\circ) = 0.5 + 0.5 = 1
$$

Thus, this method preserves the **perceptual coherence** of the virtual sound source’s position while maintaining a **constant energy output**, a critical aspect of spatial audio realism.

In summary, for any virtual angle \( \theta \in [0^\circ, 90^\circ] \), we can accurately simulate spatial position with consistent intensity using:

$$
A_R = \sin(\theta) \\
A_L = \cos(\theta)
$$

This technique forms the basis for further expansion into quadraphonic and multichannel spatialization systems.

## Implementing a Stereo Spatialization in Pure Data

Before diving into the implementation of a stereophonic spatialization system in Pure Data (Pd), it is essential to note that the objects used to compute trigonometric functions such as sine and cosine require the input angle to be expressed in **radians**, not degrees. Recall that one radian is the angle formed when the length of the arc of a circle equals the radius. From this definition, we derive the following equivalences:

- 360° = \(2\pi\) radians  
- 180° = \(\pi\) radians  
- 90° = \(\frac{\pi}{2}\) radians

To convert degrees to radians, we multiply the angle in degrees by the ratio \( \frac{2\pi}{360} \). To optimize performance and avoid redundant calculations, we can precompute this factor:

$$
\text{Angle}_{\text{rad}} = \text{Angle}_{^\circ} \times 0.0174533
$$

![Fig. ](/assets/screenshots/space/intenssity-panning.png){width=50%}

The patch illustrated below simulates a virtual sound source using stereophonic panning. This technique, commonly referred to as *intensity panning*, adjusts the balance between two loudspeakers. The source signal (from a subpatch named `sonido`) is split into two channels, each multiplied by the cosine and sine of the specified angle (in radians) to distribute the amplitude between the left and right speakers. As we vary the angle between 0° and 90°, the source appears to move along an arc, emulating the listener's perception of spatial position.

### Simulating Distance

To simulate distance, we must account for how amplitude diminishes as the source moves away from the listener. Unlike intensity, which decreases with the square of the distance, amplitude is inversely proportional to the distance:

$$
A \propto \frac{1}{d}
$$

Thus, the amplitude perceived by the listener is:

$$
A_{\text{perceived}} = \frac{A_{\text{source}}}{d}
$$

To manage both the **angle of lateralization** and the **distance to the source** simultaneously, we use the `slider2d` object from the `ELSE` library. This object provides a two-dimensional matrix that captures mouse pointer movement and returns the corresponding \(x, y\) coordinates.

![Fig. ](/assets/screenshots/space/distance-panning.png){width=80%}

We map the horizontal axis (x) to the angle (0°–90°) and the vertical axis (y) to the distance. These values are then routed via `[send]` and `[receive]` objects for use throughout the patch. It’s crucial to prevent division by zero in distance calculations, so we define the minimum distance as 1—equivalent to the baseline distance between the listener and speakers. If speakers are placed 2 meters away, then a distance of \(d = 1\) equals 2 meters in real space, \(d = 2\) would correspond to 4 meters, and so on. In this system, **distance is relative to the speaker-listener spacing**.

### Simulating Air Absorption

To further enhance the realism of our spatialization, we simulate **air absorption**, which—although minimal at short distances—contributes to the perceptual illusion of depth. As distance increases, high frequencies are attenuated, which we simulate using a **low-pass filter** (`[lop~]`).

![Fig. ](/assets/screenshots/space/distance-panning-lop.png)

We control the cutoff frequency of this filter using a `[expr]` object (expr (1 - $f1) * 7000 + 1000). For example, we can scale a distance range of 1–10 to a cutoff frequency range of 8000–1000 Hz. As the source moves farther away, the filter reduces the brightness of the sound, mimicking atmospheric filtering.

### Simulating Reverberant Spaces

Consider a large environment with significant reverberation—such as a church or garage. When someone speaks nearby, the **direct sound** dominates, and reverberation is minimal. Conversely, at greater distances, the direct signal weakens while reverberation remains relatively constant. This is due to the fact that reverberation is a function of the environment and not as sensitive to the source's location.

![Fig. ](/assets/screenshots/space/distance-panning-lop-reverb.png)

To simulate this phenomenon, we introduce a **reverberation chamber** using `[freeverb~]`. A subpatch named `reverb` handles this effect, and the reverb level is controlled via a rotary dial (`[knob]` from the `ELSE` library). By configuring the knob for a low initial setting, we simulate the acoustic behavior where reverberation becomes more prominent as the source moves farther from the listener. As the source approaches, the increased amplitude of the direct signal masks the reverberation, completing our auditory illusion.






