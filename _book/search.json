[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Creative Coding with Pure Data",
    "section": "",
    "text": "Preface\nThis book was born out of the classroom—but it does not stay there. It is the result of many years of teaching creative coding and interactive media at public universities, engaging with students from a wide range of disciplines, backgrounds, and interests. Over time, a common thread emerged: the desire to bridge artistic expression and technical skill, to write code not just as a means to an end, but as a space of exploration, experimentation, and play.\nIn these pages, you’ll find the distilled insights, exercises, and creative strategies that have shaped countless workshops and academic programs. The goal has always been twofold: to equip readers with the tools to build interactive digital systems, and to nurture a mindset where sound, movement, code, and structure can be explored as artistic materials. Whether it’s a generative soundscape, a data-driven artwork, or a custom tool built from scratch, the projects in this book are designed to foster technical growth while encouraging individual expression.\nThis is not a manual in the traditional sense, nor is it a fixed curriculum. Instead, this book invites you to engage with Pure Data on your own terms, navigating its chapters in whatever order best suits your curiosity. The modular structure is intentional: it supports creative detours, sudden insights, and unexpected connections between ideas. You are encouraged to experiment, remix, and stretch the boundaries of the examples presented. Treat the book as both a guide and a sandbox—one where art, sound, code, and interactivity come alive through your engagement.\nWhether you’re an artist exploring new tools, a programmer seeking creative outlets, or a student diving into the world of interactive media, I hope this book helps you discover how code can become a language for imagination.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-am-i",
    "href": "index.html#who-am-i",
    "title": "Creative Coding with Pure Data",
    "section": "Who am I?",
    "text": "Who am I?\nMy name is Ezequiel Abregú, and I am a sound artist, composer, multi-instrumentalist, and researcher originally from Buenos Aires, Argentina. My artistic practice encompasses sound recordings, audio installations, performances, sound sculptures, sound design, and compositions for chamber music, choreography, and theater. I am particularly interested in the interplay between music, performance, sound art, live electronics, auditory and visual perception, interactive media, and the application of technology in art.\n\n\n\n\nDr. Ezequiel Abregú\n\n\n\nI hold a Ph.D. focusing on the relationship between visual and auditory perception in sound art, and a degree in Composition with Electroacoustic Media from the National University of Quilmes (UNQ). My academic career includes teaching positions at several institutions: I am a professor at the University of Quilmes (Computing Applied to Music area), the National University of Arts (Multimedia Arts area), and the University of Tres de Febrero (Electronic Arts area).\nMy passion for programming and digital audio applications has led me to explore various programming languages and tools over the past two decades, including C, C++, Python, and Pure Data. I am an advocate of the open-source philosophy, regularly working with Linux and sharing my projects in publicly accessible repositories. My technical expertise extends to hardware development using microcontrollers and single-board computers, enabling me to adopt a hands-on approach in both my artistic and research endeavors.\nMore information about my work can be found on my personal website ezequielabregu.net.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "Creative Coding with Pure Data",
    "section": "What is this book about?",
    "text": "What is this book about?\nThe goal of this book is to support undergraduate and postgraduate students in exploring the intersection of creativity and technology alongside peers from diverse backgrounds. Building on years of teaching experience at several public universities1, this work encourages the integration of a creative mindset with programming skills to design original tools, algorithms, and artworks. Through this synthesis, the book invites students to engage with sound, interactivity, and control protocols in both technical and expressive dimensions.\nRather than offering a fixed, linear progression, the structure of this book is deliberately open and modular. Readers are encouraged to navigate the content according to their interests, needs, or curiosity. This flexibility supports an experimental approach to learning, where exploration and play are not only welcome but essential.\nBy approaching programming as a creative practice, this book invites you to think, make, and reflect through code. Whether you’re building an interactive sound installation, prototyping a digital instrument, or simply experimenting with new ideas, the goal is to empower you with the tools and concepts to express yourself in the digital domain—and to enjoy the process along the way.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Creative Coding with Pure Data",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book is intended for anyone interested in learning about creative coding using Pure Data. It is suitable for beginners who are new to programming and want to explore the world of interactive media, as well as for experienced programmers looking to expand their skills in sound synthesis and data visualization.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-you-going-to-learn",
    "href": "index.html#what-are-you-going-to-learn",
    "title": "Creative Coding with Pure Data",
    "section": "What are you going to learn?",
    "text": "What are you going to learn?\nIn this book, you will learn how to use Pure Data to create interactive audio-visual projects. You will explore various techniques for sound synthesis, data visualization, and interactive installations. The book will also cover best practices for organizing and managing your Pd projects, as well as tips for debugging and optimizing your code.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-you-dont-going-to-learn",
    "href": "index.html#what-are-you-dont-going-to-learn",
    "title": "Creative Coding with Pure Data",
    "section": "What are you don’t going to learn?",
    "text": "What are you don’t going to learn?\nThis book is not intended to be a comprehensive guide to all aspects of Pure Data. Instead, it focuses on specific topics and projects that are relevant to creative coding. While the book will cover a wide range of techniques and concepts, it will not delve into every detail or aspect of Pure Data. This book is not a substitute for the official Pure Data documentation or other resources. It is meant to complement these resources and provide a practical, hands-on approach to learning Pure Data.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#pre-requisites",
    "href": "index.html#pre-requisites",
    "title": "Creative Coding with Pure Data",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nTo get the most out of this book, you should have a basic understanding of programming and some familiarity with the Pure Data language.\nIf you are new to Pure Data or programming, there are several free online resources that can help you get started:\n\nPure Data FLOSS Manual – A beginner-friendly and comprehensive guide to Pure Data.\nOfficial Pure Data Documentation – The official manuals and reference materials for Pure Data.\nMiller Puckette’s “Theory and Techniques of Electronic Music” – A comprehensive book by the creator of Pure Data, covering both theory and practical techniques.\nProgramming Electronic Music in Pd - Designed for self-study, principally for composers. It begins with explanations of basic programming and acoustic principles then gradually builds up to the most advanced electronic music processing techniques.\nPatchstorage - A community-driven platform for sharing and discovering Pure Data patches. It features a wide range of projects, from simple examples to complex installations, and serves as a valuable resource for learning and inspiration.\n\nThese resources can be consulted before or alongside this book to strengthen your foundational knowledge.\n\nRecommended Pure Data distributions\nThis book is based on Pure Data Vanilla distribution, which is the most widely used version. You can download it from the official website:\nhttps://puredata.info/downloads/pure-data\nThis version is maintained by the original author, Miller Puckette, and is the most stable supported version of Pure Data. It includes all the core features and libraries needed for most projects, making it a great starting point for beginners and experienced users alike. Pure Data Vanilla is a free and open-source visual programming language for multimedia applications, widely used in the fields of sound synthesis, interactive installations, and live performance. It is available for Windows, macOS, and Linux operating systems.\nHowever, there are several other distributions that you may find useful for specific projects or needs:\n\nPurr Data – A fork of Pd-l2ork that focuses on usability and accessibility, with a more polished interface and additional features. Purr Data serves the same purpose, but offers a new and much improved graphical user interface and includes many 3rd party plug-ins. Like Pd, it runs on Linux, macOS and Windows, and is open-source throughout.\nPlugdata – plugdata is a plugin wrapper designed for Miller Puckette’s Pure Data (Pd), featuring an enhanced graphical user interface (GUI) created using JUCE, headed by Timothy Schoen. this project is still a work in progress, and may still have some bugs. By default, plugdata comes with the cyclone and ELSE collections of externals and abstractions.\n\n\n\nRecommended libraries and externals\nThe following libraries are recommended to be used with Pure Data. They are not included in the Pure Data Vanilla distribution, but they can be easily installed and used with it:\n\ncyclone\nELSE\niemlib\nlist-abs\nzexy\nGem\nceammc\ncomport\nmrpeach\nfreeverb~\njmmmp\nmapping\n\nThese libraries provide additional objects and features that can enhance your projects and make it easier to work with sound synthesis, data visualization, and interactive installations.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Creative Coding with Pure Data",
    "section": "Contributing",
    "text": "Contributing\nIf you would like to contribute to this book, please feel free to fork the repository and submit a pull request. I welcome any suggestions, corrections, or improvements to the content. You can also report issues or request features by opening an issue in the repository. You can find the source code for this book on GitHub.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Creative Coding with Pure Data",
    "section": "Contact",
    "text": "Contact\nIf you have any questions, comments, or feedback about this book, please feel free to reach out to me at eabregu.dev@gmail.com. I would love to hear from you!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Creative Coding with Pure Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to express my gratitude to the following individuals and organizations for their support and contributions to this book:\n\nPure Data for providing a powerful and flexible platform for creative coding.\nThe Pd community for their invaluable resources, tutorials, and support.\nThe open-source community for their dedication to sharing knowledge and tools for creative coding.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Creative Coding with Pure Data",
    "section": "License",
    "text": "License\nThis book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. You are free to share and adapt the material, provided you give appropriate credit, do not use it for commercial purposes, and distribute your contributions under the same license.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Creative Coding with Pure Data",
    "section": "",
    "text": "Multimedia Arts UNA, Electronic Arts UNTREF, Bachelor of Music and Technology UNQ, Master’s Degree in Sound Art UNQ, Doctorate in Arts UNA↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Pure Data?\nTraditionally, the process of creating software applications has relied heavily on text-based programming languages. Developers would write code manually in text files and execute them later to observe the outcome. While this method is efficient for those trained in programming, it often presents a steep learning curve for artists, musicians, and other creatives without a technical background. The abstract nature of text-based coding can act as a barrier to entry, especially for those more accustomed to working in visual or tactile media.\nPure Data offers a radically different approach by introducing a graphical programming environment tailored to creative exploration. Instead of writing lines of code, users work with visual representations of functions—called objects—that are placed and connected on a canvas. This paradigm allows users to construct interactive programs, known as “patches,” by literally drawing connections between elements. Each object can receive messages that modify its behavior in real time, creating a highly responsive and accessible system for building audio, visual, or multimedia tools.\nThe design of Pure Data echoes the modular synthesis systems of the 20th century, where sound was shaped by routing audio through a network of physical devices linked with patch cables. This historical reference not only grounds Pure Data in a legacy of experimental electronic music but also makes it intuitive for users familiar with analog workflows. By visually “patching” connections, users can simulate and extend these traditional processes in a digital context.\nTo fully engage with the creative potential of Pure Data, one can develop or modify algorithms for digital audio using imaginative coding strategies. For instance, building a real-time audio granulator that reinterprets live microphone input can offer both technical challenge and artistic reward. Through this lens, programming becomes a form of creative expression, blending logical structures with aesthetic decisions. By experimenting with signal flow, modulation techniques, or interactive sensors, users cultivate a mindset that is both inventive and analytical—unlocking new possibilities in digital sound design and performance.\nPure Data (or Pd) is a real-time graphical programming environment for audio, video, and graphical processing. Pure Data is commonly used for live music performance, VeeJaying, sound effects, composition, audio analysis, interfacing with sensors, using cameras, controlling robots or even interacting with websites. Because all of these various media are handled as digital data within the program, many fascinating opportunities for cross-synthesis between them exist. Sound can be used to manipulate video, which could then be streamed over the internet to another computer which might analyze that video and use it to control a motor-driven installation.\nProgramming with Pure Data is a unique interaction that is much closer to the experience of manipulating things in the physical world. The most basic unit of functionality is a box, and the program is formed by connecting these boxes together into diagrams that both represent the flow of data while actually performing the operations mapped out in the diagram. The program itself is always running, there is no separation between writing the program and running the program, and each action takes effect the moment it is completed.\nThe community of users and programmers around Pure Data have created additional functions (called “externals” or “external libraries”) which are used for a wide variety of other purposes, such as video processing, the playback and streaming of MP3s or Quicktime video, the manipulation and display of 3-dimensional objects and the modeling of virtual physical objects. There is a wide range of external libraries available which give Pure Data additional features. Just about any kind of programming is feasible using Pure Data as long as there are externals libraries which provide the most basic units of functionality required.\nThe core of Pure Data written and maintained by Miller S. Puckette and includes the work of many developers, making the whole package very much a community effort.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#why-pure-data",
    "href": "chapters/introduction.html#why-pure-data",
    "title": "1  Introduction",
    "section": "1.2 Why Pure Data?",
    "text": "1.2 Why Pure Data?\nPure Data (Pd) is a powerful, open-source environment for creative coding, offering a uniquely visual approach to programming that is especially suited for artists, musicians, and interactive media designers. Its intuitive interface and modular structure make it a flexible and accessible tool for developing real-time audio and visual projects—from live performances to experimental installations.\nOne of Pure Data’s standout features is its graphical programming interface, which replaces traditional lines of code with visual objects and patch cords. This allows users to construct complex behaviors by connecting elements on screen, making it easier to prototype and refine creative ideas. Real-time processing capabilities mean that audio and visual data can be generated, modified, and responded to instantly—ideal for performances, generative art, or interactive systems that react to sensors or user input.\nBeyond its creative potential, Pure Data offers practical advantages: it is cross-platform, running on Windows, macOS, and Linux, and integrates easily with tools like Arduino, Raspberry Pi, and Max/MSP, enabling hybrid systems that combine digital and physical components. A rich ecosystem of external libraries supports advanced functions like synthesis, visualization, and computer vision. Its open-source nature encourages exploration and collaboration, with a supportive community, extensive documentation, and countless tutorials available. Because it’s free and widely used in education, Pd is not only an effective tool for artistic expression but also a valuable learning resource for developing a strong foundation in programming and multimedia design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#what-is-creative-coding",
    "href": "chapters/introduction.html#what-is-creative-coding",
    "title": "1  Introduction",
    "section": "1.3 What is Creative Coding?",
    "text": "1.3 What is Creative Coding?\nCreative coding is the practice of using programming as a tool for artistic expression. It transforms code from a purely functional medium into a creative one, enabling the development of visual art, music, interactive experiences, and experimental media. This approach encourages artists, designers, and technologists to explore beyond the limits of traditional art forms, embracing code as a flexible and dynamic means of invention and communication.\nThe applications of creative coding are diverse, ranging from generative visuals and algorithmic design to responsive installations and live audiovisual performances. Creators often use languages and environments specifically designed to support creative work, such as Processing, OpenFrameworks, Max/MSP, and Pure Data. These platforms make it easier to manipulate data, control media in real time, and experiment with unconventional interfaces and outputs.\nImportantly, creative coding transcends the boundaries of specific disciplines. It can intersect with visual art, music, dance, theater, architecture, and even narrative writing. What unites these practices is the use of code as an expressive tool—one that invites innovation, play, and conceptual exploration. Closely tied to the values of the open-source movement, creative coding thrives in a culture of sharing, where artists and developers freely exchange code, tutorials, and ideas. This collaborative ecosystem fosters continuous learning and reinvention, empowering creators to expand what is possible through digital technology.\nFor an in-depth exploration of creative coding, consider checking out these resources:\n\nAwesome Creative coding - A curated list of resources, libraries, and tools for creative coding.\nCreative Code Berlin - A collection of creative coding resources, tutorials, and projects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#getting-started",
    "href": "chapters/introduction.html#getting-started",
    "title": "1  Introduction",
    "section": "1.4 Getting Started",
    "text": "1.4 Getting Started\nThis section will guide you through the installation of Pure Data Vanilla, the official and minimal distribution of Pure Data, on Windows, macOS, and Linux systems. The installation process is straightforward and will have you up and running in no time.\n\n1.4.1 Installing Pure Data Vanilla\nThe following is a step-by-step installation of Pure Data Vanilla, the official and minimal distribution of Pure Data, on Windows, macOS, and Linux systems.\n\n\n1.4.2 What is Pure Data “Vanilla”?\nPure Data (Pd) Vanilla is the standard version maintained by its original creator, Miller Puckette. It is lightweight, highly stable, and the most recommended version for beginners and experienced users alike.\n\n\n\n1.4.3 Installing on Windows\n\n1.4.3.1 Download the Installer\n\nVisit the official Pure Data website: https://puredata.info/downloads/pure-data\nScroll down to the Windows section.\nClick the latest version link (e.g., pd-0.55-2.windows-installer.exe).\n\n\n\n1.4.3.2 Run the Installer\n\nLocate the downloaded .exe file in your Downloads folder.\nDouble-click to run the installer.\nFollow the installation wizard:\n\nChoose the installation location (default is fine).\nAllow the program to create a Start Menu shortcut.\n\n\n\n\n1.4.3.3 Verify Installation\n\nAfter installation, open Pure Data from the Start Menu or desktop shortcut.\nThe main Pd window should appear with a blank patch ready.\n\n\n\n\n\n1.4.4 Installing on macOS\n\n1.4.4.1 Download the Disk Image\n\nVisit: https://msp.ucsd.edu/software.html\nScroll to the macOS section.\nDownload the .dmg file (e.g., Pd-0.54-1.dmg).\n\n\n\n1.4.4.2 Install the Application\n\nOpen the downloaded .dmg file.\nDrag the Pure Data icon into the Applications folder.\n\n\n\n1.4.4.3 Open Pure Data\n\nOpen your Applications folder.\nRight-click (or Ctrl + click) the Pure Data icon and select Open.\n\nThe first time, macOS may warn that the app is from an unidentified developer.\nConfirm to proceed.\n\n\n\n\n1.4.4.4 Optional: Enable Audio Permissions\n\nIf prompted, allow Pure Data to access the microphone.\nOpen System Preferences &gt; Security & Privacy &gt; Microphone and ensure Pure Data is enabled.\n\n\n\n\n\n1.4.5 Installing on Linux\nPure Data is available in the package repositories of most major Linux distributions. Below are instructions for popular systems.\n\n1.4.5.1 Debian/Ubuntu-based Systems\nsudo apt update\nsudo apt install puredata\n\n\n1.4.5.2 Fedora\nsudo dnf install puredata\n\n\n1.4.5.3 Arch Linux\nsudo pacman -S puredata\n\n\n1.4.5.4 Verify Installation\n\nOpen a terminal and type pd.\nThe Pure Data GUI should launch, displaying a blank patch.\nIf you encounter issues, check your package manager or consult the Pure Data community for troubleshooting.\n\n\n\n\n1.4.6 Installing Externals\nPure Data’s functionality can be extended through the use of externals—additional libraries that provide new objects and features. These externals are created by the Pd community and can add everything from new audio effects to advanced data processing tools. Installing externals is straightforward thanks to Pure Data’s built-in package manager.\n\n1.4.6.1 Step 1: Open Pure Data\n\nLaunch Pure Data (Pd) on your computer.\nMake sure you are using the Vanilla version for best compatibility with the package manager.\n\n\n\n1.4.6.2 Step 2: Access the “Find Externals” Tool\n\nIn the Pure Data menu, go to Help → Find Externals…\nThis opens the Deken package manager, which allows you to search for and install externals directly from within Pd.\n\n\n\n1.4.6.3 Step 3: Search for an External\n\nIn the search bar, type the name of the external or library you want to install (for example, zexy, cyclone, or iemlib).\nPress Enter or click the Search button.\nA list of matching externals will appear, showing available versions for your platform.\n\n\n\n1.4.6.4 Step 4: Install the External\n\nClick on the desired external in the list.\nChoose the version that matches your operating system and Pd version.\nClick Install.\nThe external will be downloaded and placed in your Pd externals folder (typically ~/Documents/Pd/externals on macOS and Linux, or C:\\Users\\&lt;YourName&gt;\\Documents\\Pd\\externals on Windows).\n\n\n\n1.4.6.5 Step 5: Add the External to Your Pd Path (if needed)\n\nMost externals are automatically available after installation.\nIf Pd cannot find the external, you may need to add its folder to Pd’s search path:\n\nGo to Preferences → Path…\nClick New and add the path to the external’s folder (for example, ~/Documents/Pd/externals/cyclone).\nClick OK and restart Pd.\n\n\n\n\n1.4.6.6 Step 6: Use the External in Your Patch\n\nIn your patch, create an object with the name of the external’s library, followed by the object you want to use.\nFor example, to use the counter object from the cyclone library:\n[cyclone/counter]\nSome libraries require you to load them with a special object, such as [declare -lib cyclone] at the top of your patch.\n\n\n\n1.4.6.7 Step 7: Verify Installation\n\nIf the object appears without errors (no red box), the external is installed correctly.\nIf you see errors, double-check the installation path and that you are using the correct object/library name.\n\n\n\n1.4.6.8 Manual Installation (Advanced)\nIf you need to install an external manually:\n\nDownload the external from https://deken.puredata.info/ or the developer’s website.\nExtract the files to your Pd externals folder.\nAdd the folder to Pd’s path as described above.\n\n\nTip: Always check the documentation for each external, as installation steps or requirements may vary.\n\nFor more details, see the official Pure Data documentation on externals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html",
    "href": "chapters/playrec.html",
    "title": "2  Rec & Play",
    "section": "",
    "text": "2.1 I’m Sitting in a Room – Alvin Lucier\nIn this chapter, we will explore the relationship between recording and playback. We will look at how these two processes can be used to create new sounds and compositions. We will also discuss the technical aspects of recording and playback, including the equipment and techniques used in the process. We will also consider the artistic implications of recording and playback, including how these processes can be used to create new forms of expression and communication.\nI’m Sitting in a Room (Alvin Lucier) consists of a 15-minute and 23-second sound recording. The piece opens with Lucier’s voice as he declares he is sitting in a room different from ours. His voice trembles slightly as he delivers the text, describing what will unfold over the next 15 minutes:\nAs listeners, we know what is going to happen, but we don’t know how it will happen (Hasse 2012). We listen, following Lucier’s recorded voice. Then, Lucier plays the recording into the room and re-records it. This time, we begin to hear more of the room’s acoustic characteristics. He continues to play back and re-record his voice—over and over—until his speech becomes softened, almost dissolved, into the sonic reflections of the room in which the piece was recorded and re-recorded.\nOne effective way to study a piece is to replicate its technical aspects and the devices involved. The aim of this activity is to recreate the technical setup of I’m Sitting in a Room, focusing on the processes of recording, playback, and automation.\nThere are many ways to implement this technical system. Just to mention two environments we’re currently working with: it’s relatively straightforward in Pure Data.\nIn terms of hardware, besides a computer, you’ll need a speaker (mono audio output), a microphone (mono audio input), and a semi-reverberant room.\nThe system should include at least two manual (non-automated) controls:\nChoose a room whose acoustic or musical qualities you’d like to evoke. Connect the microphone to the input of tape recorder #1. From the output of tape recorder #2, connect to an amplifier and speaker. Use the following text, or any other text of any length:\nThe following steps outline the process:\nAll the recorded generations, presented in chronological order, create a tape composition whose duration is determined by the length of the original statement and the number of generations produced. Make versions in which a single statement is recycled through different rooms. Create versions using one or more speakers in different languages and spaces. Try versions where, for each generation, the microphone is moved to different parts of the room(s). You may also develop versions that can be performed in real time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#im-sitting-in-a-room-alvin-lucier",
    "href": "chapters/playrec.html#im-sitting-in-a-room-alvin-lucier",
    "title": "2  Rec & Play",
    "section": "",
    "text": "“…I am recording the sound of my speaking voice and I am going to play it back into the room again and again…”\n\n\n\n\nI am sitting in a room design\n\n\n\n\n\n\n\n\nStart recording\nStop recording\n\n\n\nI am sitting in a room different from the one you are in now. I am recording the sound of my speaking voice, and I am going to play it back into the room again and again until the resonant frequencies of the room reinforce themselves so that any semblance of my speech, with perhaps the exception of rhythm, is destroyed. What you will hear, then, are the natural resonant frequencies of the room articulated by speech. I regard this activity not so much as a demonstration of a physical fact, but more as a way to smooth out any irregularities my speech might have.\n\n\n\nRecord your voice through the microphone into tape recorder #1.\nRewind the tape, transfer it to tape recorder #2, and play it back into the room through the speaker. - Record a second generation of the statement via the microphone back into tape recorder #1.\nRewind this second generation to the beginning and splice it to the end of the original statement in tape recorder #2.\nPlayback only the second generation into the room and record a third generation into tape recorder #1.\nContinue this process through multiple generations.\n\n\n\n2.1.1 Pure Data implementation of I’m sitting in a room\n \n\n\n\nPure Data implementation of I am sitting in a room\n\n\nIn this section, we will break down the Pure Data patch I-am-sitting-in-a-room.pd step by step. This patch is inspired by Alvin Lucier’s iconic piece and demonstrates how to recursively record and play back sound to reveal the resonant frequencies of a room.\n\n2.1.1.1 Conceptual Overview\nThe patch enables you to:\n\nRecord your voice or any sound in a room.\nPlay back the recording into the room and re-record it.\nRepeat this process, so the room’s resonances become more pronounced with each generation.\n\n\n\n\n2.1.2 System Diagram\nThe following diagram illustrates the recursive nature of the recording and playback process, similar to how Lucier’s piece unfolds. Each generation of tape recorders captures the previous one, creating a feedback loop that emphasizes the room’s resonances.\n\n\n\n\n\nflowchart TB\n  %% Top level source\n  Input[Microphone]\n\n  subgraph PLAYBACK RECORDERS\n    direction LR\n    subgraph RecorderA [DEVICE A]\n      RecordA[Record A.wav]\n      PlayA[Play A.wav]\n      RecordA -.-&gt; PlayA\n    end\n    \n    subgraph RecorderB [DEVICE B]\n      RecordB[Record B.wav]\n      PlayB[Play B.wav]\n      RecordB -.-&gt;PlayB\n    end\n  end\n  \n  %% Output at the bottom  \n  Output((Speaker))\n  \n  %% Clear connections showing signal flow\n  start([1-start]) --&gt; RecordA\n  stop([2-stop]) --&gt; RecordA\n  stop([2-stop]) --&gt; RecordB\n  stop([2-stop]) --&gt; PlayA\n\n  Input ==&gt; RecordA\n  Input ==&gt; RecordB\n\n  PlayA ==&gt; Output\n  PlayB ==&gt; Output\n  \n  %% Feedback path showing recursion\n  PlayB -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordA\n  PlayA -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordB\n  Output ==&gt; room[\"ROOM'S&lt;br&gt;REFLECTIONS\"]:::roomStyle ==&gt; Input\n\n  classDef roomStyle fill:#f5f5f5,stroke:#333,stroke-dasharray:5 5 \n\n\n\n\n\n\n \nThe following sequence diagram illustrates the process of recording and playback in the patch. It shows how the audio input is captured, recorded, played back, and re-recorded in a loop, emphasizing the room’s resonances.\n\n\n\n\n\n\nsequenceDiagram\n    participant Start as Start\n    participant Stop as Stop\n    participant Input as Mic\n    participant RecA as Rec A.wav\n    participant PlayA as Play A.wav\n    participant RecB as Rec B.wav\n    participant PlayB as Play B.wav\n    participant Output as Speaker\n    participant Room as Room\n\n    %% Generation 1\n    activate Input\n    Start--&gt;&gt;RecA: Start button\n    activate RecA\n    Input-&gt;&gt;RecA: Mic input\n\n\n    Stop--xRecA: Stop Rec A  \n    deactivate RecA   \n    Stop--&gt;&gt;RecB: Start Rec B\n    activate RecB \n    Input-&gt;&gt;RecB: Mic input          \n    Stop--&gt;&gt;PlayA: Start Play A\n    activate PlayA\n\n    PlayA-&gt;&gt;Output: Playback A.wav\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n    PlayA--xRecB: Trigger Stop Rec B\n    deactivate RecB\n    deactivate PlayA\n\n    PlayA--&gt;&gt;RecA: Trigger Start Rec B\n    activate RecA\n    Input-&gt;&gt;RecA: Mic input \n    PlayA--&gt;&gt;PlayB: Trigger Start Play A\n    activate PlayB\n\n    %% Generation 2\n    PlayB-&gt;&gt;Output: Playback B.wav\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n\n    %% Generation 3 (process repeats)\n    PlayB--xRecA: Trigger Stop Rec A\n    deactivate RecA\n    deactivate PlayB\n    PlayB--&gt;&gt;RecB: Trigger Start Rec B\n    activate RecB \n    Input-&gt;&gt;RecB: Mic input\n    PlayB--&gt;&gt;PlayA: Trigger Start Play A\n    activate PlayA\n    PlayA-&gt;&gt;Output: Playback\n    Output-&gt;&gt;Room: Acoustic Space\n    Room-&gt;&gt;Input: Acoustic reflections captured\n\n    Note over Input,Room: The process continues, alternating between devices and reinforcing the room's resonances.\n    deactivate Input\n    deactivate PlayA\n    deactivate RecB\n\n\n\n\n\n\n\n\n\n2.1.3 Step 1: Audio Input and Routing\n\nadc~ receives audio from your microphone.\nThe signal is sent to s~ audio.input, making it available to other parts of the patch.\nThis modular routing allows the same input to be used for both recording and playback chains.\n\n\n\n2.1.4 Step 2: Recording Your Voice (record-A.wav)\n\nPress the Start Recording button (bng labeled “Start Recording”).\nThis triggers a message:\nopen record-A.wav, start → writesf~\nThe incoming audio from your microphone is now being recorded to record-A.wav.\nPress the Stop Recording button (bng labeled “Stop Recording”) to send the stop message, ending the recording.\n\n\n\n2.1.5 Step 3: Playback and Re-recording (record-B.wav)\n\nTo play back your recording, another button triggers:\nopen record-A.wav, start → readsf~\nThe playback signal is routed to:\n\nthrow~ audio (so you hear it through the speakers)\nwritesf~ (recording to record-B.wav)\n\nThis means the playback of your first recording is re-recorded, capturing both the original sound and the room’s response.\n\n\n\n2.1.6 Step 4: Recursive Process\n\nYou can repeat the process:\n\nPlay back record-B.wav and record it again, each time reinforcing the room’s resonances.\n\nEach iteration makes the speech less intelligible and the room’s resonant frequencies more prominent, just like in Lucier’s piece.\n\n\n\n2.1.7 Step 5: Output\n\nAll audio routed to throw~ audio is collected by catch~ audio and sent to dac~ for playback through your speakers or headphones.\n\n\n\n2.1.8 Key Objects Table\n\n\n\nObject\nPurpose\n\n\n\n\nadc~\nAudio input from microphone\n\n\nwritesf~\nRecord audio to a file\n\n\nreadsf~\nPlay audio from a file\n\n\nthrow~/catch~\nMix and route audio signals\n\n\ndac~\nAudio output to speakers\n\n\nbng\nButton for triggering actions\n\n\nmsg\nSend commands to objects (open, start, stop)\n\n\n\n\n\n2.1.9 How to Use the Patch\n\nStart Recording:\nClick the “Start Recording” button and speak or make a sound.\nStop Recording:\nClick the “Stop Recording” button to finish.\nPlayback and Re-record:\nUse the playback button to play your recording into the room and simultaneously re-record it.\nRepeat:\nRepeat the playback and re-recording process as many times as you like to hear the room’s resonances emerge.\n\nThis patch is a practical and creative way to explore acoustic phenomena and the transformation of sound through recursive recording, echoing the spirit of Lucier’s original work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#i-am-sitting-in-a-freeverb-patch-walkthrough",
    "href": "chapters/playrec.html#i-am-sitting-in-a-freeverb-patch-walkthrough",
    "title": "2  Rec & Play",
    "section": "2.2 I am sitting in a [freeverb~] — Patch Walkthrough",
    "text": "2.2 I am sitting in a [freeverb~] — Patch Walkthrough\nThe patch I-am-sitting-in-a-room-freeverb.pd extends the original I am sitting in a room concept by introducing a virtual acoustic environment using reverb and allowing for pre-recorded audio input instead of just live microphone input. This adaptation provides more creative flexibility and consistent results compared to using a physical room.\n\n\n\nPure Data implementation of I am sitting in a room with freeverb\n\n\nThis patch is designed to simulate the recursive recording process of Lucier’s piece while allowing for more control over the sound environment. It uses the freeverb~ object to create a virtual room effect, enabling you to manipulate the acoustic characteristics of the sound without relying on a physical space.\n\n2.2.1 Conceptual Overview\nThe patch enables you to: - Use a pre-recorded audio file OR live input as your source material - Add artificial reverb to simulate room characteristics - Follow the same recursive recording process as the original - Achieve more controlled, repeatable results\n\n\n2.2.2 System Diagram\n\n\n\n\n\nflowchart TB\n  %% Top level source\n  AudioFile[Audio File]\n  \n  %% Horizontal arrangement of recorders A and B\n  subgraph PLAYBACK RECORDERS\n    direction LR\n    subgraph RecorderA [DEVICE A]\n      RecordA[writesf~ A.wav]\n      PlayA[readsf~ A.wav]\n      RecordA ==&gt; PlayA\n    end\n    \n    subgraph RecorderB [DEVICE B]\n      RecordB[writesf~ B.wav]\n      PlayB[readsf~ B.wav]\n      RecordB ==&gt; PlayB\n    end\n  end\n  \n  subgraph VIRTUAL ROOM\n  %% Reverb block in the middle\n    VirtualRoom(((freeverb~&lt;br&gt;Virtual Room)))\n  end  \n  \n  %% Output at the bottom  \n  Output[dac~]\n  \n  %% Clear connections showing signal flow\n  AudioFile --&gt; RecordA\n  AudioFile --&gt; Output\n  \n  PlayA ==&gt; VirtualRoom\n  PlayB ==&gt; VirtualRoom\n  \n  VirtualRoom ==&gt; RecordB\n  VirtualRoom ==&gt; RecordA\n  VirtualRoom --&gt; Output\n  \n  %% Feedback path showing recursion\n  PlayB -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordA\n  PlayA -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordB\n\n\n\n\n\n\nThis diagram illustrates the recursive nature of the recording and playback process, similar to how Lucier’s piece unfolds. Each generation of tape recorders captures the previous one, creating a feedback loop that emphasizes the room’s resonances.\n\n\n2.2.3 Unique Features of the [freeverb~] Version\n\n2.2.3.1 1. Audio File Input\nUnlike the original patch that only uses microphone input, this version can:\n\nPlay a pre-recorded audio file (speech.wav) as the initial source\nProcess this file through reverb before recording\nTrigger playback with a button (labeled “2. Playback audio”)\n\n\n\n\n\n\nflowchart LR\n    bng([bng]) --&gt; msg[\"msg open speech.wav, 1\"]\n    msg --&gt; readsf[\"readsf~\"]\n    readsf --&gt; freeverb[\"freeverb~\"]\n    freeverb --&gt; send[\"s~ audio.input\"]\n\n\n\n\n\n\n\n\n2.2.3.2 2. Virtual Acoustic Environment\nInstead of relying on a physical room’s acoustics, this patch uses freeverb~ objects to create a simulated acoustic space:\n\nEvery audio signal (input and playback) passes through a freeverb~ object\nThis creates consistent reverberation that can be adjusted and controlled\nMultiple freeverb~ objects process different stages of the audio for cumulative effects\n\n\n\n2.2.3.3 3. Enhanced Control Flow\nThe patch includes numbered controls for better workflow:\n\n\n\n\n\n\n\n\nButton\nLabel\nFunction\n\n\n\n\n1\nStart recording\nBegins recording the input source to record-A.wav\n\n\n2\nPlayback audio\nPlays the speech.wav file through reverb into the system\n\n\n3\nStop recording\nStops the current recording process\n\n\n\n\n\n2.2.3.4 4. Multiple Send/Receive Paths\nThe patch uses additional send/receive pairs to manage signal routing:\n\ns~/r~ audio.input - Routes input signals to the recording object\ns~/r~ player.A - Routes playback from file A to recorder B\ns~/r~ player.B - Routes playback from file B back to recorder A\nthrow~/catch~ audio - Collects all signals to be sent to the output\n\n\n\n\n2.2.4 Step-by-Step Explanation\n\n2.2.4.1 Step 1: Initial Audio Source Options\nThis patch provides two possible sources for the initial recording:\n\nPre-recorded file input:\n\nClick “2. Playback audio” button to play speech.wav\nThe file is played through readsf~, processed by freeverb~, and sent to s~ audio.input\n\nLive input: (not explicitly shown in this version but could be added using adc~)\n\nAny signal sent to s~ audio.input will be recorded when recording starts\n\n\n\n\n2.2.4.2 Step 2: Recording the First Generation\n\nClick “1. Start recording” to begin recording to record-A.wav\nThe signal from s~ audio.input is captured by r~ audio.input and sent to writesf~\nThe resulting file contains your source material with initial reverb applied\n\n\n\n2.2.4.3 Step 3: Playback and Re-recording\n\nAfter stopping recording with “3. Stop recording”, the patch automatically:\n\nOpens and plays record-A.wav through readsf~\nProcesses it through another freeverb~ for additional reverb\nSends it to both throw~ audio (for monitoring) and s~ player.A\nThe player.A signal is received and recorded to record-B.wav\n\n\n\n\n2.2.4.4 Step 4: Recursive Process\n\nWhen record-B.wav is created, it can be played back through another readsf~\nThis signal is also processed by freeverb~\nThe output is sent to both throw~ audio and s~ player.B\nThe player.B signal could be received and recorded back to record-A.wav\nThis cycle can continue, with each generation accumulating more of the virtual room’s characteristics\n\n\n\n2.2.4.5 Step 5: Output\n\nAll audio signals sent to throw~ audio are collected by catch~ audio\nThe mixed signal is sent to out~ for playback through speakers/headphones\n\n\n\n\n2.2.5 Key Objects Table\n\n\n\n\n\n\n\nObject\nPurpose in This Patch\n\n\n\n\nfreeverb~\nCreates virtual room acoustics by adding reverberation\n\n\nreadsf~\nPlays audio files (source material or previous generations)\n\n\nwritesf~\nRecords audio to file (for each generation)\n\n\ns~/r~\nRoutes audio between different parts of the patch\n\n\nthrow~/catch~\nMixes and outputs all audio signals\n\n\ndel\nAdds small delays to ensure proper sequencing of operations\n\n\nbng\nProvides buttons for user interaction\n\n\nout~\nSends audio to speakers/headphones\n\n\n\n\n\n2.2.6 Creative Applications\n\nExperiment with reverb settings: Try different room sizes, damping, and wet/dry mix\nUse different source materials: Test various speech samples, musical phrases, or sound effects\nCreate hybrid processes: Record the first generation in a real room, then switch to the virtual environment\nBuild compositional sequences: Layer different generations to create evolving textures\nCompare real vs. virtual: Process the same source through both the original and freeverb patches to contrast physical and virtual acoustics\n\nThis virtual approach offers a controlled laboratory for exploring the core concepts of Lucier’s piece, making it accessible even without an ideal acoustic space, while also opening new creative possibilities that wouldn’t be possible in a physical implementation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#references",
    "href": "chapters/playrec.html#references",
    "title": "2  Rec & Play",
    "section": "References",
    "text": "References\n\n\n\n\nHasse, Sarah. 2012. “I Am Sitting in a Room.” Body, Space & Technology 11. https://doi.org/10.16995/bst.71.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html",
    "href": "chapters/sequencers.html",
    "title": "3  Sequencers",
    "section": "",
    "text": "3.1 Arrays and Sequencers\nIn this chapter, we will explore the concept of sequencers and how they can be implemented in Pure Data.\nSequencer is a tool for organizing and controlling the playback of events in a temporally ordered sequence. This sequence consists of a series of discrete steps, where each step represents a regular time interval and can contain information about event activation or deactivation.\nIn addition to controlling musical events such as notes, chords, and percussions, a step sequencer can also manage a variety of other events. This includes parameter changes in virtual instruments or audio/video synthesizers, real-time effect automation, lighting control in live performances or multimedia installations, triggering of samples to create patterns and effect sequences, as well as firing MIDI control events to operate external hardware or software.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html#arrays-and-sequencers",
    "href": "chapters/sequencers.html#arrays-and-sequencers",
    "title": "3  Sequencers",
    "section": "",
    "text": "P1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\n\n\n\n\nx\nx\nx\nx\nx\nx\nx\nx\n\n\ni0\ni1\ni2\ni3\ni4\ni5\ni6\ni7\n\n\n\n\n\nEach cell represents a step in the sequencer.\n“P1” to “P8” indicate the steps numbered 1 to 8.\nEmpty cells represent steps without events or notes.\nYou can fill each cell with notes or events to represent the desired sequence.\n\n\n3.1.1 Represent an 8-step sequencer using pseudo code \n\n3.1.1.1 Formulation (pseudo code)\nData Structures:\n- Define a data structure to represent each step of the sequence (e.g., array, list).\n\nVariables:\n- Define an array to store the MIDI note values for each step.\n- Initialize the sequencer parameters:\n  - CurrentStep = 0\n  - Tempo\n  - NumberOfSteps = 8\n\nAlgorithm:\n1. Initialization:\n   a. Set CurrentStep to 0.\n   b. Ask the user to input the Tempo.\n   c. Set NumberOfSteps to 8.\n\n2. Loop:\n   a. While true:\n      i. Calculate the time duration for each step based on the Tempo.\n      ii. Play the MIDI note corresponding to the CurrentStep.\n      iii. Increment CurrentStep.\n      iv. If CurrentStep exceeds NumberOfSteps, reset it to 0.\n      v. Wait the calculated duration before moving to the next step.\n \n\n\n\n3.1.2 Python Implementation of the 8-Step Sequencer\nimport time\nimport mido\n\n# 1. Define a data structure to represent each step in the sequence (8 steps)\nsecuencia = [None] * 8\n\n# 2. Define an array to store MIDI note values for each step\nnotas_midi = [60, 62, 64, 65, 67, 69, 71, 72]  # C major scale\n\n# 3. Initialize the sequencer parameters\npaso_actual = 0\ntempo = 120  # beats per minute\nnum_pasos = len(secuencia)\n\n# Open the virtual MIDI port (use the correct port name, see MIDI_port_name.py)\npuerto_salida = mido.open_output('IAC Driver Bus 1')\n\n# 4. Start a loop to continuously play the sequence\nwhile True:\n    # a. Calculate the time duration for each step based on the tempo\n    duracion_paso = 60.0 / tempo  # beat duration in seconds\n\n    # b. \"Play\" the MIDI note corresponding to the current step\n    nota_actual = int(notas_midi[paso_actual])  # Ensure it's an integer\n    secuencia[paso_actual] = nota_actual\n    print(f\"Step: {paso_actual} --- Note: {secuencia[paso_actual]}\")\n\n    # Send the MIDI message\n    mensaje = mido.Message('note_on', note=nota_actual)\n    puerto_salida.send(mensaje)\n\n    # c. Increment the current step\n    paso_actual += 1\n\n    # d. If current step exceeds the number of steps, reset it to 0\n    if paso_actual &gt;= num_pasos:\n        paso_actual = 0\n\n    # e. Wait for the calculated duration before moving to the next step\n    time.sleep(duracion_paso)\n\n    # Turn off the MIDI note\n    mensaje = mido.Message('note_off', note=nota_actual)\n    puerto_salida.send(mensaje)\nThis is a simple step sequencer for MIDI. It uses the mido library to send MIDI messages and the time library to control the timing of the sequence.\nThe script starts by importing the necessary libraries and defining some data structures and initial variables. The secuencia list will hold the sequence of notes to be played, and is initially filled with None values. The notas_midi list contains the MIDI note values for each step in the sequence, representing a C Major scale.\nThen, the script initializes some sequencer parameters. The paso_actual variable tracks the current step in the sequence, tempo sets the tempo in beats per minute, and num_pasos stores the total number of steps in the sequence.\nThe script opens a virtual MIDI output port using mido.open_output(). You may need to change the port name ‘IAC Driver Bus 1’ depending on your system setup.\nThe main part of the script is a while loop that continuously plays the sequence. For each step, it calculates the step duration based on the tempo, plays the corresponding MIDI note, increments the current step, and waits for the calculated duration before moving to the next step. If the current step exceeds the number of steps in the sequence, it resets to 0. After each note is played, a note_off MIDI message is sent to stop the note.\n \n\n\n3.1.3 Represent a random sequencer using pseudo code\n \n\n\n3.1.4 Random Step Sequencer Implementation\nimport time\nimport mido\nimport random\n\n# 1. Define a data structure to represent each step of the sequence (8 steps)\nsecuencia = [None] * 8\n\n# 2. Define an array to store MIDI note values for each step\nnotas_midi = [60, 62, 64, 65, 67, 69, 71, 72]  # C major scale\n\n# 3. Initialize the sequencer parameters\npaso_actual = random.randint(0, 7)\ntempo = 120  # beats per minute\nnum_pasos = len(secuencia)\n\n# Open the virtual MIDI port (use the correct port name, see MIDI_port_name.py)\npuerto_salida = mido.open_output('IAC Driver Bus 1')\n\n# 4. Start a loop to continuously play the sequence\nwhile True:\n    # a. Calculate the time duration for each step based on the tempo\n    duracion_paso = 60.0 / tempo  # beat duration in seconds\n\n    # b. \"Play\" the MIDI note corresponding to the current step\n    nota_actual = int(notas_midi[paso_actual])  # Ensure it's an integer\n    secuencia[paso_actual] = nota_actual\n    print(f\"Step: {paso_actual} --- Note: {secuencia[paso_actual]}\")\n\n    # Send the MIDI message\n    mensaje = mido.Message('note_on', note=nota_actual)\n    puerto_salida.send(mensaje)\n\n    # c. Randomize the next step\n    paso_actual = random.randint(0, 7)\n\n    # d. If current step exceeds the number of steps, reset it to 0\n    if paso_actual &gt;= num_pasos:\n        paso_actual = 0\n\n    # e. Wait for the calculated duration before moving to the next step\n    time.sleep(duracion_paso)\n\n    # Turn off the MIDI note\n    mensaje = mido.Message('note_off', note=nota_actual)\n    puerto_salida.send(mensaje)\nThis Python script is a modified version of the step-by-step MIDI sequencer from the previous example. The main difference lies in how the current step (paso_actual) is incremented.\nIn the original script, the current step increased sequentially, creating a predictable pattern of notes. In this modified version, the current step is set to a random integer between 0 and 7 (inclusive). This means that the note sequence will be played in a random order, creating a more unpredictable pattern.\n—",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html#piano-phase-steve-reich",
    "href": "chapters/sequencers.html#piano-phase-steve-reich",
    "title": "3  Sequencers",
    "section": "3.2 Piano Phase (Steve Reich)",
    "text": "3.2 Piano Phase (Steve Reich)\n\nView Piano Phase Score (S. Reich)\nPiano Phase can be conceived as an algorithm. Starting with two pianos playing the same sequence of notes at the same speed, one of the pianists begins to gradually accelerate their tempo.\nWhen the separation between the notes played by both pianos reaches a fraction of a note’s duration, the phase is shifted and the cycle repeats. This process continues endlessly, creating a hypnotic effect of rhythmic phasing that evolves over time as an infinite sequence of iterations.\n\n\n\nPhase Difference\n\n\n\n\n\n  Performance and visualization of the first section from Steve Reich’s 1967 piece Piano Phase\n\n\nFigure 3.1\n\n\n\n\n3.2.1 Piano Phase process as an algorithm.\n\n3.2.1.1 Algorithm Formulation (pseudo code)\n1. Initialize two pianists with the same note sequence.\n2. Set an initial tempo for both pianists.\n3. Repeat until the desired phase is reached:\n     a. Gradually increase the second pianist's tempo.\n     b. Compare note positions between both pianists.\n     c. When the separation between the notes reaches a fraction of a note’s duration, invert the phase.\n     d. Continue playback.\n4. Repeat the cycle indefinitely to create a continuous and evolving rhythmic phasing effect.\n\n\n\n3.2.2 Piano Phase Implementation in Python (single sequencer)\nimport time\nimport mido\n\n# Define a MIDI note sequence to represent the musical part\nnotas_midi = [64, 66, 71, 73, 74, 66, 64, 73, 71, 66, 74, 73]  # C major scale\n\n# Open two MIDI output ports\npuerto_salida1 = mido.open_output('IAC Driver Bus 1')\n\n# Initialize the sequencer parameters\ntempo = int(120 * 4)  # beats per minute\nduracion_paso = 60.0 / tempo  # beat duration in seconds\n\n# Start a loop to continuously play the sequence\npaso_actual = 0\nwhile True:\n    # \"Play\" the MIDI note corresponding to the current step\n    nota_actual = notas_midi[paso_actual]\n    mensaje_on = mido.Message('note_on', note=nota_actual)\n    mensaje_off = mido.Message('note_off', note=nota_actual)\n\n    puerto_salida1.send(mensaje_on)\n    time.sleep(duracion_paso)  # Wait one beat before going to the next step\n    puerto_salida1.send(mensaje_off)  # Send 'note_off' message\n\n    # Increment the current step\n    paso_actual += 1\n    if paso_actual &gt;= len(notas_midi):\n        paso_actual = 0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html#random-melody-generator",
    "href": "chapters/sequencers.html#random-melody-generator",
    "title": "3  Sequencers",
    "section": "3.3 Random Melody Generator",
    "text": "3.3 Random Melody Generator\nThis chapter provides a step-by-step explanation of the music-scale-B.pd Pure Data patch. The patch demonstrates how to generate a melody using a musical scale, store it in an array, and play it back using MIDI.\n\n\n\nRandom Melody Generator\n\n\n\n3.3.1 Patch Overview\nThe patch is organized into several functional blocks:\n\nScale and Melody Generation\nMelody Storage\nPlayback Control\nMIDI Output\n\nBelow is a simplified diagram of the main data flow:\n\n\n\n\n\nflowchart TD\n    A[Scale Input] --&gt; B[Melody Generation]\n    B --&gt; C[Store in Array]\n    C --&gt; D[Playback Control]\n    D --&gt; E[MIDI Output]\n\n\n\n\n\n\n\n\n\n3.3.2 Scale and Melody Generation\n\n3.3.2.1 Scale Definition\nThe patch starts with a message box containing the scale intervals:\n0 2 4 5 7 9 11\nFor example, this represents a major scale in semitones accordding to the following mapping:\n\n\n\n\nNote\nInterval\n\n\n\n\nC\n0\n\n\nC# / Db\n1\n\n\nD\n2\n\n\nD# / Eb\n3\n\n\nE\n4\n\n\nF\n5\n\n\nF# / Gb\n6\n\n\nG\n7\n\n\nG# / Ab\n8\n\n\nA\n9\n\n\nA# / Bb\n10\n\n\nB\n11\n\n\n\n\n \nThe scale is appended to a list and processed to determine its length.\n\n\n3.3.2.2 Random Note Selection\nA random number between 0 and 47 is generated (random 48), then shifted up by 60 to get a MIDI note in a typical range.\nThe note is then mapped to the scale using modulo operations and list indexing.\n\n\n3.3.2.3 Melody Construction\nThe patch uses a loop (until, i, + 1) to generate a sequence of notes.\nEach note is calculated based on the scale and stored in a list.\n\n\n\n3.3.3 Melody Storage\nThe generated melody is stored in a Pure Data array called melody.\nThe array is visualized in the patch for reference.\n\n\n\n\n\ngraph LR\n    MelodyList --&gt;|tabwrite| MelodyArray\n\n\n\n\n\n\n\n\n3.3.4 Playback Control\nA metro object (metronome) triggers playback at a tempo set by a horizontal slider.\nEach bang from the metronome advances an index, which reads the next note from the melody array.\n\n\n3.3.5 MIDI Output\nThe note value is sent to a makenote object, which creates a MIDI note with velocity and duration.\nThe note is then sent to the noteout object, which outputs the MIDI note to your system’s MIDI device.\n\n\n3.3.6 Step-by-Step Data Flow\n\nInitialize Scale: The scale intervals are defined and appended to a list.\nGenerate Melody: A loop generates random notes mapped to the scale, storing them in the melody array.\nPlayback: A metronome triggers reading from the array, sending notes to MIDI output.\nVisualization: The melody array is displayed as a graph in the patch.\n\n\n\n3.3.7 Key Objects and Their Roles\n\n\n\n\nObject\nPurpose\n\n\n\n\nrandom 48\nGenerates random note indices\n\n\n+ 60\nShifts notes to a higher MIDI octave\n\n\nmod 12\nMaps notes to scale degrees\n\n\nlist-idx\nRetrieves scale degree from the list\n\n\ntabwrite\nWrites notes to the melody array\n\n\nmetro\nControls playback timing\n\n\ntabread\nReads notes from the melody array\n\n\nmakenote\nCreates MIDI notes with velocity/duration\n\n\nnoteout\nSends MIDI notes to output\n\n\n\n\n\n\n3.3.8 Diagram: Melody Generation and Playback\n\n\n\n\n\nflowchart TD\n    S[Scale Message] --&gt; L[list Append]\n    L --&gt; R[random + Offset]\n    R --&gt; M[Modulo/Indexing]\n    M --&gt; A[Melody Array]\n    A --&gt; P[Playback - metro, tabread]\n    P --&gt; MN[makenote]\n    MN --&gt; NO[noteout]\n\n\n\n\n\n\n\n\n3.3.9 Summary\nThis patch demonstrates how to algorithmically generate a melody using a musical scale, store it, and play it back via MIDI in Pure Data. The modular structure allows for easy experimentation with different scales, lengths, and playback parameters.\n\nHere is the translated version of your Markdown text with the code, links, and format preserved:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html#the-euclidean-algorithm-generates-traditional-musical-rhythms",
    "href": "chapters/sequencers.html#the-euclidean-algorithm-generates-traditional-musical-rhythms",
    "title": "3  Sequencers",
    "section": "3.4 The Euclidean Algorithm Generates Traditional Musical Rhythms",
    "text": "3.4 The Euclidean Algorithm Generates Traditional Musical Rhythms\nGodfried Toussaint (2005)\n\n\n\nEucliden image\n\n\nLink to the original article\n\n3.4.1 Summary\nThe Euclidean algorithm (as presented in Euclid’s Elements) calculates the greatest common divisor of two given integers. It is shown here that the structure of the Euclidean algorithm can be used to efficiently generate a wide variety of rhythms used as timelines (ostinatos) in sub-Saharan African music in particular, and world music in general. These rhythms, here called Euclidean rhythms, have the property that their onset patterns are distributed as evenly as possible. Euclidean rhythms also find application in nuclear physics accelerators and computer science and are closely related to several families of words and sequences of interest in the combinatorics of words, such as Euclidean strings, with which the rhythms are compared.\n\n\n3.4.2 Hypothesis\nSeveral researchers have observed that rhythms in traditional world music tend to exhibit patterns distributed as regularly or evenly as possible.\nPatterns of maximal evenness can be described using the Euclidean algorithm on the greatest common divisor of two integers.\n\n\n3.4.3 Patterns of Maximal Evenness\nThe Pattern of Maximal Evenness is a concept used in music theory to create Euclidean rhythms. Euclidean rhythms are rhythmic patterns that evenly distribute beats over a time cycle.\nIn essence, the Pattern of Maximal Evenness seeks to distribute a specific number of beats evenly within a given time span. This is achieved by dividing the time into equal parts and assigning beats to these divisions uniformly.\nExample:\nx = beat\n· = rest\n[×· ×· ×· ×· ] → [1 0 1 0 1 0 1 0] (8,4) = 4 beats evenly distributed over 8 pulses\n[×· ·×· ·×·] → [1 0 0 1 0 0 1 0] (8,3) = 3 beats evenly distributed over 8 pulses\n\n\n3.4.4 Euclidean Algorithm\nOne of the oldest known algorithms, described in Euclid’s Elements (around 300 BCE) in Proposition 2 of Book VII, now known as the Euclidean algorithm, calculates the greatest common divisor of two given integers.\nThe idea is very simple. The smaller number is repeatedly subtracted from the larger until the larger becomes zero or smaller than the smaller one, in which case it becomes the remainder. This remainder is then repeatedly subtracted from the smaller number to obtain a new remainder. This process continues until the remainder is zero.\nTo be more precise, consider the numbers 5 and 8 as an example:\n\nFirst, we divide 8 by 5. This gives a quotient of 1 and a remainder of 3.\nThen, we divide 5 by 3, which gives a quotient of 1 and a remainder of 2.\nNext, we divide 3 by 2, which gives a quotient of 1 and a remainder of 1.\nFinally, we divide 2 by 1, which gives a quotient of 2 and a remainder of 0.\n\nThe idea is to keep dividing the previous divisor by the remainder obtained in each step until the remainder is 0. When we reach a remainder of 0, the previous divisor is the greatest common divisor of the two numbers.\nIn short, the process can be seen as a sequence of equations:\n8 = (1)(5) + 3\n5 = (1)(3) + 2\n3 = (1)(2) + 1\n2 = (1)(2) + 0\nNote: 8 = (1)(5) + 3 means that 8 is divided by 5 once, yielding a quotient of 1 and a remainder of 3.\n \nIn essence, it involves successive divisions to find the greatest common divisor of two positive numbers (GCD from now on).\nThe GCD of two numbers a and b, assuming a &gt; b, is found by first dividing a by b, and obtaining the remainder r.\nThe GCD of a and b is the same as that of b and r. When we divide a by b, we obtain a quotient c and a remainder r such that:\na = c · b + r\nExamples:\nLet’s compute the GCD of 17 and 7.\nSince 17 = 7 · 2 + 3, then GCD(17, 7) is equal to GCD(7, 3). Again, since 7 = 3 · 2 + 1, then GCD(7, 3) is equal to GCD(3, 1). Here, it is clear that the GCD between 3 and 1 is simply 1. Therefore, the GCD between 17 and 7 is also 1.\nGCD(17,7) = 1\n\n17 = 7 · 2 + 3\n\n7 = 3 · 2 + 1\n\n3 = 1 · 3 + 0\n \nAnother example:\nGCD(8,3) = 1\n\n8 = 3 · 2 + 2\n\n3 = 2 · 1 + 1\n\n2 = 1 · 2 + 0\n\n\n3.4.5 How does computing the GCD turn into maximally even distributed patterns?\nRepresent a binary sequence of k ones [1] and n − k zeros [0], where each [0] bit represents a time interval and the ones [1] indicate signal triggers.\nThe problem then reduces to:\nConstruct a binary sequence of n bits with k ones such that the ones are distributed as evenly as possible among the zeros.\nA simple case is when k evenly divides n (with no remainder), in which case we should place ones every n/k bits. For example, if n = 16 and k = 4, then the solution is:\n[1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0]\nThis case corresponds to n and k having a common divisor of k (in this case 4).\nMore generally, if the greatest common divisor between n and k is g, we would expect the solution to decompose into g repetitions of a sequence of n/g bits.\nThis connection with greatest common divisors suggests that we could compute a maximally even rhythm using an algorithm like Euclid’s.\n \n\n\n3.4.6 Example (13, 5)\nLet’s consider a sequence with n = 13 and k = 5.\nSince 13 − 5 = 8, we start with a sequence consisting of 5 ones, followed by 8 zeros, which can be thought of as 13 one-bit sequences:\n[1 1 1 1 1 0 0 0 0 0 0 0 0]\nWe begin moving zeros by placing one zero after each one, creating five 2-bit sequences, with three remaining zeros:\n[10] [10] [10] [10] [10] [0] [0] [0]\n13 = 5 · 2 + 3\nThen we distribute the three remaining zeros similarly, placing a [0] after each [10] sequence:\n[100] [100] [100] [10] [10]\n5 = 3 · 1 + 2\nWe now have three 3-bit sequences, and a remainder of two 2-bit sequences. So we continue the same way, placing one [10] after each [100]:\n[10010] [10010] [100]\n3 = 2 · 1 + 1\nThe process stops when the remainder consists of a single sequence (here, [100]), or we run out of zeros.\nThe final sequence is, therefore, the concatenation of [10010], [10010], and [100]:\n[1 0 0 1 0 1 0 0 1 0 1 0 0]\n2 = 1 · 2 + 0\n \n\n\n3.4.7 Example (17, 7)\nSuppose we have 17 pulses and want to evenly distribute 7 beats over them.\n1. We align the number of beats and silences (7 ones and 10 zeros):\n[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n\n\n\n1 1 1 1 1 1 1\n0 0 0 0 0 0 0 0 0 0\n\n\n\n\n\n2. We form 7 groups, corresponding to the division of 17 by 7; we get 7 groups of [1 0] and 3 remaining zeros [000], which means the next step forms 3 groups until only one or zero groups remain.\n[1 0] [1 0] [1 0] [1 0] [1 0] [1 0] [1 0] [0] [0] [0]\n17 = 7 · 2 + 3\n\n\n\n1\n1\n1\n1\n1\n1\n1\n0 0 0\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n \n3. Again, this corresponds to dividing 7 by 3. In our case, we are left with only one group and we are done.\n \n[1 0 0] [1 0 0] [1 0 0] [1 0] [1 0] [1 0] [1 0]\n\n\n\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n[1 0 0 1 0] [1 0 1 0 0] [1 0 0 1 0] [1 0]\n7 = 3 · 2 + 1\n\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n1\n1\n1\n\n\n\n\n\n\n0\n0\n0\n\n\n\n\n\n\n\n \n4. Finally, the rhythm is obtained by reading the grouping column by column, from left to right, step by step.\n[1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0]\n3 = 1 · 3 + 0\n \n\n\n3.4.8 Implementation in Pure Data - Euclidean sequencer\n(index * hits ) % steps\n↓\n[&lt; notes]\nwhere:\nindex = index of the Euclidean series (array)\nhits = number of notes to be played\nsteps = array size\n\n\n3.4.9 References\n\nThe Euclidean Algorithm Generates Traditional Musical Rhythms\nPiano-Phase\n\n\nHere is the full English translation of your markdown document, keeping the code, links, and formatting as requested:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html#cellular-automata",
    "href": "chapters/sequencers.html#cellular-automata",
    "title": "3  Sequencers",
    "section": "3.5 Cellular Automata",
    "text": "3.5 Cellular Automata\n\n\n\nCellular Automaton\n\n\nA cellular automaton is a mathematical and computational model for a dynamic system that evolves in discrete steps. It is suitable for modeling natural systems that can be described as a massive collection of simple objects interacting locally with each other.\nA cellular automaton is a collection of “colored” cells on a grid of specified shape that evolves through a series of discrete time steps according to a set of rules based on the states of neighboring cells. The rules are then applied iteratively over as many time steps as desired.\nCellular automata come in a variety of forms and types. One of the most fundamental properties of a cellular automaton is the type of grid on which it is computed. The simplest such grid is a one-dimensional line. In two dimensions, square, triangular, and hexagonal grids can be considered.\nOne must also specify the number of colors (or distinct states) k that a cellular automaton can assume. This number is typically an integer, with k=2 (binary, [1] or [0]) being the simplest choice. For a binary automaton, color 0 is commonly referred to as “white” and color 1 as “black”. However, cellular automata with a continuous range of possible values can also be considered.\nIn addition to the grid on which a cellular automaton resides and the colors its cells can assume, one must also specify the neighborhood over which the cells influence each other.\n\n3.5.1 One-Dimensional Cellular Automata (1DCA)\nThe simplest option is \"nearest neighbors\", where only the cells directly adjacent to a given cell can influence it at each time step. Two common neighborhoods in the case of a two-dimensional cellular automaton on a square grid are the so-called Moore neighborhood (a square neighborhood) and the von Neumann neighborhood (a diamond-shaped neighborhood).\nCellular automata are considered as a vector. Each component of the vector is called a cell. Each cell is assumed to take only two states:\n\n[0] (white)\n[1] (black)\n\nThis type of automaton is known as an elementary one-dimensional cellular automaton (1DCA). A dynamic process is performed, starting from an initial configuration C(0) of each of the cells (stage 0), and at each new stage, the state of each cell is calculated based on the state of the neighboring cells and the cell itself in the previous stage.\n\n\n3.5.2 The Case of Rule 30\nRule 30 is a binary one-dimensional cellular automaton introduced by Stephen Wolfram in 1983. It considers an infinite one-dimensional array of cellular automaton cells with only two states, with each cell in some initial state.\nAt discrete time intervals, each cell changes state spontaneously based on its current state and the state of its two neighbors.\nWhat it consists of:\n\nEach cell can be in one of two states: alive or dead.\nThe next generation of a cell is determined by the current state of the cell and the state of its two neighboring cells.\nThere are 8 possible configurations of neighboring states (3^2), and for each, Rule 30 defines whether the cell lives or dies in the next generation.\n\nFor Rule 30, the set of rules that governs the automaton’s next state is:\n\n\n\nConfiguration\nNext State\nBinary Code\n\n\n\n\n000\nDead\n000\n\n\n001\nAlive\n011\n\n\n010\nDead\n000\n\n\n011\nAlive\n011\n\n\n100\nAlive\n011\n\n\n101\nDead\n000\n\n\n110\nAlive\n011\n\n\n111\nDead\n000\n\n\n\nIn the following diagram, the top row shows the state of the central cell (cell i) and its two neighboring cells at a given stage, and the bottom row shows the state of the central cell in the next stage.\nFor example, in the first case of the figure:\n\nif the state of a cell at a given stage is [1] (black) and\nits two neighbors at that stage are also [1] (black),\nthen the cell’s state in the next stage will be [0] (white).\n\n Source\nIt is called Rule 30 because in binary, 000111102 = 30.\nLet’s break down the procedure for determining the next state in the 8 combinations:\nInput configurations: Consider the 8 possible input combinations of the three cells (a central cell and its left and right neighbors). Since each cell can be in one of two possible states (0 or 1), the combinations are 000, 001, 010, 011, 100, 101, 110, and 111.\nBinary representation of the rule: Rule 30 is represented by the number 30 in binary, which is 00011110. This binary representation determines the rules for the next state of the central cell for each of the 8 possible combinations.\nBit correspondence: The 8 bits of the binary representation (00011110) correspond to the 8 input combinations in order. From right to left, the bits represent the next state of the central cell for each input combination.\nFor example, the least significant bit of 00011110 (the rightmost bit) is 0. This means that when the input combination is 000, the next state of the central cell will be 0.\nNext state determination: For each input combination (e.g., 000, 001, 010, 011, 100, 101, 110, 111), the corresponding bit in the binary representation of Rule 30 [00011110] indicates the next state of the central cell.\n\n\n\nInput Configuration\nNext State\n\n\n\n\n000\n0\n\n\n001\n1\n\n\n010\n1\n\n\n011\n1\n\n\n100\n1\n\n\n101\n0\n\n\n110\n0\n\n\n111\n0\n\n\n\nFor example, if the input combination is 000 and the corresponding bit in Rule 30 is 0, then the next state of the central cell will be 0.\nTherefore, the rules are not arbitrary but are determined by the binary representation of Rule 30, which specifies the next state of the central cell for each input combination of its neighbors.\n\n\n3.5.3 Rule 30 Implementation in Python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef rule30(cells):\n    \"\"\"Applies Rule 30 to the input cells.\"\"\"\n    new_cells = np.zeros_like(cells)\n    extended_cells = np.concatenate(([cells[-1]], cells, [cells[0]]))  # Apply periodic boundary conditions\n    for i in range(1, len(extended_cells) - 1):\n        neighborhood = extended_cells[i-1:i+2]\n        if np.array_equal(neighborhood, [1, 1, 1]) or np.array_equal(neighborhood, [1, 1, 0]) or \\\n           np.array_equal(neighborhood, [1, 0, 1]) or np.array_equal(neighborhood, [0, 0, 0]):\n            new_cells[i-1] = 0\n        else:\n            new_cells[i-1] = 1\n    return new_cells\n\ndef main():\n    # Initialize the cells\n    cells = np.zeros(100)\n    cells[50] = 1  # Start with one cell in the middle\n\n    # Apply Rule 30 for 100 steps\n    history = [cells]\n    for i in range(100):\n        cells = rule30(cells)\n        history.append(cells)\n\n    # Display the history as an image\n    plt.imshow(history, cmap='binary')\n    plt.show()\n\nif __name__ == \"__main__\":\n    main()\nThis Python script uses the numpy and matplotlib libraries to simulate and visualize the evolution of a cellular automaton using Rule 30.\nThe rule30 function is the core of this script. It takes a one-dimensional array of cells as input, where each cell is either 0 or 1. The function first creates a new array new_cells with the same shape as the input, filled with zeros. It then extends the input array at both ends to apply periodic boundary conditions, meaning that the first cell is considered a neighbor of the last cell and vice versa.\nNext, the function iterates over each cell in the extended array (excluding the added boundary cells). For each cell, it considers the cell and its two neighbors as a neighborhood. If the neighborhood matches one of four specific patterns ([1, 1, 1], [1, 1, 0], [1, 0, 1], or [0, 0, 0]), the corresponding cell in new_cells is set to 0. Otherwise, it is set to 1. This is the implementation of Rule 30. Finally, the function returns the new array of cells.\nThe main function initializes a one-dimensional array of 100 cells, all set to 0 except the middle cell, which is set to 1. It then applies the rule30 function to this array 100 times, storing each resulting array in a list called history. This list is then visualized as a binary image using matplotlib’s imshow function, where each row corresponds to one step in the cellular automaton’s evolution.\nThe script is designed to run as a standalone program. The line if __name__ == \"__main__\": ensures that the main function is called only when the script is run directly, not when imported as a module.\n\n\n3.5.4 Two-Dimensional Cellular Automata (2DCA)\nTwo-dimensional cellular automata are an extension of one-dimensional cellular automata, where cells not only have neighbors to the left and right, but also above and below. This allows modeling of more complex and structurally rich systems, such as two-dimensional phenomena like wave propagation, growth patterns in biology, fire spread, fluid simulation, among others.\nTwo-dimensional cellular automata (2DCA) are computational models that simulate dynamic systems on a two-dimensional grid.\nThey consist of:\n\nCells: Each cell in the grid can have a finite state, such as alive or dead, and can change state according to the automaton’s rules.\nNeighborhood: Each cell has a neighborhood, which is a set of adjacent cells that influence its state. The neighborhood can be rectangular, hexagonal, circular, or of any other shape.\nTransition rule: The transition rule defines how the state of a cell changes based on its current state and the state of the cells in its neighborhood. The rule can be deterministic or probabilistic.\nEvolution: The cellular automaton evolves through iterations. In each iteration, the state of each cell is updated according to the transition rule.\n\nHere is the full translation of your markdown file into English, with all code, links, and formatting preserved:\n\n\n\n3.5.5 Case Study: Conway’s Game of Life\n\n\n\nGame of life\n\n\nSource\nConway’s Game of Life, also known simply as the Game of Life, is a cellular automaton devised by British mathematician John Horton Conway in 1970. It is the best-known example of a cellular automaton.\nThe “game” is actually a zero-player game, meaning its evolution is determined by its initial state, requiring no further input from human players. One interacts with the Game of Life by creating an initial configuration and observing how it evolves.\nView Original Article - MATHEMATICAL GAMES - The fantastic combinations of John Conway’s new solitaire game life (Martin Gardner)\nGOL (Game of Life) and CGOL (Conway’s Game of Life) are commonly used acronyms.\n\n\n\nGame of Life\n\n\nSource\n\n3.5.5.1 Rules\nThe universe of the Game of Life is an infinite two-dimensional orthogonal grid of square cells, each of which (at any given time) is in one of two possible states, alive (alternatively “on”) or dead (alternatively “off”). At each time step, the following transitions occur:\nThe four rules of Conway’s Game of Life:\n1. Overpopulation: Any live cell with more than three live neighbors dies due to overpopulation.\n2. Underpopulation: Any live cell with fewer than two live neighbors dies due to underpopulation.\n3. Stability: Any live cell with two or three live neighbors survives to the next generation.\n4. Reproduction: Any dead cell with exactly three live neighbors becomes a live cell.\n \nWe can also summarize the rules in a table:\n\n\n\n\n\n\n\n\n\n\nRule\nDescription\nCurrent State\nLive Neighbors\nNext State\n\n\n\n\nOverpopulation\nDeath by overcrowding\nAlive\nMore than 3\nDead\n\n\nUnderpopulation\nDeath by isolation\nAlive\nFewer than 2\nDead\n\n\nStability\nSurvival\nAlive\n2 or 3\nAlive\n\n\nReproduction\nBirth\nDead\n3\nAlive\n\n\n\n \nOr summarized as:\n0 → 3 live neighbors → 1 1 → &lt; 2 or &gt; 3 live neighbors → 0\nWhere 0 represents a dead cell and 1 a live cell.\n \n\n\n\nGame of life rules\n\n\nSource\n \n\n\n\nGame of life rules\n\n\nSource\nThe initial pattern constitutes the system’s ‘seed’. The first generation is created by applying the above rules simultaneously to each cell in the seed; births and deaths occur simultaneously, and the discrete moment in which this occurs is sometimes called a step. (In other words, each generation is a pure function of the previous one.) The rules continue to be applied repeatedly to create more generations.\n\n\n\n3.5.6 Origins\nConway was interested in a problem presented in the 1940s by renowned mathematician John von Neumann, who was trying to find a hypothetical machine that could build copies of itself and succeeded when he found a mathematical model for such a machine with very complicated rules on a rectangular grid. The Game of Life emerged as Conway’s successful attempt to simplify von Neumann’s ideas.\nThe game made its first public appearance in the October 1970 issue of Scientific American, in Martin Gardner’s “Mathematical Games” column, under the title “The fantastic combinations of John Conway’s new solitaire game ‘Life’”.\nSince its publication, Conway’s Game of Life has attracted significant interest due to the surprising ways patterns can evolve.\nLife is an example of emergence and self-organization. It is of interest to physicists, biologists, economists, mathematicians, philosophers, generative scientists, and others, as it shows how complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counterintuitive notion that “design” and “organization” can spontaneously arise in the absence of a designer.\nConway carefully selected the rules, after considerable experimentation, to meet three criteria:\n\nThere should be no initial pattern for which a simple proof exists that the population can grow without limit.\nThere must be initial patterns that appear to grow indefinitely.\nThere should be simple initial patterns that evolve and change for a considerable period before ending in one of the following ways:\n\ndying out completely (due to overcrowding or becoming too sparse); or\nsettling into a stable configuration that remains unchanged thereafter, or entering an oscillating phase in which they repeat a cycle endlessly of two or more periods.\n\n\n\n\n3.5.7 Patterns\nMany different types of patterns occur in the Game of Life, including static patterns (“still lifes”), repeating patterns (“oscillators” – a superset of still lifes), and patterns that move across the board (“spaceships”). Common examples of these three classes are shown below, with live cells in black and dead cells in white.\n\n\n\nGosper glider gun\n\n\nUsing the provided rules, you can investigate the evolution of simple patterns:\n\n\n\n3 cells\n\n\n\n\n\n4 cells\n\n\nSource\nPatterns that evolve for long periods before stabilizing are called Methuselahs, the first of which discovered was the R-pentomino.\n\n\n\nR-pendomino\n\n\nDiehard is a pattern that eventually disappears, rather than stabilizing, after 130 generations, which is believed to be the maximum for initial patterns with seven or fewer cells.\n\n\n\nDiehard\n\n\nAcorn takes 5,206 generations to produce 633 cells, including 13 escaping gliders.\n\n\n\nAcorn\n\n\nConway originally conjectured that no pattern could grow indefinitely; that is, for any initial configuration with a finite number of live cells, the population could not grow beyond some finite upper bound. The Gosper glider gun pattern produces its first glider in the 15th generation, and another every 30 generations thereafter.\n\n\n\nGosper’s glider gun\n\n\n\n\n\nGosper glider gun\n\n\nFor many years, this pattern was the smallest known. In 2015, a gun called Simkin glider gun was discovered, which emits a glider every 120 generations, and has fewer live cells but is spread across a larger bounding box at its ends.\n\n\n\nSimkin glider gun\n\n\nSource \nSource\n\n\n3.5.8 Python Implementation of Game of Life\nimport time\nimport pygame\nimport numpy as np\n\nCOLOR_BG = (10, 10, 10,)  # Color de fondo\nCOLOR_GRID = (40, 40, 40)  # Color de la cuadrícula\nCOLOR_DIE_NEXT = (170, 170, 170)  # Color de las células que mueren en la siguiente generación\nCOLOR_ALIVE_NEXT = (255, 255, 255)  # Color de las células que siguen vivas en la siguiente generación\n\npygame.init()\npygame.display.set_caption(\"conway's game of life\")  # Título de la ventana del juego\n\n# Función para actualizar la pantalla con las células\ndef update(screen, cells, size, with_progress=False):\n    updated_cells = np.zeros((cells.shape[0], cells.shape[1]))  # Matriz para almacenar las células actualizadas\n\n    for row, col in np.ndindex(cells.shape):\n        alive = np.sum(cells[row-1:row+2, col-1:col+2]) - cells[row, col]  # Cálculo de células vecinas vivas\n        color = COLOR_BG if cells[row, col] == 0 else COLOR_ALIVE_NEXT  # Color de la célula actual\n\n        if cells[row, col] == 1:  # Si la célula actual está viva\n            if alive &lt; 2 or alive &gt; 3:  # Si tiene menos de 2 o más de 3 vecinos vivos, muere\n                if with_progress:\n                    color = COLOR_DIE_NEXT\n            elif 2 &lt;= alive &lt;= 3:  # Si tiene 2 o 3 vecinos vivos, sigue viva\n                updated_cells[row, col] = 1\n                if with_progress:\n                    color = COLOR_ALIVE_NEXT\n        else:  # Si la célula actual está muerta\n            if alive == 3:  # Si tiene exactamente 3 vecinos vivos, revive\n                updated_cells[row, col] = 1\n                if with_progress:\n                    color = COLOR_ALIVE_NEXT\n\n        pygame.draw.rect(screen, color, (col * size, row * size, size - 1, size - 1))  # Dibuja la célula en la pantalla\n\n    return updated_cells  # Devuelve las células actualizadas\n\n# Función principal del programa\ndef main():\n    pygame.init()\n    screen = pygame.display.set_mode((800, 600))  # Crea la ventana del juego\n\n    cells = np.zeros((60, 80))  # Crea una matriz de células muertas\n    screen.fill(COLOR_GRID)  # Rellena la pantalla con el color de la cuadrícula\n    update(screen, cells, 10)  # Actualiza la pantalla con las células\n\n    pygame.display.flip()\n    pygame.display.update()\n\n    running = False  # Variable para controlar si el juego está en ejecución\n\n    while True:\n        for Q in pygame.event.get():\n            if Q.type == pygame.QUIT:  # Si se cierra la ventana, termina el programa\n                pygame.quit()\n                return\n            elif Q.type == pygame.KEYDOWN:\n                if Q.key == pygame.K_SPACE:  # Si se presiona la tecla espacio, se inicia o pausa el juego\n                    running = not running\n                    update(screen, cells, 10)\n                    pygame.display.update()\n            if pygame.mouse.get_pressed()[0]:  # Si se presiona el botón izquierdo del ratón\n                pos = pygame.mouse.get_pos()  # Obtiene la posición del ratón\n                cells[pos[1] // 10, pos[0] // 10] = 1  # Marca la célula correspondiente como viva\n                update(screen, cells, 10)\n                pygame.display.update()\n\n        screen.fill(COLOR_GRID)  # Rellena la pantalla con el color de la cuadrícula\n\n        if running:  # Si el juego está en ejecución\n            cells = update(screen, cells, 10, with_progress=True)  # Actualiza las células con progreso\n            pygame.display.update()\n\n        time.sleep(0.001)  # Espera un breve tiempo para controlar la velocidad del juego\n\nif __name__ == \"__main__\":\n    main()\n\nThis Python script implements Conway’s Game of Life, a cellular automaton devised by British mathematician John Horton Conway. It is a zero-player game, meaning its evolution is determined entirely by its initial state, with no further input required.\nThe script begins by importing the necessary modules: time, pygame for the graphical interface, and numpy to handle the game grid as a 2D matrix. It then defines color constants used for visualizing the game.\nThe pygame.init() function is called to initialize all imported Pygame modules. The window title is set to “Conway’s Game of Life” using pygame.display.set_caption().\nThe update() function updates the game state and redraws the grid. It takes four arguments: screen (the Pygame surface to draw on), cells (the current game state as a 2D numpy array), size (the pixel size of each cell), and with_progress (a boolean indicating whether to display cells that will change in the next generation).\nThe function creates a new 2D matrix updated_cells filled with zeros, matching the shape of cells. It then iterates over each cell, calculates the number of live neighbors, and applies the game rules to determine whether the cell will be alive in the next generation. The function draws each cell on the screen using the appropriate color and returns updated_cells.\nThe main() function initializes Pygame, creates the game window, and initializes the game state as a 2D numpy array of zeros (representing dead cells). It then enters the main loop, which handles Pygame events (such as closing the window or key presses), updates the game state if it is running, and redraws the grid. The game can be started or paused by pressing the spacebar, and cells can be toggled manually by clicking on them.\nFinally, the script calls main() to launch the game. Press the spacebar to begin.\n\n\n3.5.9 Further Reading\n\nThe Game Of Life – Emergence In Generative Art (2020)\nConway’s Game of Life – Life on Computer by Swaraj Kalbande\nWikipedia – Conway’s Game of Life\n\n\n\n3.5.10 Interactive Websites\n\nJohn Conway’s Game of Life – An Introduction to Cellular Automata\nconwaylife.com\nCellular Automata Megathread",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html",
    "href": "chapters/synthesis.html",
    "title": "4  Synthesis",
    "section": "",
    "text": "4.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#oscillatory-movements",
    "href": "chapters/synthesis.html#oscillatory-movements",
    "title": "4  Synthesis",
    "section": "4.2 Oscillatory Movements",
    "text": "4.2 Oscillatory Movements\nOscillatory motion will be analyzed in detail—not only in terms of technical implementation but also through its creative and conceptual applications. Using example patches provided in both Pure Data and VCV Rack, we will explore how oscillation becomes an expressive tool in interactive works and live sound experiments. This approach will form the core of the module’s practical component, where students are encouraged to adapt the exercises to their own interests: whether by replicating existing works, developing new compositions, or analyzing the results of comparative experiments between different oscillators and their spectral behaviors.\n\n4.2.1 Between Technique and Aesthetics\nThis chapter begins with the idea of delving deeper into oscillatory movements from both a technical and aesthetic perspective. Building upon tools introduced in previous chapters we will work with oscillators and explore their many applications. Drawing on the text The Poetics of Signal Processing by Jonathan Sterne and Tara Rodgers, we will approach signal processing not merely as a set of mathematical operations, but as a form of artistic expression. In this context, signal manipulation becomes more than just a means to produce sound—it becomes a way to articulate broader cultural ideas about sound in the digital age.\nWe will explore what could be described as the “rawness” of analog oscillators—that tangible, organic quality inherent in their operation. We will examine how this rawness both contrasts with and complements the digital techniques we’ve developed so far. The use of oscillators in their various forms will serve as a starting point for a series of hands-on exercises, where theory and practice are deeply intertwined.\nThrough this combination of theory, practice, and critical reflection on signal processing, the module aims to open up new sonic possibilities. The references provided are intended not only to contextualize student work, but also to inspire expanded thinking about how these principles might inform personal projects.\nThis practical and flexible framework is reflected in the module’s initial activity on oscillatory movement. Students will have the freedom to choose their working environment and the context in which they wish to develop their experiments. Oscillator implementation can be approached from multiple angles: waveform manipulation, waveshaper design, or even the creation of interactive sound pieces. Each student will be invited to present a short description of their process, along with their code or patch, in order to foster dialogue between theory and practice.\nIn this way, the module not only deepens our understanding of the technical aspects of signal processing, but also invites critical reflection on the role of these technologies in shaping meaning and sonic aesthetics. Special emphasis is placed on the oscillator’s capacity to act as a driver of creativity and expression.\n\n\n4.2.2 Poetics of Signal Processing\nOne of the most important aspects of this chapter is the idea of “poetics” in signal processing. This concept is not just about the technical aspects of sound manipulation, but also about the cultural and artistic implications of these processes. The term “poetics” suggests a deeper engagement with the materiality of sound and its representation in various contexts. I’m going to talk a bit about my reflections and observations on the text Poetics of Signal Processing (Sterne and Rodgers 2011). First, I want to mention the authors, who I find very interesting. The first is Jonathan Sterne, a professor and director of the Culture and Technology program at McGill University in Canada. His work focuses on the cultural dimension of communication technologies, and he specializes in the history and theory of sound in the modern Western world. Then we have another author, a sound artist and musician named Tara Rodgers. She’s an electronic music composer, programmer, and historian of electronic music. She holds a PhD in Communication and has worked in the field of Women’s Studies. She created the platform Pink Noises, which is worth checking out—especially because it gathers a series of interviews with improvisers, composers, and instrument builders. There’s a cross-section of gender, sexuality, feminism, music, sound studies, theater, performance, and performative arts in general.\nLet’s get into some comments on specific sections of the article. The first is “The Sonic Turn,” which describes the emergence of a new culture of listening beginning in the second half of the 20th century. This is discussed through the work of authors like Cox and Kahn. There’s also a growing interest in oral history and anthropology among social scientists, and the emergence of sound art within the art world during the 20th century.Another key point is the growing interest in listening itself, and in the creative possibilities enabled by recording, reproduction, and other forms of sound transmission. This leads us to ask: where do today’s sound technologies come from? What are we going to address in this course?\nWe can start by discussing the “audible past,” or what the article refers to as the “auditory past.” The text mentions that between 1900 and 1925, sound becomes an object of thought and practice. Before this period, sound was thought of in more idealized terms—mainly through the lens of voice and music, along with all the structures they imply. During this period, there were significant socioeconomic and cultural shifts—capitalism, rationalism, science, and colonialism—that influenced ideas and practices related to sound and listening. These changes were not only cultural but social as well. In this “audible past,” even the most basic mechanical elements of sound reproduction technologies were shaped by how they had been used up until then. In this way, sound technologies are tied to habits—they sometimes enable new habits, like new ways of listening, or sometimes they solidify and reinforce existing ones.\nSo let’s reflect on the “poetics” of signal processing. At first, that might sound surprising—poetics? But we can start with a very basic unit: the signal. Signals have a certain materiality. Sound has materiality—it occupies space in a transmission, recording, or playback channel. It exists in a medium and can be manipulated in various ways. There’s a clear distinction between analog electrical signals, electronic signals, and digital ones. Each implies different content and meanings. So, signal processing occurs in the medium of sound transmission, but in a technologized era—that is, the present. This involves manipulating sound in what we might call a “translucent state.” Here, the transducer becomes central: we’ll talk a lot in the course about this idea of converting one form of energy—measurable by a certain magnitude—into another. This concerns almost everything in sound or image that reaches our senses through electronic media. This also exists in the domains of the musician, the playback device, the listener, and the interstices between them.\nRegarding the poetics of signal processing as signal [music plays briefly], the article refers mostly to the figurative dimensions of the process itself. These are the ways that processing is represented in the discourse of audio technology, particularly from a technical or engineering perspective. Signal processing carries cultural meanings—it’s not an isolated technical fact. It has cultural significance. Two metaphorical frameworks commonly used in everyday language among users and creators are discussed in the article: cooking and travel. These are two metaphors I find fascinating.\nLet’s begin with cooking—specifically, “the raw and the cooked.” This draws from the work of anthropologist Claude (Lévi-Strauss 1964), who analyzed the raw, the cooked, and the rotten. The axis of raw/cooked belongs to culture, while fresh/rotten relates more to nature. This is a very important reference: cooking is a cultural operation. Fire—the act of cooking—is the basis of a social order, of stability.\nIn this sense, when we talk about the raw and the cooked in relation to sound, rawness doesn’t mean purity. It’s a relative condition—it refers to the availability of audio for further processing. This is very useful when thinking in opposition to the Hi-Fi culture. We might even think of a scale of rawness, or various degrees of it, which relate to how sound can be manipulated and used.\nWithin this raw/cooked metaphor, terms like slicing (cutting into slices) and dicing (cutting into cubes) appear—these are actual signal processing terms and very fitting metaphors.\nThinking further with this metaphor: raw audio might be seen as passive, something that must be “cooked” through technological processes. This reveals the technologized nature of music technologies, where composition becomes a kind of masculine performance of technological mastery. And I want to stress this point again: composition is often framed as a male-dominated act of technical skill.\nPaul Théberge (Théberge 1997) analyzes musicians as consumers within the sound tech industry. In one chapter of his book, he studies advertising in music tech magazines, showing how the marketing of music technologies has been directed predominantly at men. Fortunately, this is changing slowly. In the raw/cooked metaphor, the idea of sound as a material to be processed and preserved for future use emerges in the late 19th century—just like technologies developed to preserve and can food. It’s a very strong metaphor: processed food and processed sound were both invented to extend and control organic life through technological preservation.\nNow let’s move to the other metaphor—travel. Signal processing can be thought of as a journey, which I find very exciting. We can connect this to topology—a mathematical field that studies spatial relationships. The term topos refers to place. In this sense, we can think of electronics as the arrangement and interaction of various components. A synthesizer circuit, or an oscillator, can be conceived as a space in itself—a map. Early texts in electroacoustics from the late 19th and early 20th centuries started to describe sound and electricity as fluid media. They used water metaphors—talking about waves, oscillations, flow, and current. This “processing as travel” metaphor involves the idea of particles moving through space, with the destination originally being the human ear. Today, that destination could be a transducer—or a computer.\nEven the inner ear was once conceptualized as a terrain made up of interconnected parts through which vibrations would travel. These metaphors matter: sea voyages during the historical periods mentioned in the article also symbolized scientific exploration and the conquest of the unknown. One example I love is that in the 1800s, Lord Kelvin created what could be considered the first synthesizer—but it didn’t produce sound. It was a mechanical device designed to predict tides. I’ll share some images to illustrate this. It essentially summed simple waves into one more complex waveform.\nSo, to conclude this idea of processing as travel, the text also reflects on how maritime metaphors privilege a particular kind of subject—a white, Western male as the ideal “navigator” of synthetic sound waves. This is a clearly colonial and masculinist rhetoric. Generating and controlling electronic sounds becomes associated with a kind of pleasure aligned with capitalism, and also with danger—of disobedient or unruly sounds.\nSwitching to a less metaphorical aspect, the text discusses Helmholtz’s On the Sensations of Tone (Helmholtz 1954). It laid the epistemological foundations for synthesis techniques. Helmholtz argued that any sound could be broken down into volume, pitch, and timbre. For him, sound was a material with clearly defined properties. These properties could be analyzed and then mimicked using synthesis techniques. However, other researchers, like Jessica Roland, explored different approaches. She compared sound to things like rain and wind. Her approach emphasized experience, memory, and the use of synthesis as a kind of onomatopoeia—imitation of natural phenomena. For Roland, unpredictability and chaos are at the heart of synthesis.\nIn conclusion, one of the key points of the article is that metaphors in audio-technical discourse—supposedly neutral or instrumental—are actually shaped by the cultural positions of specific subjects living in specific societies. They are deeply entangled with issues of gender, race, class, and culture. The language of technical culture is highly metaphorical and filled with implicit assumptions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#sound-sources",
    "href": "chapters/synthesis.html#sound-sources",
    "title": "4  Synthesis",
    "section": "4.3 Sound Sources",
    "text": "4.3 Sound Sources\nOscillators are a fundamental component of sound synthesis and play a crucial role in the creation of electronic music. They generate periodic waveforms, which can be manipulated to produce a wide range of sounds. In this section, we will explore the different types of oscillators, their characteristics, and how they can be used creatively in sound design.\n\n4.3.1 Oscillators\nSound sources in synthesizers are largely based on mathematics. There are two fundamental types: waveforms and random signals. Waveforms are typically described as simple geometric shapes—sawtooth, square, pulse, sine, and triangle being the most common. These shapes are mathematically straightforward and electronically feasible to generate. On the other hand, random waveforms produce noise, a constantly shifting mixture of all frequencies.\n\n\n\nFig. Sine Oscillator & White Noise Generator\n\n\nOscillators are one of the core building blocks of synthesizers, often implemented as function generators. A function generator produces a waveform that may be continuous or triggered and can take arbitrary shapes. In a basic analog subtractive synthesizer, an oscillator usually outputs a few continuous waveforms, with frequency controlled by voltage. Since these sources typically output a continuous signal, modifiers must be applied to shape timbre or envelope the sound.\n\n\n4.3.2 Sine & Cosine\nA “pure tone” consists of a single frequency and is produced by a sine wave oscillator, which can be implemented using either the sine or cosine function. These functions take an angle value, or “phase,” as input. Below, we see the angle “alpha,” the sine’s amplitude value, and the cosine’s output denoted as x.\n\nIn the resulting graph, amplitude is on the vertical axis and angle on the horizontal axis. The cosine output traces the same function as the sine but starts at 1, meaning it has a different initial phase. Sine and cosine are essentially the same waveform offset by 90 degrees of phase.\n\n\n\nFig.\n\n\nPure Data’s native [sin] and [cos] objects take angle values in radians (0 to 2π). However, the audio object [cos~] uses a linear range from 0 to 1 to represent a full cycle. The ELSE library provides the [pi] object, which outputs the constant π. This can be stored in a [value] object and accessed within [expr]. To convert a linear 0–1 range into radians, multiply it by 2 * pi. Then, [cos] and [sin] yield amplitude values accordingly.\n\n\n\n\nFig.\n\n\n\n\n\n4.3.3 Phasor\nIn the following example, we implement a sine oscillator using the [sin~] object and the native [phasor~] object.\n\n\n\nFig.\n\n\nTwo graphs illustrate this: in the top one, the horizontal axis is time, and the vertical axis shows a steadily increasing phase, forming a linear ramp. In the bottom graph, this ramp is transformed into a sine waveform, with amplitude on the vertical axis.\nThe [phasor~] object outputs a linear ramp from 0 to just under 1, representing a complete cycle. It is ideal for driving objects like [cos~] and [sin~], which expect a 0–1 input representing phase progression.\nThe input to [phasor~] is frequency, expressed in cycles per second (hertz). This defines how many full 0–1 cycles occur per second.\nNote that [phasor~] never actually reaches 1—it wraps around to 0. Due to its cyclic nature, 1 is functionally equivalent to 0, just like 360° equals 0° in circular geometry.\nThe output of [phasor~] can be described as a “running phase.” It defines the angular increment applied to the phase at every audio sample.\n\n\n\n\nFig.\n\n\n\n\n\n4.3.4 Oscillator\nIn the analog domain, oscillators are commonly referred to as VCOs (Voltage Controlled Oscillators). VCOs allow frequency or pitch to be controlled via voltage. Some VCOs also feature voltage control inputs for modulation (typically FM) and for altering the waveform shape—usually the pulse width of square waves, although some VCOs allow shaping other waveforms as well.\nMany VCOs include an additional input for synchronization with another VCO’s signal. Phase sync forces the VCO to reset its phase in sync with the incoming signal, limiting operation to harmonics of the input frequency. This results in a harsh, buzzy tone. Softer sync techniques can yield timbral variations rather than locking to an exact frequency.\nA typical VCO offers controls for coarse and fine tuning, waveform selection (often sine, triangle, square, sawtooth, and pulse), pulse width modulation (PWM), and output level. Some VCOs also provide multiple simultaneous waveform outputs and sub-octave outputs one or two octaves below the main signal. Pulse width modulation (PWM) allows dynamic alteration of the pulse waveform shape.\nTo summarize, an oscillator is typically defined by:\n\nWaveform function (sine, sawtooth, square, triangle)\nFrequency (Hz)\nInitial phase (degrees)\nPeak amplitude (optional)\n\nHow do we control these parameters in our model?\n\n\n\nFig.\n\n\n\n\n4.3.5 VCO\nIn this example, [phasor~] and [cos~] form an oscillator. The [cos~] object outputs amplitude values from -1 to 1, yielding a maximum amplitude of 1 without additional gain control.\n\n\n\nFig.\n\n\nThe waveform produced is a cosine. While [phasor~] sets the frequency, it can also define the initial phase. Since sine and cosine are essentially phase-shifted versions of the same function, we can easily produce sine waves as well. However, the initial phase does not affect the perceived pitch of a pure tone.\nTry this patch with different phase offsets. Note that [phasor~] also accepts negative frequencies, reversing the phase direction.\nConnecting [phasor~] to [cos~] replicates the functionality of the [osc~] object.\n\n\n\nFig.\n\n\n\n\n4.3.6 Waveforms\nThe sine wave is the simplest oscillator, generating a pure tone. Other basic and musically useful waveforms include triangle, sawtooth, and square.\nThe sine wave is a smooth, rounded waveform based on the sine function. It contains only one harmonic—the fundamental—which makes it less suitable for subtractive synthesis as it lacks overtones to filter.\nA triangle wave consists of two linear slopes. It contains small amounts of odd harmonics, providing just enough spectral content for filtering.\nA square wave contains only odd harmonics and produces a hollow, synthetic sound. A sawtooth wave contains both odd and even harmonics and sounds bright. Some pulse waves may contain even more harmonic content than basic sawtooth waves. Variants like “super-saw” replace linear slopes with exponential ones and alternate teeth with gaps, producing an even richer harmonic spectrum.\n\n\n4.3.7 Waveshapers\nThis section focuses on creating oscillators in Pure Data (Pd) using the [phasor~] object. The only true oscillator in Pd Vanilla is [osc~], a sine wave oscillator. Even standard waveforms must be built manually.\nAs mentioned earlier, [osc~] is essentially [phasor~] connected to [cos~]. The [phasor~] object outputs a 0–1 ramp, functionally similar to a sawtooth wave with half the amplitude and an offset. [cos~] multiplies this ramp by 2π and computes its cosine. The result is a sine wave oscillator.\n\n\n\nFig.\n\n\nAnother way to construct this oscillator is by using [expr~] and [value]. Here we use the [pi] abstraction to calculate π (or approximate it by sending 1 to [atan] and multiplying the result by 4). We then multiply by 2 and store it in a [value] object. [value] acts like a global variable: any object using the same name accesses the same value. [expr~] can then use this to calculate the cosine, just like [cos~]. While this approach may be more CPU-intensive, it helps deepen understanding of oscillator construction in Pd.\n\n\n\nFig.\n\n\n\n4.3.7.1 Sawtooth Oscillator\nSince [phasor~] already produces a ramp, generating a sawtooth wave is straightforward. Simply multiply [phasor~] by 2 to get the correct amplitude, then subtract 1 to shift the range to -1 to 1.\n\n\n\nFig.\n\n\n\n\n4.3.7.2 Square Wave Oscillator\nTo create a square wave, you can use [expr~] (included in Pd Vanilla), but [&gt;~] is faster and more CPU-efficient. A square wave toggles between -1 and 1. The [&gt;~] object compares its left input signal to a threshold (right input or argument). It outputs 1 if the input is greater than the threshold, and 0 otherwise. Using 0.5 as the threshold with a [phasor~] input yields a square wave: 0 for half the cycle, 1 for the other half.\n\n\n\nFig.\n\n\n\n\n4.3.7.3 Triangle Wave Oscillator\nAmong standard waveforms, the triangle wave is the most complex to construct. Starting with [phasor~], which is an upward ramp from 0 to 1, we create an inverted version by multiplying it by -1 and then adding 1. This gives us a descending ramp from 1 to 0.\nNow we have both ascending and descending ramps. Sending both to [min~] (which outputs the smaller of two values) gives us a triangle waveform spanning 0 to 0.5. [min~] effectively splices the ascending and descending ramps to form a symmetric triangle wave.\n\n\n\nFig.\n\n\n\n\n\n4.3.8 Frequency\nFrequency is presented here in terms of angular velocity! One common unit of measurement is the hertz (Hz), which equals “cycles per second.” Frequency also determines a period of oscillation, which is simply the inverse of frequency. For example, a frequency of 100 Hz corresponds to a period of 0.01 seconds (or 10 milliseconds):\nPeriod = 1 / Frequency\nPeriod = 1 / 100 Hz = 0.01 s = 10 ms\n\n\n\nFig.\n\n\nWe can convert between Hz and milliseconds using this same relationship. Another way to express period is in number of samples, which requires the sampling rate to perform the conversion:\n\n\n\nFig.\n\n\nAngular velocity units require both an angle and a time unit. One cycle per second defines a full cycle (360 degrees) as the angular unit, and seconds as the time unit. Other units are also possible. For example, the angle may be expressed in radians and the time unit as a single sample, yielding a unit of “radians per sample.”\nTo convert Hz to radians per sample, multiply by 2π and divide by the sampling rate. See below for this conversion and the [hz2rad] and [rad2hz] objects from the ELSE library that handle it.\n\n\n4.3.9 Phase\nThe term phase can be used in various contexts, often making it ambiguous and potentially confusing. A useful strategy is to adopt more specific terminology instead of simply referring to “phase” in isolation. On its own, “phase” refers to a stage within a cycle—much like the four phases of the moon. Sound waveforms are cyclical, and we can speak of a positive or negative phase, as shown below. However, this original meaning is rarely used in music theory. Here, we focus on other more relevant applications and interpretations of phase (see right and below).\n\n\n\nFig.\n\n\nInitial phase refers to the point in the cycle where the oscillation begins.\nInstantaneous phase: In music theory, “phase” often refers to the instantaneous phase—a specific point in time, not a stage in a sequence. It’s helpful to adopt the term instantaneous explicitly to denote a single position within a given cycle.\nSince instantaneous phase refers to a single position within a cycle, it can also be represented as an angle. This leads to a synonymous relationship between phase and angle, though it’s important to stress that both denote a position within the cycle.\n\n\n\nFig.\n\n\nTwo oscillators operating at the same frequency can be in phase or out of phase. Being in phase means they are synchronized—there is no phase difference. Being out of phase indicates a lack of synchronization, i.e., a phase difference. This difference can take many forms, but two specific cases are of particular interest: quadrature phase and phase opposition.\nQuadrature phase is the phase difference between sine and cosine waves, which equals a quarter of a cycle (90 degrees).\nPhase opposition is the maximum possible phase difference—half a cycle or 180 degrees.\n\n\n4.3.10 Polarity\nAs we’ve seen, phase opposition leads to signal cancellation—but only under certain waveform conditions! This occurs with sine waves, for instance, but not with all waveforms or signals. Note how, in the figure to the right, inverting the sign of every amplitude value in a waveform results in cancellation when added to the original signal.\nInverting polarity means changing the sign, or multiplying by -1. For sine waves, this results in the same effect as a 180-degree phase opposition. However, a true phase inversion is different, as it involves a time or phase shift.\nDespite this distinction, the term phase inversion is often misused when it really refers to a polarity inversion—which is neither a time shift nor a phase shift.\nYes, this can be very confusing and requires careful attention. That’s why this tutorial prefers the term polarity inversion, although many audio devices refer to a 180-degree phase shift when they are, in fact, performing a polarity inversion.\nBoth phase and polarity inversion produce the same result for sine waves due to their symmetrical waveforms, where the second half of the cycle mirrors the first with opposite sign. Other waveforms with this property include triangle and square waves (with a 0.5 pulse width).\n\n\n\nFig.\n\n\nSawtooth waves, however, do not share this symmetry. Therefore, phase opposition is not equivalent to polarity inversion in this case. The only way to achieve full cancellation of a sawtooth wave is through polarity inversion. Refer to the graphs below: only the original sawtooth combined with its polarity-inverted version results in complete cancellation.\nThe native [phasor~] and [osc~] objects include a right inlet that accepts control data to reset the phase. Whenever the inlet receives a number from 0 to 1, the waveform resets to that initial phase position. Note that this is unrelated to phase modulation techniques discussed earlier.\n\n\n\nFig.\n\n\nThe [osc~] object does not support phase modulation; we implemented it using [phasor~] in the previous examples. Hence, using a [phasor~] together with a [cos~] enables both phase modulation and oscillator resetting.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#additivity",
    "href": "chapters/synthesis.html#additivity",
    "title": "4  Synthesis",
    "section": "4.4 Additivity",
    "text": "4.4 Additivity",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#amplitude-ring-modulation",
    "href": "chapters/synthesis.html#amplitude-ring-modulation",
    "title": "4  Synthesis",
    "section": "4.5 Amplitude & Ring Modulation",
    "text": "4.5 Amplitude & Ring Modulation\nAmplitude modulation (AM) and ring modulation (RM) are two techniques that manipulate the amplitude of a signal using another signal. While they share similarities, they produce distinct results and are used in different contexts. In this section, we will explore the differences between AM and RM, their applications, and how they can be implemented in sound synthesis.\n\n4.5.1 Amplitude Modulation\nWe can modulate the amplitude of any signal—referred to as the carrier—by multiplying it with an oscillating signal, called the modulator. The modulator is typically another oscillator, and its frequency determines the modulation frequency.\n\n\n\nAmplitude Modulation\n\n\nIn the example provided, both the carrier and modulator are sine wave oscillators. This is what we call “classic” amplitude modulation (AM), where the modulator signal includes a DC offset, making it unipolar, ranging only from 0 to 1. This unipolarity ensures that the carrier’s amplitude is scaled without ever becoming negative.\n\n\n4.5.2 Ring Modulation\nRing modulation is a particular form of amplitude modulation where both the carrier and modulator signals are bipolar, meaning they oscillate between -1 and 1 without any DC offset. In this configuration, there’s no functional distinction between carrier and modulator—both behave symmetrically.\n\n\n\nRing Modulation\n\n\nNonetheless, in practical terms, the carrier is usually an audio signal such as a musical instrument, while the modulator remains a simple oscillator. A key technical detail is that when the modulator signal is negative, it inverts the polarity of the carrier signal—producing a unique and often metallic timbre.\n\n\n4.5.3 DC Offset\nAmplitude modulation schemes can involve various DC offset settings—not just limited to AM or RM. In our example, preset configurations for AM and RM are available, but one can also manually adjust peak levels and DC offset using sliders.\nObserve how, in the frequency spectrum of classic AM, we see two sidebands—above and below the carrier frequency—each at half the amplitude of the original carrier. These sidebands are spaced apart by the modulation frequency.\n\n\n\nFig.\n\n\nIn contrast, ring modulation removes the original carrier frequency entirely from the spectrum, leaving only the sidebands, which typically carry more energy than in AM.\nBy adjusting amplitude and DC offset with sliders, we can morph continuously between AM and RM modes, gaining nuanced control over the presence of the original frequency component and the energy distribution in the sidebands.\n\n\n4.5.4 Audio Samples & Modulation\nIn this example, an audio sample replaces the oscillator as the carrier signal. This demonstrates how amplitude modulation can function as an audio effect processor rather than just a synthesis technique. In fact, much of what we traditionally associate with synthesis techniques is often more accurately described as audio processing.\nConversely, many effects processors—such as filters—are integral to sound synthesis. The boundary between synthesis and processing is thus fluid and contextual.\n\n\n\nFig.\n\n\nTry both the classic AM and RM examples. In both cases, sidebands are generated for each sine wave component within the carrier signal. AM retains the carrier’s original sine components, which coexist and interact with the generated sidebands. For this reason, AM is commonly used for tremolo effects (which we’ll examine later). On the other hand, RM removes the original sine components entirely, yielding a more sonically distinctive result.\n\n\n4.5.5 Other Waveforms\nUsing more complex waveforms for the modulator signal leads to the creation of additional partials within any amplitude modulation patch—including ring modulation. Sine waves are typically favored as modulators since they offer clean and controlled results, especially when applying AM as an audio effect.\n\nHowever, in synthesis contexts, more intricate and harmonically rich methods—such as frequency and phase modulation—offer more efficient and versatile approaches for generating complex timbres. We’ll explore these in the following sections.\n\n\n4.5.6 Tremolo\nTremolo is essentially amplitude modulation using a low-frequency modulator. The key addition is a depth parameter, ranging from 0 to 1, which determines the modulation intensity. At 0, no modulation occurs (dry signal), while a depth of 1 results in full tremolo, where the carrier’s amplitude is modulated across its full range.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#frequency-modulation",
    "href": "chapters/synthesis.html#frequency-modulation",
    "title": "4  Synthesis",
    "section": "4.6 Frequency Modulation",
    "text": "4.6 Frequency Modulation\nIn general terms, to modulate a signal means to alter it in some way. In the context of this course, however, we refer specifically to using a modulating signal to control a parameter—such as amplitude, as previously discussed. We now turn to the basic structure of frequency modulation (FM), where an oscillator acts as the modulator.\nThe signal being modulated is called the carrier; in the case of frequency modulation, it is also referred to as the carrier frequency. In contrast, we have the modulating frequency, which corresponds to the frequency of the modulating oscillator. The depth of frequency variation is determined by the amplitude of the modulator and is commonly referred to as the modulation index. The modulation process itself is straightforward: we add the modulating signal to the frequency input of the carrier oscillator. See the example on the right\n\n\n\nFig.\n\n\nBy default, we have a carrier frequency of 400 Hz, a low modulation frequency of 1 Hz, and a modulation index of 100. This means the modulating signal oscillates between -100 and +100 Hz, causing the carrier frequency to vary between 300 and 500 Hz. Note that when the modulation frequency is low, the result is a vibrato-like effect.\n\n4.6.1 FM Simple\nWe apply the same structure as before. Besides a vibrato example, this section includes fully developed FM examples. As with amplitude modulation, FM produces sidebands spaced by intervals equal to the modulation frequency. However, FM can generate many more sidebands, potentially resulting in a much richer spectrum.\n\nThe higher the modulation index, the greater the number of resulting partials—enabling the creation of dense and complex waveforms. When the carrier and modulator frequencies share a simple harmonic ratio, the resulting waveform is harmonic. Otherwise, it tends to be inharmonic. Click on the message boxes to hear the preset examples.\n\n\n4.6.2 Other Waveforms\nIn this patch, we experiment with different oscillator combinations. Waveforms are arranged according to spectral complexity—from simple sine waves to rich sawtooth waves (the only waveform here containing both even and odd harmonics). The more complex the waveform used, the more intricate the FM result becomes.\n\n\n\nFig.\n\n\nOn the modulator side, the waveforms are idealized and band-unlimited—perfect in theory. However, the frequency-modulated oscillator has a limited bandwidth, which imposes practical constraints on the resulting spectrum.\n\n\n4.6.3 Exponential Frequency\nWe can also use exponential pitch values (such as MIDI note numbers) instead of linear frequency input. The key difference here is that frequency deviation—the modulation index—is now expressed in semitones, not Hertz. This shift affects the entire modulation behavior.\n\n\n\nFig.\n\n\nAs a result, the output waveform becomes asymmetrical and significantly different in character. The main change in the patch is the use of [mtof~] to convert MIDI pitch to frequency in Hz.\n\n\n4.6.4 Ratio\nIt is common practice to define the modulating frequency as a ratio of the carrier frequency. This allows us to work with a single frequency input while maintaining a consistent sonic character across different pitches.\n\n\n\nFig.\n\n\nThis approach preserves the relationship between the carrier and modulator frequencies, which is crucial since this ratio determines how the additional spectral components—partials—are distributed. Harmonic ratios (such as 0.5 or 2) yield harmonic results, while non-integer ratios lead to inharmonic spectra.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/synthesis.html#references",
    "href": "chapters/synthesis.html#references",
    "title": "4  Synthesis",
    "section": "References",
    "text": "References\n\n\n\n\nHelmholtz, Hermann. 1954. On the Sensations of Tone as a Physiological Basis for the Theory of Music. 2nd ed. New York: Dover Publications.\n\n\nLévi-Strauss, Claude. 1964. Le Cru Et Le Cuit. Vol. 1. Mythologiques. Paris: Plon.\n\n\nSterne, Jonathan, and Tara Rodgers. 2011. “Poetics of Signal Processing.” Differences 22 (2-3): 31–53. https://doi.org/10.1215/10407391-1428834.\n\n\nThéberge, Paul. 1997. Any Sound You Can Imagine: Making Music/Consuming Technology. Hanover & London: Wesleyan University Press.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Synthesis</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html",
    "href": "chapters/conclusion.html",
    "title": "7  Conclusion",
    "section": "",
    "text": "7.1 Key Takeaways\nIn this chapter, we summarize the key points discussed throughout the book. We reflect on the main topics introduced in the first chapter and the detailed discussions in the second chapter.\nThank you for reading! We hope this book has provided valuable insights and knowledge.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html#key-takeaways",
    "href": "chapters/conclusion.html#key-takeaways",
    "title": "7  Conclusion",
    "section": "",
    "text": "Highlight the main objectives of the book.\nDiscuss the significance of the topics covered.\nEncourage further exploration and learning in the subject area.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html",
    "href": "chapters/sonification.html",
    "title": "5  Sonification",
    "section": "",
    "text": "5.1 Introduction\nIn this chapter, we will explore the concept of sonification, its applications, and how it can be implemented using Pure Data (Pd). We will also discuss the importance of understanding data types and classifications in the context of sonification.\nImagine hearing the changes in global temperature over the past thousand years. What does a brainwave sound like? How can sound be used to enhance a pilot’s performance in the cockpit? These intriguing questions, among many others, fall within the realm of auditory display and sonification.\nResearchers in Auditory Display explore how the human auditory system can serve as a primary interface channel for communicating and conveying information. The goal of auditory display is to foster a deeper understanding or appreciation of the patterns and structures embedded in data beyond what is visible on the screen.\nAuditory display encompasses all aspects of human-computer interaction systems, including the hardware setup (speakers or headphones), modes of interaction with the display system, and any technical solutions for data collection, processing, and computation needed to generate sound in response to data.\nIn contrast, sonification is a core technique within auditory display: the process of rendering sound from data and interactions. Unlike voice interfaces or artistic soundscapes, auditory displays have gained increasing attention in recent years and are becoming a standard method alongside visualization for presenting data across diverse contexts.\nThe international research effort to understand every aspect of auditory display began with the founding of the International Community for Auditory Display (ICAD) in 1992. It is fascinating to observe how sonification and auditory display techniques have evolved in the relatively short time since their formal definition, with development accelerating steadily since 2011.\nAuditory display and sonification are now employed across a wide range of fields. Applications include chaos theory, biomedicine, interfaces for visually impaired users, data mining, seismology, desktop and mobile computing interaction, among many others.\nEqually diverse is the set of research disciplines required for successful sonification: physics, acoustics, psychoacoustics, perceptual research, sound engineering, and computer science form the core technical foundations. However, psychology, musicology, cognitive science, linguistics, pedagogy, social sciences, and philosophy are also essential for a comprehensive, multifaceted understanding of the description, technical implementation, usage, training, comprehension, acceptance, evaluation, and ergonomics of auditory displays and sonification in particular.\nIt is clear that in such an interdisciplinary field, a narrow focus on any single discipline risks “seeing the trees but missing the forest.” As with all interdisciplinary research efforts, auditory display and sonification face significant challenges, ranging from differing theoretical orientations across fields to even the very vocabulary used to describe our work.\nInterdisciplinary dialogue is crucial to advancing auditory display and sonification. However, the field must overcome the challenge of developing and employing a shared language that integrates many divergent disciplinary ways of speaking, thinking, and approaching problems. On the other hand, this very challenge often unlocks great creative potential and new ideas, as these varied perspectives can spark innovation and fresh insights.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#section",
    "href": "chapters/sonification.html#section",
    "title": "5  Sonification",
    "section": "",
    "text": "Category\nSubcategory\nDetails / Examples\n\n\n\n\nStatic\nStructured\n\n\n\n\n\nDataset: CSV, XLS\n\n\n\n\nFields: numeric, string, date/time\n\n\n\n\nClass: binary, multiclass\n\n\n\n\nSources: Kaggle datasets, City government\n\n\n\nImage(s)\nCompressed image (JPG) vs BMP\n\n\n\n\nSources: Instagram, Reddit, Flickr, Unsplash\n\n\n\nMIDI files\n\n\n\n\nAudio(s)\nRaw, descriptors, Fourier\n\n\n\n\nCompression type: MP3 vs FLAC vs WAV\n\n\n\n\nSources: Freesound, Audio Commons\n\n\n\nSemi-structured\n\n\n\n\n\nMarkup languages: HTML (web scraping), XML, JSON, YAML\n\n\n\n\nSources: The web itself, APIs\n\n\n\nUnstructured\n\n\n\n\n\nText(s): Huffman compression\n\n\nStream / Realtime / Signals\nAudio stream\nOnline radios, surveillance cameras\n\n\n\nVideo stream\nSurveillance cameras, Self-driving cars\n\n\n\nSensors\nArduino\n\n\n\nMIDI\n\n\n\n\nOSC",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#data-types",
    "href": "chapters/sonification.html#data-types",
    "title": "5  Sonification",
    "section": "5.2 Data Types",
    "text": "5.2 Data Types\nIn the realm of digital data, understanding the nature and classification of data types is essential for their effective processing, storage, and analysis. The table above presents a detailed taxonomy of data types broadly categorized into Static and Stream / Realtime data, further subdivided into various subtypes. This section will unpack these classifications, elaborating on their characteristics, common formats, and sources.\n\n5.2.1 Static Data\nStatic data refers to data that is stored and remains unchanged until explicitly modified. It is typically collected, stored, and then analyzed or processed offline or asynchronously. This type of data is fundamental in many computational tasks, including machine learning, data mining, and archival storage.\n\n5.2.1.1 Structured Data\nStructured data is organized in a predefined manner, typically conforming to a schema or model that allows easy querying and manipulation.\nExamples:\n\nDatasets (CSV, XLS): Tabular data with rows and columns, where each column has a defined datatype (e.g., numeric, string, datetime).\nFields: The basic elements of structured data that have specific data types, such as integers, floating-point numbers, text strings, or timestamps.\nClasses: Labels used in supervised learning, often binary (two classes) or multiclass (multiple categories).\nImages: Digital images can be structured if stored with metadata or standardized file formats. Examples include compressed formats like JPG and uncompressed formats like BMP.\nMIDI Files: Musical Instrument Digital Interface files encode structured note and control information.\nAudio: Raw waveforms or extracted descriptors (e.g., Mel-frequency cepstral coefficients - MFCCs, Fourier transforms) represent audio signals in structured formats.\nAudio Formats: Common audio file formats such as MP3 (compressed), FLAC (lossless compressed), and WAV (uncompressed).\n\nSources: Data portals (e.g., Kaggle, UCI Machine Learning Repository), social media platforms like Instagram, Reddit, Flickr (for images), and audio repositories.\n\n\n5.2.1.2 Semi-structured Data\nSemi-structured data does not conform to a rigid schema like structured data but still contains tags or markers to separate semantic elements.\nExamples:\n\nMarkup Languages: HTML, XML, JSON, and YAML files are examples of semi-structured data because they contain hierarchical tags and attributes describing the data, but the content may be variable.\n\nSources: Web content, APIs that deliver data in JSON or XML formats.\n\n\n5.2.1.3 Unstructured Data\nUnstructured data lacks a predefined data model or schema. It is the most common form of data generated and can be more challenging to analyze without preprocessing.\nExamples:\n\nTexts: Documents, emails, articles, or social media posts are typical unstructured data examples.\n\nSources: Document collections, text corpora, email archives.\n\n\n\n5.2.2 Stream / Realtime Data\nStream or realtime data refers to continuous data generated in real-time, often requiring immediate or near-immediate processing. These data types are crucial in applications like live monitoring, interactive systems, and real-time analytics.\nExamples:\n\nAudio Streams: Continuous audio feeds such as online radio broadcasts.\nVideo Streams: Live video feeds from CCTV cameras or autonomous vehicles.\nSensor Data: Real-time telemetry or environmental data from IoT devices, Arduino boards, or embedded systems.\nLive MIDI: Streaming musical performance data, used in live concerts or interactive installations.\nOSC (Open Sound Control): A protocol used for networking sound synthesizers, computers, and other multimedia devices, enabling real-time control.\n\nSources: Online radio platforms, surveillance systems, smart vehicles, embedded IoT devices, live music performance setups.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#static-data",
    "href": "chapters/sonification.html#static-data",
    "title": "5  Sonification",
    "section": "5.2 1. Static Data",
    "text": "5.2 1. Static Data\nStatic data refers to data that is stored and remains unchanged until explicitly modified. It is typically collected, stored, and then analyzed or processed offline or asynchronously. This type of data is fundamental in many computational tasks, including machine learning, data mining, and archival storage.\n\n5.2.1 1.1 Structured Data\nStructured data is organized in a predefined manner, typically conforming to a schema or model that allows easy querying and manipulation.\nExamples:\n\nDatasets (CSV, XLS): Tabular data with rows and columns, where each column has a defined datatype (e.g., numeric, string, datetime).\nFields: The basic elements of structured data that have specific data types, such as integers, floating-point numbers, text strings, or timestamps.\nClasses: Labels used in supervised learning, often binary (two classes) or multiclass (multiple categories).\nImages: Digital images can be structured if stored with metadata or standardized file formats. Examples include compressed formats like JPG and uncompressed formats like BMP.\nMIDI Files: Musical Instrument Digital Interface files encode structured note and control information.\nAudio: Raw waveforms or extracted descriptors (e.g., Mel-frequency cepstral coefficients - MFCCs, Fourier transforms) represent audio signals in structured formats.\nAudio Formats: Common audio file formats such as MP3 (compressed), FLAC (lossless compressed), and WAV (uncompressed).\n\nSources: Data portals (e.g., Kaggle, UCI Machine Learning Repository), social media platforms like Instagram, Reddit, Flickr (for images), and audio repositories.\n\n\n\n5.2.2 1.2 Semi-structured Data\nSemi-structured data does not conform to a rigid schema like structured data but still contains tags or markers to separate semantic elements.\nExamples:\n\nMarkup Languages: HTML, XML, JSON, and YAML files are examples of semi-structured data because they contain hierarchical tags and attributes describing the data, but the content may be variable.\n\nSources: Web content, APIs that deliver data in JSON or XML formats.\n\n\n\n5.2.3 1.3 Unstructured Data\nUnstructured data lacks a predefined data model or schema. It is the most common form of data generated and can be more challenging to analyze without preprocessing.\nExamples:\n\nTexts: Documents, emails, articles, or social media posts are typical unstructured data examples.\n\nSources: Document collections, text corpora, email archives.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#stream-realtime-data",
    "href": "chapters/sonification.html#stream-realtime-data",
    "title": "5  Sonification",
    "section": "5.2 Stream / Realtime Data",
    "text": "5.2 Stream / Realtime Data\nStream or realtime data refers to continuous data generated in real-time, often requiring immediate or near-immediate processing. These data types are crucial in applications like live monitoring, interactive systems, and real-time analytics.\nExamples:\n\nAudio Streams: Continuous audio feeds such as online radio broadcasts.\nVideo Streams: Live video feeds from CCTV cameras or autonomous vehicles.\nSensor Data: Real-time telemetry or environmental data from IoT devices, Arduino boards, or embedded systems.\nLive MIDI: Streaming musical performance data, used in live concerts or interactive installations.\nOSC (Open Sound Control): A protocol used for networking sound synthesizers, computers, and other multimedia devices, enabling real-time control.\n\nSources: Online radio platforms, surveillance systems, smart vehicles, embedded IoT devices, live music performance setups.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#discussion",
    "href": "chapters/sonification.html#discussion",
    "title": "5  Sonification",
    "section": "5.2 Discussion",
    "text": "5.2 Discussion\nThe classification of data into static and stream/realtime reflects fundamentally different operational paradigms in digital systems. Static data often allows batch processing, archival, and complex analysis without stringent timing constraints. In contrast, stream data demands continuous, low-latency processing to support live feedback, monitoring, or interaction.\nThe structured / semi-structured / unstructured distinction highlights the complexity of dealing with data formats:\n\nStructured data is well-suited for traditional databases and straightforward analysis.\nSemi-structured data requires flexible parsers and understanding of nested or tagged data.\nUnstructured data often necessitates advanced techniques such as natural language processing or computer vision for meaningful extraction.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#references",
    "href": "chapters/sonification.html#references",
    "title": "5  Sonification",
    "section": "5.4 References",
    "text": "5.4 References\nStonebraker, M., & Çetintemel, U. (2005). “One size fits all”: an idea whose time has come and gone. Proceedings of the 21st International Conference on Data Engineering (ICDE).\nGantz, J., & Reinsel, D. (2012). The Digital Universe in 2020: Big Data, Bigger Digital Shadows, and Biggest Growth in the Far East. IDC iView.\nHunter, J., & Kelly, S. (2006). XML and databases: technologies and applications. The VLDB Journal.\nMarr, B. (2016). Big Data in Practice: How 45 Successful Companies Used Big Data Analytics to Deliver Extraordinary Results. Wiley.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#data-classification",
    "href": "chapters/sonification.html#data-classification",
    "title": "5  Sonification",
    "section": "5.3 Data Classification",
    "text": "5.3 Data Classification\nThe classification of data into static and stream/realtime reflects fundamentally different operational paradigms in digital systems. Static data often allows batch processing, archival, and complex analysis without stringent timing constraints. In contrast, stream data demands continuous, low-latency processing to support live feedback, monitoring, or interaction.\nThe structured / semi-structured / unstructured distinction highlights the complexity of dealing with data formats:\n\nStructured data is well-suited for traditional databases and straightforward analysis.\nSemi-structured data requires flexible parsers and understanding of nested or tagged data.\nUnstructured data often necessitates advanced techniques such as natural language processing or computer vision for meaningful extraction.\n\nUnderstanding these data types and their sources is critical when designing systems for data ingestion, storage, processing, and analysis — especially in fields such as machine learning, multimedia processing, and IoT applications.\n\n\n\n\n\n\n\n\n\nType\nSubtype\nExamples\nSources\n\n\n\n\nStatic\nStructured\nSemi-structured Unstructured\n\nDatasets (CSV, XLS)\nFields: numeric, string, datetime\nClasses: binary, multiclass\nImages (compressed: JPG, uncompressed: BMP)\nMIDI files\nAudio: raw, descriptors, Fourier\nAudio formats: MP3, FLAC, WAV\nMarkup languages: HTML, XML, JSON, YAML\nTexts\n\n\nData portals\nInstagram, Reddit, Flickr\nAudio repositories\nWeb, APIs\nDocument collections\n\n\n\nStream / Realtime\n—\n\nAudio streams (e.g. online radio)\nVideo streams (e.g. CCTV, autonomous vehicles)\nSensor data (e.g. Arduino, real-time telemetry)\nLive MIDI\nOSC (Open Sound Control)\n\n\nOnline radio platforms\nSurveillance systems, smart vehicles\nIoT devices, embedded systems\nLive performance setups\nInteractive art and media systems\n\n\n\n\n\n5.3.1 References\nGantz, J., & Reinsel, D. (2012). The Digital Universe in 2020: Big Data, Bigger Digital Shadows, and Biggest Growth in the Far East. IDC iView.\nMarr, B. (2016). Big Data in Practice: How 45 Successful Companies Used Big Data Analytics to Deliver Extraordinary Results. Wiley.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#sonification-as-a-creative-framework-in-pure-data",
    "href": "chapters/sonification.html#sonification-as-a-creative-framework-in-pure-data",
    "title": "5  Sonification",
    "section": "5.4 Sonification as a Creative Framework in Pure Data",
    "text": "5.4 Sonification as a Creative Framework in Pure Data\n\n5.4.1 The Art-Science Continuum in Sonification\nSonification, traditionally defined as the systematic, objective, and reproducible translation of data into sound, finds itself at a dynamic intersection between scientific inquiry and creative expression. While often framed within the methodological rigor of data representation, its deeper potential lies equally in its artistic embodiment—an expressive convergence of logic, perception, and auditory imagination. This duality is vital when considering the use of Pure Data (Pd) in the development of creative code practices for sonification.\nIn its essence, sonification provides a means to explore and interpret data temporally. It transforms static information into dynamic sonic experiences, revealing patterns not just to the analytical mind but to the aesthetic ear. As Gresham-Lancaster argues, the field must extend beyond scientific formalism to embrace a broader spectrum of cultural and musical meaning. Pd, as an open-source visual programming environment, uniquely supports this hybridity, enabling the construction of both direct (first-order) sonification and more abstract, form-driven (second-order) approaches.\n\n\n5.4.2 First-Order and Second-Order Sonification in Pd\nA foundational concept introduced by Gresham-Lancaster is the distinction between first-order and second-order sonification. In Pd, first-order sonification typically involves straightforward mapping of data points to sound parameters—e.g., a numerical value controlling oscillator frequency or filter cutoff. This direct translation preserves the integrity of the data but may lack emotional resonance or contextual clarity for listeners unfamiliar with the dataset or mappings.\nSecond-order sonification introduces a higher level of abstraction. Here, data is not merely converted to sound but is used to control compositional structures such as rhythm, harmony, timbre, and formal development. In Pd, this could manifest through patch designs that incorporate statistical analysis of datasets to determine musical motifs or drive stochastic processes that evolve over time. By embedding data within culturally familiar or stylistically resonant frameworks—be it ambient textures, rhythmic patterns, or harmonic progressions—Pd allows the sonification to become more legible and emotionally impactful.\n\n\n5.4.3 The Role of Style, Taste, and Perception\nA key insight is that listeners inevitably interpret sound within a cultural and stylistic frame. Whether intentional or not, a Pd patch that maps data to sonic outputs will be perceived through the lens of genre conventions—ambient, noise, glitch, minimalism, etc. This reinforces the necessity for designers of sonification systems to consciously engage with aesthetic decisions. Rather than viewing these as distractions from scientific rigor, they should be recognized as essential design variables that determine how well the sonification communicates.\nFor instance, in designing a Pd patch for environmental data, layering the sonified stream within an evolving drone texture or embedding it in rhythmic pulses tied to diurnal cycles may enhance the intelligibility of subtle shifts. These choices, while extramusical, profoundly affect the user’s ability to detect and interpret meaningful changes.\n\n\n5.4.4 Toward a Craft of Musical Sonification\nThe expressive capacity of Pd invites an analogy with sculpture, as cited in Xenakis’ conception of raw mathematical outputs as “virtual stones” to be artistically shaped. The creative coder working in Pd similarly refines the raw outputs of data-driven patches, sculpting auditory forms through iterative experimentation, parameter tuning, and aesthetic discernment. This process reveals sonification not as a deterministic output of data, but as a collaboration between the structure of information and the intuition of the artist-programmer.\nMoreover, by incorporating techniques such as timbral mapping, adaptive filtering, and data-driven granular synthesis, Pd becomes a laboratory for crafting sonic experiences that honor both the source data and the listener’s perceptual world. This is particularly effective when the goal is to embed sonification within broader artistic or performative contexts—installations, interactive systems, or generative concerts—where expressivity and audience engagement are critical.\n\n\n5.4.5 The Future of Sonification in Creative Coding\nThe evolution of sonification within the creative coding domain—particularly in Pure Data—demands a reconceptualization of what constitutes success in sonification. Beyond reproducibility, the true measure lies in perceptual clarity, aesthetic engagement, and communicative power. As Gresham-Lancaster asserts, the inclusion of musical form, stylistic awareness, and cultural embedding are not luxuries, but necessities for sonification to become a widely adopted and meaningful practice.\nPure Data, with its flexibility, open-ended structure, and visual coding paradigm, provides an ideal environment to develop and test both first-order and second-order sonification systems. By embracing both scientific precision and artistic insight, Pd-based sonification becomes a model for interdisciplinary practice—where the auditory exploration of data is not just informative but transformative.\nIn sum, to cultivate a rich ecosystem of creative code in Pd, we must embrace sonification not merely as a technical mapping, but as a musical and perceptual act—a way of listening anew to the world’s data, rendered beautifully and meaningfully into sound.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#sonification-as-a-creative-framework",
    "href": "chapters/sonification.html#sonification-as-a-creative-framework",
    "title": "5  Sonification",
    "section": "5.4 Sonification as a Creative Framework",
    "text": "5.4 Sonification as a Creative Framework\nSonification, traditionally defined as the systematic, objective, and reproducible translation of data into sound, finds itself at a dynamic intersection between scientific inquiry and creative expression. While often framed within the methodological rigor of data representation, its deeper potential lies equally in its artistic embodiment—an expressive convergence of logic, perception, and auditory imagination. This duality is vital when considering the use of software in the development of creative code practices for sonification. In its essence, sonification provides a means to explore and interpret data temporally. It transforms static information into dynamic sonic experiences, revealing patterns not just to the analytical mind but to the aesthetic ear. As Gresham-Lancaster (Gresham-Lancaster 2012) argues, the field must extend beyond scientific formalism to embrace a broader spectrum of cultural and musical meaning.\nA foundational concept introduced by Gresham-Lancaster is the distinction between first-order and second-order sonification. First-order sonification typically involves straightforward mapping of data points to sound parameters—e.g., a numerical value controlling oscillator frequency or filter cutoff. This direct translation preserves the integrity of the data but may lack emotional resonance or contextual clarity for listeners unfamiliar with the dataset or mappings. Second-order sonification introduces a higher level of abstraction. Here, data is not merely converted to sound but is used to control compositional structures such as rhythm, harmony, timbre, and formal development. This could manifest through software designs that incorporate statistical analysis of datasets to determine musical motifs or drive stochastic processes that evolve over time. By embedding data within culturally familiar or stylistically resonant frameworks—be it ambient textures, rhythmic patterns, or harmonic progressions allows the sonification to become more legible and emotionally impactful.\nA key insight is that listeners inevitably interpret sound within a cultural and stylistic frame. Whether intentional or not, sonification software maps data to sonic outputs will be perceived through the lens of genre conventions—ambient, noise, glitch, minimalism, etc. This reinforces the necessity for designers of sonification systems to consciously engage with aesthetic decisions. Rather than viewing these as distractions from scientific rigor, they should be recognized as essential design variables that determine how well the sonification communicates. For instance, layering the sonified stream within an evolving drone texture or embedding it in rhythmic pulses tied to diurnal cycles may enhance the intelligibility of subtle shifts. These choices, while extramusical, profoundly affect the user’s ability to detect and interpret meaningful changes.\nThe expressive capacity of sonification technics invites an analogy with sculpture, as cited in Xenakis’ conception of raw mathematical outputs as “virtual stones” to be artistically shaped. The creative coder similarly refines the raw outputs of data-driven patches, sculpting auditory forms through iterative experimentation, parameter tuning, and aesthetic discernment. This process reveals sonification not as a deterministic output of data, but as a collaboration between the structure of information and the intuition of the artist-programmer. Moreover, by incorporating techniques such as timbral mapping, adaptive filtering, and data-driven granular synthesis, it becomes a laboratory for crafting sonic experiences that honor both the source data and the listener’s perceptual world. This is particularly effective when the goal is to embed sonification within broader artistic or performative contexts—installations, interactive systems, or generative concerts—where expressivity and audience engagement are critical.\nThe evolution of sonification within the creative coding domain demands a reconceptualization of what constitutes success in sonification. Beyond reproducibility, the true measure lies in perceptual clarity, aesthetic engagement, and communicative power. As Gresham-Lancaster asserts, the inclusion of musical form, stylistic awareness, and cultural embedding are not luxuries, but necessities for sonification to become a widely adopted and meaningful practice Pure Data, with its flexibility, open-ended structure, and visual coding paradigm, provides an ideal environment to develop and test both first-order and second-order sonification systems. By embracing both scientific precision and artistic insight, Pd-based sonification becomes a model for interdisciplinary practice—where the auditory exploration of data is not just informative but transformative.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#data-humanism-a-visual-manifesto-of-giorgia-lupi",
    "href": "chapters/sonification.html#data-humanism-a-visual-manifesto-of-giorgia-lupi",
    "title": "5  Sonification",
    "section": "5.5 Data Humanism: A Visual Manifesto of Giorgia Lupi",
    "text": "5.5 Data Humanism: A Visual Manifesto of Giorgia Lupi\n\n\n\nHumanism of Data\n\n\nData is now recognized as one of the foundational pillars of our economy, and the idea that the world is exponentially enriched with data every day has long ceased to be news.\nBig Data is no longer a distant dystopian future; it is a commodity and an intrinsic, iconic feature of our present—alongside dollars, concrete, automobiles, and Helvetica. The ways we relate to data are evolving faster than we realize, and our minds and bodies are naturally adapting to this new hybrid reality built from physical and informational structures. Visual design, with its unique power to reach deep into our subconscious instantly—bypassing language—and its inherent ability to convey vast amounts of structured and unstructured information across cultures, will play an even more central role in this quiet yet inevitable revolution.\nPioneers of data visualization like William Playfair, John Snow, Florence Nightingale, and Charles Joseph Minard were the first to harness and codify this potential in the 18th and 19th centuries. Modern advocates such as Edward Tufte, Ben Shneiderman, Jeffrey Heer, and Alberto Cairo have been instrumental in the field’s renaissance over the past twenty years, supporting the transition of these principles into the world of Big Data.\nThanks to this renewed interest, an initial wave of data visualization swept across the web, reaching a wider audience beyond the academic circles where it had previously been confined. Unfortunately, this wave was often ridden superficially—used as a linguistic shortcut to cope with the overwhelming nature of Big Data.\n“Cool” infographics promised a key to mastering this untamable complexity. When they inevitably failed to deliver on this overly optimistic expectation, we were left with gigabytes of illegible 3D pie charts and cheap, translucent user interfaces cluttered with widgets that even Tony Stark or John Anderton from Minority Report would struggle to understand.\nIn reality, visual design is often applied to data merely as a cosmetic gloss over serious and complicated problems—an attempt to make them appear simpler than they truly are. What made cheap marketing infographics so popular is perhaps their greatest contradiction: the false claim that a few pictograms and large numbers inherently have the power to “simplify complexity.” The phenomena that govern our world are, by definition, complex, multifaceted, and often difficult to grasp. So why would anyone want to dumb them down when making critical decisions or delivering important messages?\nYet, not all is bleak in this sudden craze for data visualization. We are becoming increasingly aware that there remains a considerable gap between the real potential hidden within vast datasets and the superficial images we typically use to represent them. More importantly, we now recognize that the first wave succeeded in familiarizing a broader audience with new visual languages and tools.\nHaving moved past what we might call the peak of infographics, we are left with a general audience equipped with some of the necessary skills to welcome a second wave of more meaningful, thoughtful visualization.\nWe are ready to question the impersonality of a purely technical approach to data and begin designing ways to connect numbers with what they truly represent: knowledge, behaviors, and people.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#references-1",
    "href": "chapters/sonification.html#references-1",
    "title": "5  Sonification",
    "section": "References",
    "text": "References\n\n\n\n\nGresham-Lancaster, Scot. 2012. “Relationships of Sonification to Music and Sound Art.” AI and Society 27 (2): 207–12.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#references.",
    "href": "chapters/sonification.html#references.",
    "title": "5  Sonification",
    "section": "5.6 References{.}",
    "text": "5.6 References{.}\n\n\n\n\nGresham-Lancaster, Scot. 2012. “Relationships of Sonification to Music and Sound Art.” AI and Society 27 (2): 207–12.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  },
  {
    "objectID": "chapters/sonification.html#references..unnumbered",
    "href": "chapters/sonification.html#references..unnumbered",
    "title": "5  Sonification",
    "section": "5.6 References{..unnumbered}",
    "text": "5.6 References{..unnumbered}\n\n\n\n\nGresham-Lancaster, Scot. 2012. “Relationships of Sonification to Music and Sound Art.” AI and Society 27 (2): 207–12.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sonification</span>"
    ]
  }
]