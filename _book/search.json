[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Creative Coding with Pure Data",
    "section": "",
    "text": "Preface\nThis book was born out of the classroom—but it does not stay there. It is the result of many years of teaching creative coding and interactive media at public universities, engaging with students from a wide range of disciplines, backgrounds, and interests. Over time, a common thread emerged: the desire to bridge artistic expression and technical skill, to write code not just as a means to an end, but as a space of exploration, experimentation, and play.\nIn these pages, you’ll find the distilled insights, exercises, and creative strategies that have shaped countless workshops and academic programs. The goal has always been twofold: to equip readers with the tools to build interactive digital systems, and to nurture a mindset where sound, movement, code, and structure can be explored as artistic materials. Whether it’s a generative soundscape, a data-driven artwork, or a custom tool built from scratch, the projects in this book are designed to foster technical growth while encouraging individual expression.\nThis is not a manual in the traditional sense, nor is it a fixed curriculum. Instead, this book invites you to engage with Pure Data on your own terms, navigating its chapters in whatever order best suits your curiosity. The modular structure is intentional: it supports creative detours, sudden insights, and unexpected connections between ideas. You are encouraged to experiment, remix, and stretch the boundaries of the examples presented. Treat the book as both a guide and a sandbox—one where art, sound, code, and interactivity come alive through your engagement.\nWhether you’re an artist exploring new tools, a programmer seeking creative outlets, or a student diving into the world of interactive media, I hope this book helps you discover how code can become a language for imagination.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-am-i",
    "href": "index.html#who-am-i",
    "title": "Creative Coding with Pure Data",
    "section": "Who am I?",
    "text": "Who am I?\nMy name is Ezequiel Abregú, and I am a sound artist, composer, multi-instrumentalist, and researcher originally from Buenos Aires, Argentina. My artistic practice encompasses sound recordings, audio installations, performances, sound sculptures, sound design, and compositions for chamber music, choreography, and theater. I am particularly interested in the interplay between music, performance, sound art, live electronics, auditory and visual perception, interactive media, and the application of technology in art.\n\n\n\n\nDr. Ezequiel Abregú\n\n\n\nI hold a Ph.D. focusing on the relationship between visual and auditory perception in sound art, and a degree in Composition with Electroacoustic Media from the National University of Quilmes (UNQ). My academic career includes teaching positions at several institutions: I am a professor at the University of Quilmes (Computing Applied to Music area), the National University of Arts (Multimedia Arts area), and the University of Tres de Febrero (Electronic Arts area).\nMy passion for programming and digital audio applications has led me to explore various programming languages and tools over the past two decades, including C, C++, Python, and Pure Data. I am an advocate of the open-source philosophy, regularly working with Linux and sharing my projects in publicly accessible repositories. My technical expertise extends to hardware development using microcontrollers and single-board computers, enabling me to adopt a hands-on approach in both my artistic and research endeavors.\nMore information about my work can be found on my personal website ezequielabregu.net.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "Creative Coding with Pure Data",
    "section": "What is this book about?",
    "text": "What is this book about?\nThe goal of this book is to support undergraduate and postgraduate students in exploring the intersection of creativity and technology alongside peers from diverse backgrounds. Building on years of teaching experience at several public universities1, this work encourages the integration of a creative mindset with programming skills to design original tools, algorithms, and artworks. Through this synthesis, the book invites students to engage with sound, interactivity, and control protocols in both technical and expressive dimensions.\nRather than offering a fixed, linear progression, the structure of this book is deliberately open and modular. Readers are encouraged to navigate the content according to their interests, needs, or curiosity. This flexibility supports an experimental approach to learning, where exploration and play are not only welcome but essential.\nBy approaching programming as a creative practice, this book invites you to think, make, and reflect through code. Whether you’re building an interactive sound installation, prototyping a digital instrument, or simply experimenting with new ideas, the goal is to empower you with the tools and concepts to express yourself in the digital domain—and to enjoy the process along the way.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Creative Coding with Pure Data",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book is intended for anyone interested in learning about creative coding using Pure Data. It is suitable for beginners who are new to programming and want to explore the world of interactive media, as well as for experienced programmers looking to expand their skills in sound synthesis and data visualization.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-you-going-to-learn",
    "href": "index.html#what-are-you-going-to-learn",
    "title": "Creative Coding with Pure Data",
    "section": "What are you going to learn?",
    "text": "What are you going to learn?\nIn this book, you will learn how to use Pure Data to create interactive audio-visual projects. You will explore various techniques for sound synthesis, data visualization, and interactive installations. The book will also cover best practices for organizing and managing your Pd projects, as well as tips for debugging and optimizing your code.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-are-you-dont-going-to-learn",
    "href": "index.html#what-are-you-dont-going-to-learn",
    "title": "Creative Coding with Pure Data",
    "section": "What are you don’t going to learn?",
    "text": "What are you don’t going to learn?\nThis book is not intended to be a comprehensive guide to all aspects of Pure Data. Instead, it focuses on specific topics and projects that are relevant to creative coding. While the book will cover a wide range of techniques and concepts, it will not delve into every detail or aspect of Pure Data. This book is not a substitute for the official Pure Data documentation or other resources. It is meant to complement these resources and provide a practical, hands-on approach to learning Pure Data.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#pre-requisites",
    "href": "index.html#pre-requisites",
    "title": "Creative Coding with Pure Data",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nTo get the most out of this book, you should have a basic understanding of programming and some familiarity with the Pure Data language.\nIf you are new to Pure Data or programming, there are several free online resources that can help you get started:\n\nPure Data FLOSS Manual – A beginner-friendly and comprehensive guide to Pure Data.\nOfficial Pure Data Documentation – The official manuals and reference materials for Pure Data.\nMiller Puckette’s “Theory and Techniques of Electronic Music” – A comprehensive book by the creator of Pure Data, covering both theory and practical techniques.\nProgramming Electronic Music in Pd - Designed for self-study, principally for composers. It begins with explanations of basic programming and acoustic principles then gradually builds up to the most advanced electronic music processing techniques.\nPatchstorage - A community-driven platform for sharing and discovering Pure Data patches. It features a wide range of projects, from simple examples to complex installations, and serves as a valuable resource for learning and inspiration.\n\nThese resources can be consulted before or alongside this book to strengthen your foundational knowledge.\n\nRecommended Pure Data distributions\nThis book is based on Pure Data Vanilla distribution, which is the most widely used version. You can download it from the official website:\nhttps://puredata.info/downloads/pure-data\nThis version is maintained by the original author, Miller Puckette, and is the most stable supported version of Pure Data. It includes all the core features and libraries needed for most projects, making it a great starting point for beginners and experienced users alike. Pure Data Vanilla is a free and open-source visual programming language for multimedia applications, widely used in the fields of sound synthesis, interactive installations, and live performance. It is available for Windows, macOS, and Linux operating systems.\nHowever, there are several other distributions that you may find useful for specific projects or needs:\n\nPurr Data – A fork of Pd-l2ork that focuses on usability and accessibility, with a more polished interface and additional features. Purr Data serves the same purpose, but offers a new and much improved graphical user interface and includes many 3rd party plug-ins. Like Pd, it runs on Linux, macOS and Windows, and is open-source throughout.\nPlugdata – plugdata is a plugin wrapper designed for Miller Puckette’s Pure Data (Pd), featuring an enhanced graphical user interface (GUI) created using JUCE, headed by Timothy Schoen. this project is still a work in progress, and may still have some bugs. By default, plugdata comes with the cyclone and ELSE collections of externals and abstractions.\n\n\n\nRecommended libraries and externals\nThe following libraries are recommended to be used with Pure Data. They are not included in the Pure Data Vanilla distribution, but they can be easily installed and used with it:\n\ncyclone\nELSE\niemlib\nlist-abs\nzexy\nGem\nceammc\ncomport\nmrpeach\nfreeverb~\njmmmp\nmapping\n\nThese libraries provide additional objects and features that can enhance your projects and make it easier to work with sound synthesis, data visualization, and interactive installations.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Creative Coding with Pure Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to express my gratitude to the following individuals and organizations for their support and contributions to this book:\n\nPure Data for providing a powerful and flexible platform for creative coding.\nThe Pd community for their invaluable resources, tutorials, and support.\nThe open-source community for their dedication to sharing knowledge and tools for creative coding.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Creative Coding with Pure Data",
    "section": "Contact",
    "text": "Contact\nIf you have any questions, comments, or feedback about this book, please feel free to reach out to me at eabregu.dev@gmail.com. I would love to hear from you!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Creative Coding with Pure Data",
    "section": "License",
    "text": "License\nThis book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. You are free to share and adapt the material, provided you give appropriate credit, do not use it for commercial purposes, and distribute your contributions under the same license.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Creative Coding with Pure Data",
    "section": "",
    "text": "Multimedia Arts UNA, Electronic Arts UNTREF, Bachelor of Music and Technology UNQ, Master’s Degree in Sound Art UNQ, Doctorate in Arts UNA↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is Pure Data?\nTraditionally, the process of creating software applications has relied heavily on text-based programming languages. Developers would write code manually in text files and execute them later to observe the outcome. While this method is efficient for those trained in programming, it often presents a steep learning curve for artists, musicians, and other creatives without a technical background. The abstract nature of text-based coding can act as a barrier to entry, especially for those more accustomed to working in visual or tactile media.\nPure Data offers a radically different approach by introducing a graphical programming environment tailored to creative exploration. Instead of writing lines of code, users work with visual representations of functions—called objects—that are placed and connected on a canvas. This paradigm allows users to construct interactive programs, known as “patches,” by literally drawing connections between elements. Each object can receive messages that modify its behavior in real time, creating a highly responsive and accessible system for building audio, visual, or multimedia tools.\nThe design of Pure Data echoes the modular synthesis systems of the 20th century, where sound was shaped by routing audio through a network of physical devices linked with patch cables. This historical reference not only grounds Pure Data in a legacy of experimental electronic music but also makes it intuitive for users familiar with analog workflows. By visually “patching” connections, users can simulate and extend these traditional processes in a digital context.\nTo fully engage with the creative potential of Pure Data, one can develop or modify algorithms for digital audio using imaginative coding strategies. For instance, building a real-time audio granulator that reinterprets live microphone input can offer both technical challenge and artistic reward. Through this lens, programming becomes a form of creative expression, blending logical structures with aesthetic decisions. By experimenting with signal flow, modulation techniques, or interactive sensors, users cultivate a mindset that is both inventive and analytical—unlocking new possibilities in digital sound design and performance.\nPure Data (or Pd) is a real-time graphical programming environment for audio, video, and graphical processing. Pure Data is commonly used for live music performance, VeeJaying, sound effects, composition, audio analysis, interfacing with sensors, using cameras, controlling robots or even interacting with websites. Because all of these various media are handled as digital data within the program, many fascinating opportunities for cross-synthesis between them exist. Sound can be used to manipulate video, which could then be streamed over the internet to another computer which might analyze that video and use it to control a motor-driven installation.\nProgramming with Pure Data is a unique interaction that is much closer to the experience of manipulating things in the physical world. The most basic unit of functionality is a box, and the program is formed by connecting these boxes together into diagrams that both represent the flow of data while actually performing the operations mapped out in the diagram. The program itself is always running, there is no separation between writing the program and running the program, and each action takes effect the moment it is completed.\nThe community of users and programmers around Pure Data have created additional functions (called “externals” or “external libraries”) which are used for a wide variety of other purposes, such as video processing, the playback and streaming of MP3s or Quicktime video, the manipulation and display of 3-dimensional objects and the modeling of virtual physical objects. There is a wide range of external libraries available which give Pure Data additional features. Just about any kind of programming is feasible using Pure Data as long as there are externals libraries which provide the most basic units of functionality required.\nThe core of Pure Data written and maintained by Miller S. Puckette and includes the work of many developers, making the whole package very much a community effort.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#why-pure-data",
    "href": "chapters/introduction.html#why-pure-data",
    "title": "1  Introduction",
    "section": "1.2 Why Pure Data?",
    "text": "1.2 Why Pure Data?\nPure Data (Pd) is a powerful, open-source environment for creative coding, offering a uniquely visual approach to programming that is especially suited for artists, musicians, and interactive media designers. Its intuitive interface and modular structure make it a flexible and accessible tool for developing real-time audio and visual projects—from live performances to experimental installations.\nOne of Pure Data’s standout features is its graphical programming interface, which replaces traditional lines of code with visual objects and patch cords. This allows users to construct complex behaviors by connecting elements on screen, making it easier to prototype and refine creative ideas. Real-time processing capabilities mean that audio and visual data can be generated, modified, and responded to instantly—ideal for performances, generative art, or interactive systems that react to sensors or user input.\nBeyond its creative potential, Pure Data offers practical advantages: it is cross-platform, running on Windows, macOS, and Linux, and integrates easily with tools like Arduino, Raspberry Pi, and Max/MSP, enabling hybrid systems that combine digital and physical components. A rich ecosystem of external libraries supports advanced functions like synthesis, visualization, and computer vision. Its open-source nature encourages exploration and collaboration, with a supportive community, extensive documentation, and countless tutorials available. Because it’s free and widely used in education, Pd is not only an effective tool for artistic expression but also a valuable learning resource for developing a strong foundation in programming and multimedia design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#what-is-creative-coding",
    "href": "chapters/introduction.html#what-is-creative-coding",
    "title": "1  Introduction",
    "section": "1.3 What is Creative Coding?",
    "text": "1.3 What is Creative Coding?\nCreative coding is the practice of using programming as a tool for artistic expression. It transforms code from a purely functional medium into a creative one, enabling the development of visual art, music, interactive experiences, and experimental media. This approach encourages artists, designers, and technologists to explore beyond the limits of traditional art forms, embracing code as a flexible and dynamic means of invention and communication.\nThe applications of creative coding are diverse, ranging from generative visuals and algorithmic design to responsive installations and live audiovisual performances. Creators often use languages and environments specifically designed to support creative work, such as Processing, OpenFrameworks, Max/MSP, and Pure Data. These platforms make it easier to manipulate data, control media in real time, and experiment with unconventional interfaces and outputs.\nImportantly, creative coding transcends the boundaries of specific disciplines. It can intersect with visual art, music, dance, theater, architecture, and even narrative writing. What unites these practices is the use of code as an expressive tool—one that invites innovation, play, and conceptual exploration. Closely tied to the values of the open-source movement, creative coding thrives in a culture of sharing, where artists and developers freely exchange code, tutorials, and ideas. This collaborative ecosystem fosters continuous learning and reinvention, empowering creators to expand what is possible through digital technology.\nFor an in-depth exploration of creative coding, consider checking out these resources:\n\nAwesome Creative coding - A curated list of resources, libraries, and tools for creative coding.\nCreative Code Berlin - A collection of creative coding resources, tutorials, and projects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/introduction.html#getting-started",
    "href": "chapters/introduction.html#getting-started",
    "title": "1  Introduction",
    "section": "1.4 Getting Started",
    "text": "1.4 Getting Started\nThis section will guide you through the installation of Pure Data Vanilla, the official and minimal distribution of Pure Data, on Windows, macOS, and Linux systems. The installation process is straightforward and will have you up and running in no time.\n\n1.4.1 Installing Pure Data Vanilla\nThe following is a step-by-step installation of Pure Data Vanilla, the official and minimal distribution of Pure Data, on Windows, macOS, and Linux systems.\n\n\n1.4.2 What is Pure Data “Vanilla”?\nPure Data (Pd) Vanilla is the standard version maintained by its original creator, Miller Puckette. It is lightweight, highly stable, and the most recommended version for beginners and experienced users alike.\n\n\n\n1.4.3 Installing on Windows\n\n1.4.3.1 Download the Installer\n\nVisit the official Pure Data website: https://puredata.info/downloads/pure-data\nScroll down to the Windows section.\nClick the latest version link (e.g., pd-0.55-2.windows-installer.exe).\n\n\n\n1.4.3.2 Run the Installer\n\nLocate the downloaded .exe file in your Downloads folder.\nDouble-click to run the installer.\nFollow the installation wizard:\n\nChoose the installation location (default is fine).\nAllow the program to create a Start Menu shortcut.\n\n\n\n\n1.4.3.3 Verify Installation\n\nAfter installation, open Pure Data from the Start Menu or desktop shortcut.\nThe main Pd window should appear with a blank patch ready.\n\n\n\n\n\n1.4.4 Installing on macOS\n\n1.4.4.1 Download the Disk Image\n\nVisit: https://msp.ucsd.edu/software.html\nScroll to the macOS section.\nDownload the .dmg file (e.g., Pd-0.54-1.dmg).\n\n\n\n1.4.4.2 Install the Application\n\nOpen the downloaded .dmg file.\nDrag the Pure Data icon into the Applications folder.\n\n\n\n1.4.4.3 Open Pure Data\n\nOpen your Applications folder.\nRight-click (or Ctrl + click) the Pure Data icon and select Open.\n\nThe first time, macOS may warn that the app is from an unidentified developer.\nConfirm to proceed.\n\n\n\n\n1.4.4.4 Optional: Enable Audio Permissions\n\nIf prompted, allow Pure Data to access the microphone.\nOpen System Preferences &gt; Security & Privacy &gt; Microphone and ensure Pure Data is enabled.\n\n\n\n\n\n1.4.5 Installing on Linux\nPure Data is available in the package repositories of most major Linux distributions. Below are instructions for popular systems.\n\n1.4.5.1 Debian/Ubuntu-based Systems\nsudo apt update\nsudo apt install puredata\n\n\n1.4.5.2 Fedora\nsudo dnf install puredata\n\n\n1.4.5.3 Arch Linux\nsudo pacman -S puredata\n\n\n1.4.5.4 Verify Installation\n\nOpen a terminal and type pd.\nThe Pure Data GUI should launch, displaying a blank patch.\nIf you encounter issues, check your package manager or consult the Pure Data community for troubleshooting.\n\n\n\n\n1.4.6 Installing Externals\nPure Data’s functionality can be extended through the use of externals—additional libraries that provide new objects and features. These externals are created by the Pd community and can add everything from new audio effects to advanced data processing tools. Installing externals is straightforward thanks to Pure Data’s built-in package manager.\n\n1.4.6.1 Step 1: Open Pure Data\n\nLaunch Pure Data (Pd) on your computer.\nMake sure you are using the Vanilla version for best compatibility with the package manager.\n\n\n\n1.4.6.2 Step 2: Access the “Find Externals” Tool\n\nIn the Pure Data menu, go to Help → Find Externals…\nThis opens the Deken package manager, which allows you to search for and install externals directly from within Pd.\n\n\n\n1.4.6.3 Step 3: Search for an External\n\nIn the search bar, type the name of the external or library you want to install (for example, zexy, cyclone, or iemlib).\nPress Enter or click the Search button.\nA list of matching externals will appear, showing available versions for your platform.\n\n\n\n1.4.6.4 Step 4: Install the External\n\nClick on the desired external in the list.\nChoose the version that matches your operating system and Pd version.\nClick Install.\nThe external will be downloaded and placed in your Pd externals folder (typically ~/Documents/Pd/externals on macOS and Linux, or C:\\Users\\&lt;YourName&gt;\\Documents\\Pd\\externals on Windows).\n\n\n\n1.4.6.5 Step 5: Add the External to Your Pd Path (if needed)\n\nMost externals are automatically available after installation.\nIf Pd cannot find the external, you may need to add its folder to Pd’s search path:\n\nGo to Preferences → Path…\nClick New and add the path to the external’s folder (for example, ~/Documents/Pd/externals/cyclone).\nClick OK and restart Pd.\n\n\n\n\n1.4.6.6 Step 6: Use the External in Your Patch\n\nIn your patch, create an object with the name of the external’s library, followed by the object you want to use.\nFor example, to use the counter object from the cyclone library:\n[cyclone/counter]\nSome libraries require you to load them with a special object, such as [declare -lib cyclone] at the top of your patch.\n\n\n\n1.4.6.7 Step 7: Verify Installation\n\nIf the object appears without errors (no red box), the external is installed correctly.\nIf you see errors, double-check the installation path and that you are using the correct object/library name.\n\n\n\n1.4.6.8 Manual Installation (Advanced)\nIf you need to install an external manually:\n\nDownload the external from https://deken.puredata.info/ or the developer’s website.\nExtract the files to your Pd externals folder.\nAdd the folder to Pd’s path as described above.\n\n\nTip: Always check the documentation for each external, as installation steps or requirements may vary.\n\nFor more details, see the official Pure Data documentation on externals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html",
    "href": "chapters/playrec.html",
    "title": "2  Rec & Play",
    "section": "",
    "text": "2.1 I’m Sitting in a Room – Alvin Lucier\nIn this chapter, we will explore the relationship between recording and playback. We will look at how these two processes can be used to create new sounds and compositions. We will also discuss the technical aspects of recording and playback, including the equipment and techniques used in the process. We will also consider the artistic implications of recording and playback, including how these processes can be used to create new forms of expression and communication.\nI’m Sitting in a Room (Alvin Lucier) consists of a 15-minute and 23-second sound recording. The piece opens with Lucier’s voice as he declares he is sitting in a room different from ours. His voice trembles slightly as he delivers the text, describing what will unfold over the next 15 minutes:\nAs listeners, we know what is going to happen, but we don’t know how it will happen (Hasse 2012). We listen, following Lucier’s recorded voice. Then, Lucier plays the recording into the room and re-records it. This time, we begin to hear more of the room’s acoustic characteristics. He continues to play back and re-record his voice—over and over—until his speech becomes softened, almost dissolved, into the sonic reflections of the room in which the piece was recorded and re-recorded.\nOne effective way to study a piece is to replicate its technical aspects and the devices involved. The aim of this activity is to recreate the technical setup of I’m Sitting in a Room, focusing on the processes of recording, playback, and automation.\nThere are many ways to implement this technical system. Just to mention two environments we’re currently working with: it’s relatively straightforward in Pure Data.\nIn terms of hardware, besides a computer, you’ll need a speaker (mono audio output), a microphone (mono audio input), and a semi-reverberant room.\nThe system should include at least two manual (non-automated) controls:\nChoose a room whose acoustic or musical qualities you’d like to evoke. Connect the microphone to the input of tape recorder #1. From the output of tape recorder #2, connect to an amplifier and speaker. Use the following text, or any other text of any length:\nThe following steps outline the process:\nAll the recorded generations, presented in chronological order, create a tape composition whose duration is determined by the length of the original statement and the number of generations produced. Make versions in which a single statement is recycled through different rooms. Create versions using one or more speakers in different languages and spaces. Try versions where, for each generation, the microphone is moved to different parts of the room(s). You may also develop versions that can be performed in real time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#im-sitting-in-a-room-alvin-lucier",
    "href": "chapters/playrec.html#im-sitting-in-a-room-alvin-lucier",
    "title": "2  Rec & Play",
    "section": "",
    "text": "Alvin Lucier performing I’m sitting in a room\n\n\n\n\n“…I am recording the sound of my speaking voice and I am going to play it back into the room again and again…”\n\n\n\n\n\n\n\nStart recording\nStop recording\n\n\n\nI am sitting in a room different from the one you are in now. I am recording the sound of my speaking voice, and I am going to play it back into the room again and again until the resonant frequencies of the room reinforce themselves so that any semblance of my speech, with perhaps the exception of rhythm, is destroyed. What you will hear, then, are the natural resonant frequencies of the room articulated by speech. I regard this activity not so much as a demonstration of a physical fact, but more as a way to smooth out any irregularities my speech might have.\n\n\n\nRecord your voice through the microphone into tape recorder #1.\nRewind the tape, transfer it to tape recorder #2, and play it back into the room through the speaker. - Record a second generation of the statement via the microphone back into tape recorder #1.\nRewind this second generation to the beginning and splice it to the end of the original statement in tape recorder #2.\nPlayback only the second generation into the room and record a third generation into tape recorder #1.\nContinue this process through multiple generations.\n\n\n\n\n\nI am sitting in a room design\n\n\n\n2.1.1 Pure Data implementation of I’m sitting in a room\n  \nIn this section, we will break down the Pure Data patch I-am-sitting-in-a-room.pd step by step. This patch is inspired by Alvin Lucier’s iconic piece and demonstrates how to recursively record and play back sound to reveal the resonant frequencies of a room.\n\n\n2.1.1.1 Conceptual Overview\nThe patch enables you to:\n\nRecord your voice or any sound in a room.\nPlay back the recording into the room and re-record it.\nRepeat this process, so the room’s resonances become more pronounced with each generation.\n\n\n\n\n\n2.1.2 System Diagram\nThe following diagram illustrates the recursive nature of the recording and playback process, similar to how Lucier’s piece unfolds. Each generation of tape recorders captures the previous one, creating a feedback loop that emphasizes the room’s resonances.\n\n\n\n\n\nflowchart TB\n  %% Top level source\n  Input[Microphone]\n  \n  %% Horizontal arrangement of recorders A and B\n  subgraph PLAYBACK RECORDERS\n    direction LR\n    subgraph RecorderA [DEVICE A]\n      RecordA[writesf~ A.wav]\n      PlayA[readsf~ A.wav]\n      RecordA -.-&gt; PlayA\n    end\n    \n    subgraph RecorderB [DEVICE B]\n      RecordB[writesf~ B.wav]\n      PlayB[readsf~ B.wav]\n      RecordB -.-&gt;PlayB\n    end\n  end\n  \n  %% Output at the bottom  \n  Output[Speaker]\n  \n  %% Clear connections showing signal flow\n  Input --&gt; RecordA\n  Input ==&gt; Output\n  \n  PlayA --&gt; Output\n  PlayB --&gt; Output\n  \n  PlayA --&gt; RecordB\n  PlayB --&gt; RecordA\n  \n  %% Feedback path showing recursion\n  PlayB -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordA\n  PlayA -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordB\n  Output ==&gt; room[\"ROOM'S&lt;br&gt;REFLECTIONS\"]:::roomStyle ==&gt; Input\n\n  classDef roomStyle fill:#f5f5f5,stroke:#333,stroke-dasharray:5 5 \n\n\n\n\n\n\n\n\n2.1.3 Step 1: Audio Input and Routing\n\nadc~ receives audio from your microphone.\nThe signal is sent to s~ audio.input, making it available to other parts of the patch.\nThis modular routing allows the same input to be used for both recording and playback chains.\n\n\n\n\n2.1.4 Step 2: Recording Your Voice (record-A.wav)\n\nPress the Start Recording button (bng labeled “Start Recording”).\nThis triggers a message:\nopen record-A.wav, start → writesf~\nThe incoming audio from your microphone is now being recorded to record-A.wav.\nPress the Stop Recording button (bng labeled “Stop Recording”) to send the stop message, ending the recording.\n\n\n\n\n2.1.5 Step 3: Playback and Re-recording (record-B.wav)\n\nTo play back your recording, another button triggers:\nopen record-A.wav, start → readsf~\nThe playback signal is routed to:\n\nthrow~ audio (so you hear it through the speakers)\nwritesf~ (recording to record-B.wav)\n\nThis means the playback of your first recording is re-recorded, capturing both the original sound and the room’s response.\n\n\n\n\n2.1.6 Step 4: Recursive Process\n\nYou can repeat the process:\n\nPlay back record-B.wav and record it again, each time reinforcing the room’s resonances.\n\nEach iteration makes the speech less intelligible and the room’s resonant frequencies more prominent, just like in Lucier’s piece.\n\n\n\n\n2.1.7 Step 5: Output\n\nAll audio routed to throw~ audio is collected by catch~ audio and sent to dac~ for playback through your speakers or headphones.\n\n\n\n\n2.1.8 Key Objects Table\n\n\n\nObject\nPurpose\n\n\n\n\nadc~\nAudio input from microphone\n\n\nwritesf~\nRecord audio to a file\n\n\nreadsf~\nPlay audio from a file\n\n\nthrow~/catch~\nMix and route audio signals\n\n\ndac~\nAudio output to speakers\n\n\nbng\nButton for triggering actions\n\n\nmsg\nSend commands to objects (open, start, stop)\n\n\n\n\n\n\n2.1.9 How to Use the Patch\n\nStart Recording:\nClick the “Start Recording” button and speak or make a sound.\nStop Recording:\nClick the “Stop Recording” button to finish.\nPlayback and Re-record:\nUse the playback button to play your recording into the room and simultaneously re-record it.\nRepeat:\nRepeat the playback and re-recording process as many times as you like to hear the room’s resonances emerge.\n\n\nThis patch is a practical and creative way to explore acoustic phenomena and the transformation of sound through recursive recording, echoing the spirit of Lucier’s original work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html",
    "href": "chapters/sequencers.html",
    "title": "3  Sequencers",
    "section": "",
    "text": "3.1 Arrays and Sequencers\nIn this chapter, we will explore the concept of sequencers and how they can be implemented in Pure Data.\nSequencer is a tool for organizing and controlling the playback of events in a temporally ordered sequence. This sequence consists of a series of discrete steps, where each step represents a regular time interval and can contain information about event activation or deactivation.\nIn addition to controlling musical events such as notes, chords, and percussions, a step sequencer can also manage a variety of other events. This includes parameter changes in virtual instruments or audio/video synthesizers, real-time effect automation, lighting control in live performances or multimedia installations, triggering of samples to create patterns and effect sequences, as well as firing MIDI control events to operate external hardware or software.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html#arrays-and-sequencers",
    "href": "chapters/sequencers.html#arrays-and-sequencers",
    "title": "3  Sequencers",
    "section": "",
    "text": "P1\nP2\nP3\nP4\nP5\nP6\nP7\nP8\n\n\n\n\nx\nx\nx\nx\nx\nx\nx\nx\n\n\ni0\ni1\ni2\ni3\ni4\ni5\ni6\ni7\n\n\n\n\n\nEach cell represents a step in the sequencer.\n“P1” to “P8” indicate the steps numbered 1 to 8.\nEmpty cells represent steps without events or notes.\nYou can fill each cell with notes or events to represent the desired sequence.\n\n\n3.1.1 Represent an 8-step sequencer using pseudo code \n\n3.1.1.1 Formulation (pseudo code)\nData Structures:\n- Define a data structure to represent each step of the sequence (e.g., array, list).\n\nVariables:\n- Define an array to store the MIDI note values for each step.\n- Initialize the sequencer parameters:\n  - CurrentStep = 0\n  - Tempo\n  - NumberOfSteps = 8\n\nAlgorithm:\n1. Initialization:\n   a. Set CurrentStep to 0.\n   b. Ask the user to input the Tempo.\n   c. Set NumberOfSteps to 8.\n\n2. Loop:\n   a. While true:\n      i. Calculate the time duration for each step based on the Tempo.\n      ii. Play the MIDI note corresponding to the CurrentStep.\n      iii. Increment CurrentStep.\n      iv. If CurrentStep exceeds NumberOfSteps, reset it to 0.\n      v. Wait the calculated duration before moving to the next step.\n \n\n\n\n3.1.2 Python Implementation of the 8-Step Sequencer\nimport time\nimport mido\n\n# 1. Define a data structure to represent each step in the sequence (8 steps)\nsecuencia = [None] * 8\n\n# 2. Define an array to store MIDI note values for each step\nnotas_midi = [60, 62, 64, 65, 67, 69, 71, 72]  # C major scale\n\n# 3. Initialize the sequencer parameters\npaso_actual = 0\ntempo = 120  # beats per minute\nnum_pasos = len(secuencia)\n\n# Open the virtual MIDI port (use the correct port name, see MIDI_port_name.py)\npuerto_salida = mido.open_output('IAC Driver Bus 1')\n\n# 4. Start a loop to continuously play the sequence\nwhile True:\n    # a. Calculate the time duration for each step based on the tempo\n    duracion_paso = 60.0 / tempo  # beat duration in seconds\n\n    # b. \"Play\" the MIDI note corresponding to the current step\n    nota_actual = int(notas_midi[paso_actual])  # Ensure it's an integer\n    secuencia[paso_actual] = nota_actual\n    print(f\"Step: {paso_actual} --- Note: {secuencia[paso_actual]}\")\n\n    # Send the MIDI message\n    mensaje = mido.Message('note_on', note=nota_actual)\n    puerto_salida.send(mensaje)\n\n    # c. Increment the current step\n    paso_actual += 1\n\n    # d. If current step exceeds the number of steps, reset it to 0\n    if paso_actual &gt;= num_pasos:\n        paso_actual = 0\n\n    # e. Wait for the calculated duration before moving to the next step\n    time.sleep(duracion_paso)\n\n    # Turn off the MIDI note\n    mensaje = mido.Message('note_off', note=nota_actual)\n    puerto_salida.send(mensaje)\nThis is a simple step sequencer for MIDI. It uses the mido library to send MIDI messages and the time library to control the timing of the sequence.\nThe script starts by importing the necessary libraries and defining some data structures and initial variables. The secuencia list will hold the sequence of notes to be played, and is initially filled with None values. The notas_midi list contains the MIDI note values for each step in the sequence, representing a C Major scale.\nThen, the script initializes some sequencer parameters. The paso_actual variable tracks the current step in the sequence, tempo sets the tempo in beats per minute, and num_pasos stores the total number of steps in the sequence.\nThe script opens a virtual MIDI output port using mido.open_output(). You may need to change the port name ‘IAC Driver Bus 1’ depending on your system setup.\nThe main part of the script is a while loop that continuously plays the sequence. For each step, it calculates the step duration based on the tempo, plays the corresponding MIDI note, increments the current step, and waits for the calculated duration before moving to the next step. If the current step exceeds the number of steps in the sequence, it resets to 0. After each note is played, a note_off MIDI message is sent to stop the note.\n \n\n\n3.1.3 Represent a random sequencer using pseudo code\n \n\n\n3.1.4 Random Step Sequencer Implementation\nimport time\nimport mido\nimport random\n\n# 1. Define a data structure to represent each step of the sequence (8 steps)\nsecuencia = [None] * 8\n\n# 2. Define an array to store MIDI note values for each step\nnotas_midi = [60, 62, 64, 65, 67, 69, 71, 72]  # C major scale\n\n# 3. Initialize the sequencer parameters\npaso_actual = random.randint(0, 7)\ntempo = 120  # beats per minute\nnum_pasos = len(secuencia)\n\n# Open the virtual MIDI port (use the correct port name, see MIDI_port_name.py)\npuerto_salida = mido.open_output('IAC Driver Bus 1')\n\n# 4. Start a loop to continuously play the sequence\nwhile True:\n    # a. Calculate the time duration for each step based on the tempo\n    duracion_paso = 60.0 / tempo  # beat duration in seconds\n\n    # b. \"Play\" the MIDI note corresponding to the current step\n    nota_actual = int(notas_midi[paso_actual])  # Ensure it's an integer\n    secuencia[paso_actual] = nota_actual\n    print(f\"Step: {paso_actual} --- Note: {secuencia[paso_actual]}\")\n\n    # Send the MIDI message\n    mensaje = mido.Message('note_on', note=nota_actual)\n    puerto_salida.send(mensaje)\n\n    # c. Randomize the next step\n    paso_actual = random.randint(0, 7)\n\n    # d. If current step exceeds the number of steps, reset it to 0\n    if paso_actual &gt;= num_pasos:\n        paso_actual = 0\n\n    # e. Wait for the calculated duration before moving to the next step\n    time.sleep(duracion_paso)\n\n    # Turn off the MIDI note\n    mensaje = mido.Message('note_off', note=nota_actual)\n    puerto_salida.send(mensaje)\nThis Python script is a modified version of the step-by-step MIDI sequencer from the previous example. The main difference lies in how the current step (paso_actual) is incremented.\nIn the original script, the current step increased sequentially, creating a predictable pattern of notes. In this modified version, the current step is set to a random integer between 0 and 7 (inclusive). This means that the note sequence will be played in a random order, creating a more unpredictable pattern.\n—",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html#piano-phase-steve-reich",
    "href": "chapters/sequencers.html#piano-phase-steve-reich",
    "title": "3  Sequencers",
    "section": "3.2 Piano Phase (Steve Reich)",
    "text": "3.2 Piano Phase (Steve Reich)\n\nView Piano Phase Score (S. Reich)\nPiano Phase can be conceived as an algorithm. Starting with two pianos playing the same sequence of notes at the same speed, one of the pianists begins to gradually accelerate their tempo.\nWhen the separation between the notes played by both pianos reaches a fraction of a note’s duration, the phase is shifted and the cycle repeats. This process continues endlessly, creating a hypnotic effect of rhythmic phasing that evolves over time as an infinite sequence of iterations.\n\n\n\nPhase Difference\n\n\n\n\n\n  Performance and visualization of the first section from Steve Reich’s 1967 piece Piano Phase\n\n\nFigure 3.1\n\n\n\n\n3.2.1 Piano Phase process as an algorithm.\n\n3.2.1.1 Algorithm Formulation (pseudo code)\n1. Initialize two pianists with the same note sequence.\n2. Set an initial tempo for both pianists.\n3. Repeat until the desired phase is reached:\n     a. Gradually increase the second pianist's tempo.\n     b. Compare note positions between both pianists.\n     c. When the separation between the notes reaches a fraction of a note’s duration, invert the phase.\n     d. Continue playback.\n4. Repeat the cycle indefinitely to create a continuous and evolving rhythmic phasing effect.\n\n\n\n3.2.2 Piano Phase Implementation in Python (single sequencer)\nimport time\nimport mido\n\n# Define a MIDI note sequence to represent the musical part\nnotas_midi = [64, 66, 71, 73, 74, 66, 64, 73, 71, 66, 74, 73]  # C major scale\n\n# Open two MIDI output ports\npuerto_salida1 = mido.open_output('IAC Driver Bus 1')\n\n# Initialize the sequencer parameters\ntempo = int(120 * 4)  # beats per minute\nduracion_paso = 60.0 / tempo  # beat duration in seconds\n\n# Start a loop to continuously play the sequence\npaso_actual = 0\nwhile True:\n    # \"Play\" the MIDI note corresponding to the current step\n    nota_actual = notas_midi[paso_actual]\n    mensaje_on = mido.Message('note_on', note=nota_actual)\n    mensaje_off = mido.Message('note_off', note=nota_actual)\n\n    puerto_salida1.send(mensaje_on)\n    time.sleep(duracion_paso)  # Wait one beat before going to the next step\n    puerto_salida1.send(mensaje_off)  # Send 'note_off' message\n\n    # Increment the current step\n    paso_actual += 1\n    if paso_actual &gt;= len(notas_midi):\n        paso_actual = 0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html#random-melody-generator",
    "href": "chapters/sequencers.html#random-melody-generator",
    "title": "3  Sequencers",
    "section": "3.3 Random Melody Generator",
    "text": "3.3 Random Melody Generator\nThis chapter provides a step-by-step explanation of the music-scale-B.pd Pure Data patch. The patch demonstrates how to generate a melody using a musical scale, store it in an array, and play it back using MIDI.\n\n\n\nRandom Melody Generator\n\n\n\n3.3.1 Patch Overview\nThe patch is organized into several functional blocks:\n\nScale and Melody Generation\nMelody Storage\nPlayback Control\nMIDI Output\n\nBelow is a simplified diagram of the main data flow:\n\n\n\n\n\nflowchart TD\n    A[Scale Input] --&gt; B[Melody Generation]\n    B --&gt; C[Store in Array]\n    C --&gt; D[Playback Control]\n    D --&gt; E[MIDI Output]\n\n\n\n\n\n\n\n\n\n3.3.2 Scale and Melody Generation\n\n3.3.2.1 Scale Definition\nThe patch starts with a message box containing the scale intervals:\n0 2 4 5 7 9 11\nFor example, this represents a major scale in semitones accordding to the following mapping:\n\n\n\n\nNote\nInterval\n\n\n\n\nC\n0\n\n\nC# / Db\n1\n\n\nD\n2\n\n\nD# / Eb\n3\n\n\nE\n4\n\n\nF\n5\n\n\nF# / Gb\n6\n\n\nG\n7\n\n\nG# / Ab\n8\n\n\nA\n9\n\n\nA# / Bb\n10\n\n\nB\n11\n\n\n\n\n \nThe scale is appended to a list and processed to determine its length.\n\n\n3.3.2.2 Random Note Selection\nA random number between 0 and 47 is generated (random 48), then shifted up by 60 to get a MIDI note in a typical range.\nThe note is then mapped to the scale using modulo operations and list indexing.\n\n\n3.3.2.3 Melody Construction\nThe patch uses a loop (until, i, + 1) to generate a sequence of notes.\nEach note is calculated based on the scale and stored in a list.\n\n\n\n3.3.3 Melody Storage\nThe generated melody is stored in a Pure Data array called melody.\nThe array is visualized in the patch for reference.\n\n\n\n\n\ngraph LR\n    MelodyList --&gt;|tabwrite| MelodyArray\n\n\n\n\n\n\n\n\n3.3.4 Playback Control\nA metro object (metronome) triggers playback at a tempo set by a horizontal slider.\nEach bang from the metronome advances an index, which reads the next note from the melody array.\n\n\n3.3.5 MIDI Output\nThe note value is sent to a makenote object, which creates a MIDI note with velocity and duration.\nThe note is then sent to the noteout object, which outputs the MIDI note to your system’s MIDI device.\n\n\n3.3.6 Step-by-Step Data Flow\n\nInitialize Scale: The scale intervals are defined and appended to a list.\nGenerate Melody: A loop generates random notes mapped to the scale, storing them in the melody array.\nPlayback: A metronome triggers reading from the array, sending notes to MIDI output.\nVisualization: The melody array is displayed as a graph in the patch.\n\n\n\n3.3.7 Key Objects and Their Roles\n\n\n\n\nObject\nPurpose\n\n\n\n\nrandom 48\nGenerates random note indices\n\n\n+ 60\nShifts notes to a higher MIDI octave\n\n\nmod 12\nMaps notes to scale degrees\n\n\nlist-idx\nRetrieves scale degree from the list\n\n\ntabwrite\nWrites notes to the melody array\n\n\nmetro\nControls playback timing\n\n\ntabread\nReads notes from the melody array\n\n\nmakenote\nCreates MIDI notes with velocity/duration\n\n\nnoteout\nSends MIDI notes to output\n\n\n\n\n\n\n3.3.8 Diagram: Melody Generation and Playback\n\n\n\n\n\nflowchart TD\n    S[Scale Message] --&gt; L[list Append]\n    L --&gt; R[random + Offset]\n    R --&gt; M[Modulo/Indexing]\n    M --&gt; A[Melody Array]\n    A --&gt; P[Playback - metro, tabread]\n    P --&gt; MN[makenote]\n    MN --&gt; NO[noteout]\n\n\n\n\n\n\n\n\n3.3.9 Summary\nThis patch demonstrates how to algorithmically generate a melody using a musical scale, store it, and play it back via MIDI in Pure Data. The modular structure allows for easy experimentation with different scales, lengths, and playback parameters.\n\nHere is the translated version of your Markdown text with the code, links, and format preserved:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html#the-euclidean-algorithm-generates-traditional-musical-rhythms",
    "href": "chapters/sequencers.html#the-euclidean-algorithm-generates-traditional-musical-rhythms",
    "title": "3  Sequencers",
    "section": "3.4 The Euclidean Algorithm Generates Traditional Musical Rhythms",
    "text": "3.4 The Euclidean Algorithm Generates Traditional Musical Rhythms\nGodfried Toussaint (2005)\n\n\n\nEucliden image\n\n\nLink to the original article\n\n3.4.1 Summary\nThe Euclidean algorithm (as presented in Euclid’s Elements) calculates the greatest common divisor of two given integers. It is shown here that the structure of the Euclidean algorithm can be used to efficiently generate a wide variety of rhythms used as timelines (ostinatos) in sub-Saharan African music in particular, and world music in general. These rhythms, here called Euclidean rhythms, have the property that their onset patterns are distributed as evenly as possible. Euclidean rhythms also find application in nuclear physics accelerators and computer science and are closely related to several families of words and sequences of interest in the combinatorics of words, such as Euclidean strings, with which the rhythms are compared.\n\n\n3.4.2 Hypothesis\nSeveral researchers have observed that rhythms in traditional world music tend to exhibit patterns distributed as regularly or evenly as possible.\nPatterns of maximal evenness can be described using the Euclidean algorithm on the greatest common divisor of two integers.\n\n\n3.4.3 Patterns of Maximal Evenness\nThe Pattern of Maximal Evenness is a concept used in music theory to create Euclidean rhythms. Euclidean rhythms are rhythmic patterns that evenly distribute beats over a time cycle.\nIn essence, the Pattern of Maximal Evenness seeks to distribute a specific number of beats evenly within a given time span. This is achieved by dividing the time into equal parts and assigning beats to these divisions uniformly.\nExample:\nx = beat\n· = rest\n[×· ×· ×· ×· ] → [1 0 1 0 1 0 1 0] (8,4) = 4 beats evenly distributed over 8 pulses\n[×· ·×· ·×·] → [1 0 0 1 0 0 1 0] (8,3) = 3 beats evenly distributed over 8 pulses\n\n\n3.4.4 Euclidean Algorithm\nOne of the oldest known algorithms, described in Euclid’s Elements (around 300 BCE) in Proposition 2 of Book VII, now known as the Euclidean algorithm, calculates the greatest common divisor of two given integers.\nThe idea is very simple. The smaller number is repeatedly subtracted from the larger until the larger becomes zero or smaller than the smaller one, in which case it becomes the remainder. This remainder is then repeatedly subtracted from the smaller number to obtain a new remainder. This process continues until the remainder is zero.\nTo be more precise, consider the numbers 5 and 8 as an example:\n\nFirst, we divide 8 by 5. This gives a quotient of 1 and a remainder of 3.\nThen, we divide 5 by 3, which gives a quotient of 1 and a remainder of 2.\nNext, we divide 3 by 2, which gives a quotient of 1 and a remainder of 1.\nFinally, we divide 2 by 1, which gives a quotient of 2 and a remainder of 0.\n\nThe idea is to keep dividing the previous divisor by the remainder obtained in each step until the remainder is 0. When we reach a remainder of 0, the previous divisor is the greatest common divisor of the two numbers.\nIn short, the process can be seen as a sequence of equations:\n8 = (1)(5) + 3\n5 = (1)(3) + 2\n3 = (1)(2) + 1\n2 = (1)(2) + 0\nNote: 8 = (1)(5) + 3 means that 8 is divided by 5 once, yielding a quotient of 1 and a remainder of 3.\n \nIn essence, it involves successive divisions to find the greatest common divisor of two positive numbers (GCD from now on).\nThe GCD of two numbers a and b, assuming a &gt; b, is found by first dividing a by b, and obtaining the remainder r.\nThe GCD of a and b is the same as that of b and r. When we divide a by b, we obtain a quotient c and a remainder r such that:\na = c · b + r\nExamples:\nLet’s compute the GCD of 17 and 7.\nSince 17 = 7 · 2 + 3, then GCD(17, 7) is equal to GCD(7, 3). Again, since 7 = 3 · 2 + 1, then GCD(7, 3) is equal to GCD(3, 1). Here, it is clear that the GCD between 3 and 1 is simply 1. Therefore, the GCD between 17 and 7 is also 1.\nGCD(17,7) = 1\n\n17 = 7 · 2 + 3\n\n7 = 3 · 2 + 1\n\n3 = 1 · 3 + 0\n \nAnother example:\nGCD(8,3) = 1\n\n8 = 3 · 2 + 2\n\n3 = 2 · 1 + 1\n\n2 = 1 · 2 + 0\n\n\n3.4.5 How does computing the GCD turn into maximally even distributed patterns?\nRepresent a binary sequence of k ones [1] and n − k zeros [0], where each [0] bit represents a time interval and the ones [1] indicate signal triggers.\nThe problem then reduces to:\nConstruct a binary sequence of n bits with k ones such that the ones are distributed as evenly as possible among the zeros.\nA simple case is when k evenly divides n (with no remainder), in which case we should place ones every n/k bits. For example, if n = 16 and k = 4, then the solution is:\n[1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0]\nThis case corresponds to n and k having a common divisor of k (in this case 4).\nMore generally, if the greatest common divisor between n and k is g, we would expect the solution to decompose into g repetitions of a sequence of n/g bits.\nThis connection with greatest common divisors suggests that we could compute a maximally even rhythm using an algorithm like Euclid’s.\n \n\n\n3.4.6 Example (13, 5)\nLet’s consider a sequence with n = 13 and k = 5.\nSince 13 − 5 = 8, we start with a sequence consisting of 5 ones, followed by 8 zeros, which can be thought of as 13 one-bit sequences:\n[1 1 1 1 1 0 0 0 0 0 0 0 0]\nWe begin moving zeros by placing one zero after each one, creating five 2-bit sequences, with three remaining zeros:\n[10] [10] [10] [10] [10] [0] [0] [0]\n13 = 5 · 2 + 3\nThen we distribute the three remaining zeros similarly, placing a [0] after each [10] sequence:\n[100] [100] [100] [10] [10]\n5 = 3 · 1 + 2\nWe now have three 3-bit sequences, and a remainder of two 2-bit sequences. So we continue the same way, placing one [10] after each [100]:\n[10010] [10010] [100]\n3 = 2 · 1 + 1\nThe process stops when the remainder consists of a single sequence (here, [100]), or we run out of zeros.\nThe final sequence is, therefore, the concatenation of [10010], [10010], and [100]:\n[1 0 0 1 0 1 0 0 1 0 1 0 0]\n2 = 1 · 2 + 0\n \n\n\n3.4.7 Example (17, 7)\nSuppose we have 17 pulses and want to evenly distribute 7 beats over them.\n1. We align the number of beats and silences (7 ones and 10 zeros):\n[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n\n\n\n1 1 1 1 1 1 1\n0 0 0 0 0 0 0 0 0 0\n\n\n\n\n\n2. We form 7 groups, corresponding to the division of 17 by 7; we get 7 groups of [1 0] and 3 remaining zeros [000], which means the next step forms 3 groups until only one or zero groups remain.\n[1 0] [1 0] [1 0] [1 0] [1 0] [1 0] [1 0] [0] [0] [0]\n17 = 7 · 2 + 3\n\n\n\n1\n1\n1\n1\n1\n1\n1\n0 0 0\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n \n3. Again, this corresponds to dividing 7 by 3. In our case, we are left with only one group and we are done.\n \n[1 0 0] [1 0 0] [1 0 0] [1 0] [1 0] [1 0] [1 0]\n\n\n\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n[1 0 0 1 0] [1 0 1 0 0] [1 0 0 1 0] [1 0]\n7 = 3 · 2 + 1\n\n\n\n1\n1\n1\n1\n\n\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n1\n1\n1\n\n\n\n\n\n\n0\n0\n0\n\n\n\n\n\n\n\n \n4. Finally, the rhythm is obtained by reading the grouping column by column, from left to right, step by step.\n[1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0]\n3 = 1 · 3 + 0\n \n\n\n3.4.8 Implementation in Pure Data - Euclidean sequencer\n(index * hits ) % steps\n↓\n[&lt; notes]\nwhere:\nindex = index of the Euclidean series (array)\nhits = number of notes to be played\nsteps = array size\n\n\n3.4.9 References\n\nThe Euclidean Algorithm Generates Traditional Musical Rhythms\nPiano-Phase\n\n\nHere is the full English translation of your markdown document, keeping the code, links, and formatting as requested:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/sequencers.html#cellular-automata",
    "href": "chapters/sequencers.html#cellular-automata",
    "title": "3  Sequencers",
    "section": "3.5 Cellular Automata",
    "text": "3.5 Cellular Automata\n\n\n\nCellular Automaton\n\n\nA cellular automaton is a mathematical and computational model for a dynamic system that evolves in discrete steps. It is suitable for modeling natural systems that can be described as a massive collection of simple objects interacting locally with each other.\nA cellular automaton is a collection of “colored” cells on a grid of specified shape that evolves through a series of discrete time steps according to a set of rules based on the states of neighboring cells. The rules are then applied iteratively over as many time steps as desired.\nCellular automata come in a variety of forms and types. One of the most fundamental properties of a cellular automaton is the type of grid on which it is computed. The simplest such grid is a one-dimensional line. In two dimensions, square, triangular, and hexagonal grids can be considered.\nOne must also specify the number of colors (or distinct states) k that a cellular automaton can assume. This number is typically an integer, with k=2 (binary, [1] or [0]) being the simplest choice. For a binary automaton, color 0 is commonly referred to as “white” and color 1 as “black”. However, cellular automata with a continuous range of possible values can also be considered.\nIn addition to the grid on which a cellular automaton resides and the colors its cells can assume, one must also specify the neighborhood over which the cells influence each other.\n\n3.5.1 One-Dimensional Cellular Automata (1DCA)\nThe simplest option is \"nearest neighbors\", where only the cells directly adjacent to a given cell can influence it at each time step. Two common neighborhoods in the case of a two-dimensional cellular automaton on a square grid are the so-called Moore neighborhood (a square neighborhood) and the von Neumann neighborhood (a diamond-shaped neighborhood).\nCellular automata are considered as a vector. Each component of the vector is called a cell. Each cell is assumed to take only two states:\n\n[0] (white)\n[1] (black)\n\nThis type of automaton is known as an elementary one-dimensional cellular automaton (1DCA). A dynamic process is performed, starting from an initial configuration C(0) of each of the cells (stage 0), and at each new stage, the state of each cell is calculated based on the state of the neighboring cells and the cell itself in the previous stage.\n\n\n3.5.2 The Case of Rule 30\nRule 30 is a binary one-dimensional cellular automaton introduced by Stephen Wolfram in 1983. It considers an infinite one-dimensional array of cellular automaton cells with only two states, with each cell in some initial state.\nAt discrete time intervals, each cell changes state spontaneously based on its current state and the state of its two neighbors.\nWhat it consists of:\n\nEach cell can be in one of two states: alive or dead.\nThe next generation of a cell is determined by the current state of the cell and the state of its two neighboring cells.\nThere are 8 possible configurations of neighboring states (3^2), and for each, Rule 30 defines whether the cell lives or dies in the next generation.\n\nFor Rule 30, the set of rules that governs the automaton’s next state is:\n\n\n\nConfiguration\nNext State\nBinary Code\n\n\n\n\n000\nDead\n000\n\n\n001\nAlive\n011\n\n\n010\nDead\n000\n\n\n011\nAlive\n011\n\n\n100\nAlive\n011\n\n\n101\nDead\n000\n\n\n110\nAlive\n011\n\n\n111\nDead\n000\n\n\n\nIn the following diagram, the top row shows the state of the central cell (cell i) and its two neighboring cells at a given stage, and the bottom row shows the state of the central cell in the next stage.\nFor example, in the first case of the figure:\n\nif the state of a cell at a given stage is [1] (black) and\nits two neighbors at that stage are also [1] (black),\nthen the cell’s state in the next stage will be [0] (white).\n\n Source\nIt is called Rule 30 because in binary, 000111102 = 30.\nLet’s break down the procedure for determining the next state in the 8 combinations:\nInput configurations: Consider the 8 possible input combinations of the three cells (a central cell and its left and right neighbors). Since each cell can be in one of two possible states (0 or 1), the combinations are 000, 001, 010, 011, 100, 101, 110, and 111.\nBinary representation of the rule: Rule 30 is represented by the number 30 in binary, which is 00011110. This binary representation determines the rules for the next state of the central cell for each of the 8 possible combinations.\nBit correspondence: The 8 bits of the binary representation (00011110) correspond to the 8 input combinations in order. From right to left, the bits represent the next state of the central cell for each input combination.\nFor example, the least significant bit of 00011110 (the rightmost bit) is 0. This means that when the input combination is 000, the next state of the central cell will be 0.\nNext state determination: For each input combination (e.g., 000, 001, 010, 011, 100, 101, 110, 111), the corresponding bit in the binary representation of Rule 30 [00011110] indicates the next state of the central cell.\n\n\n\nInput Configuration\nNext State\n\n\n\n\n000\n0\n\n\n001\n1\n\n\n010\n1\n\n\n011\n1\n\n\n100\n1\n\n\n101\n0\n\n\n110\n0\n\n\n111\n0\n\n\n\nFor example, if the input combination is 000 and the corresponding bit in Rule 30 is 0, then the next state of the central cell will be 0.\nTherefore, the rules are not arbitrary but are determined by the binary representation of Rule 30, which specifies the next state of the central cell for each input combination of its neighbors.\n\n\n3.5.3 Rule 30 Implementation in Python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef rule30(cells):\n    \"\"\"Applies Rule 30 to the input cells.\"\"\"\n    new_cells = np.zeros_like(cells)\n    extended_cells = np.concatenate(([cells[-1]], cells, [cells[0]]))  # Apply periodic boundary conditions\n    for i in range(1, len(extended_cells) - 1):\n        neighborhood = extended_cells[i-1:i+2]\n        if np.array_equal(neighborhood, [1, 1, 1]) or np.array_equal(neighborhood, [1, 1, 0]) or \\\n           np.array_equal(neighborhood, [1, 0, 1]) or np.array_equal(neighborhood, [0, 0, 0]):\n            new_cells[i-1] = 0\n        else:\n            new_cells[i-1] = 1\n    return new_cells\n\ndef main():\n    # Initialize the cells\n    cells = np.zeros(100)\n    cells[50] = 1  # Start with one cell in the middle\n\n    # Apply Rule 30 for 100 steps\n    history = [cells]\n    for i in range(100):\n        cells = rule30(cells)\n        history.append(cells)\n\n    # Display the history as an image\n    plt.imshow(history, cmap='binary')\n    plt.show()\n\nif __name__ == \"__main__\":\n    main()\nThis Python script uses the numpy and matplotlib libraries to simulate and visualize the evolution of a cellular automaton using Rule 30.\nThe rule30 function is the core of this script. It takes a one-dimensional array of cells as input, where each cell is either 0 or 1. The function first creates a new array new_cells with the same shape as the input, filled with zeros. It then extends the input array at both ends to apply periodic boundary conditions, meaning that the first cell is considered a neighbor of the last cell and vice versa.\nNext, the function iterates over each cell in the extended array (excluding the added boundary cells). For each cell, it considers the cell and its two neighbors as a neighborhood. If the neighborhood matches one of four specific patterns ([1, 1, 1], [1, 1, 0], [1, 0, 1], or [0, 0, 0]), the corresponding cell in new_cells is set to 0. Otherwise, it is set to 1. This is the implementation of Rule 30. Finally, the function returns the new array of cells.\nThe main function initializes a one-dimensional array of 100 cells, all set to 0 except the middle cell, which is set to 1. It then applies the rule30 function to this array 100 times, storing each resulting array in a list called history. This list is then visualized as a binary image using matplotlib’s imshow function, where each row corresponds to one step in the cellular automaton’s evolution.\nThe script is designed to run as a standalone program. The line if __name__ == \"__main__\": ensures that the main function is called only when the script is run directly, not when imported as a module.\n\n\n3.5.4 Two-Dimensional Cellular Automata (2DCA)\nTwo-dimensional cellular automata are an extension of one-dimensional cellular automata, where cells not only have neighbors to the left and right, but also above and below. This allows modeling of more complex and structurally rich systems, such as two-dimensional phenomena like wave propagation, growth patterns in biology, fire spread, fluid simulation, among others.\nTwo-dimensional cellular automata (2DCA) are computational models that simulate dynamic systems on a two-dimensional grid.\nThey consist of:\n\nCells: Each cell in the grid can have a finite state, such as alive or dead, and can change state according to the automaton’s rules.\nNeighborhood: Each cell has a neighborhood, which is a set of adjacent cells that influence its state. The neighborhood can be rectangular, hexagonal, circular, or of any other shape.\nTransition rule: The transition rule defines how the state of a cell changes based on its current state and the state of the cells in its neighborhood. The rule can be deterministic or probabilistic.\nEvolution: The cellular automaton evolves through iterations. In each iteration, the state of each cell is updated according to the transition rule.\n\nHere is the full translation of your markdown file into English, with all code, links, and formatting preserved:\n\n\n\n3.5.5 Case Study: Conway’s Game of Life\n\n\n\nGame of life\n\n\nSource\nConway’s Game of Life, also known simply as the Game of Life, is a cellular automaton devised by British mathematician John Horton Conway in 1970. It is the best-known example of a cellular automaton.\nThe “game” is actually a zero-player game, meaning its evolution is determined by its initial state, requiring no further input from human players. One interacts with the Game of Life by creating an initial configuration and observing how it evolves.\nView Original Article - MATHEMATICAL GAMES - The fantastic combinations of John Conway’s new solitaire game life (Martin Gardner)\nGOL (Game of Life) and CGOL (Conway’s Game of Life) are commonly used acronyms.\n\n\n\nGame of Life\n\n\nSource\n\n3.5.5.1 Rules\nThe universe of the Game of Life is an infinite two-dimensional orthogonal grid of square cells, each of which (at any given time) is in one of two possible states, alive (alternatively “on”) or dead (alternatively “off”). At each time step, the following transitions occur:\nThe four rules of Conway’s Game of Life:\n1. Overpopulation: Any live cell with more than three live neighbors dies due to overpopulation.\n2. Underpopulation: Any live cell with fewer than two live neighbors dies due to underpopulation.\n3. Stability: Any live cell with two or three live neighbors survives to the next generation.\n4. Reproduction: Any dead cell with exactly three live neighbors becomes a live cell.\n \nWe can also summarize the rules in a table:\n\n\n\n\n\n\n\n\n\n\nRule\nDescription\nCurrent State\nLive Neighbors\nNext State\n\n\n\n\nOverpopulation\nDeath by overcrowding\nAlive\nMore than 3\nDead\n\n\nUnderpopulation\nDeath by isolation\nAlive\nFewer than 2\nDead\n\n\nStability\nSurvival\nAlive\n2 or 3\nAlive\n\n\nReproduction\nBirth\nDead\n3\nAlive\n\n\n\n \nOr summarized as:\n0 → 3 live neighbors → 1 1 → &lt; 2 or &gt; 3 live neighbors → 0\nWhere 0 represents a dead cell and 1 a live cell.\n \n\n\n\nGame of life rules\n\n\nSource\n \n\n\n\nGame of life rules\n\n\nSource\nThe initial pattern constitutes the system’s ‘seed’. The first generation is created by applying the above rules simultaneously to each cell in the seed; births and deaths occur simultaneously, and the discrete moment in which this occurs is sometimes called a step. (In other words, each generation is a pure function of the previous one.) The rules continue to be applied repeatedly to create more generations.\n\n\n\n3.5.6 Origins\nConway was interested in a problem presented in the 1940s by renowned mathematician John von Neumann, who was trying to find a hypothetical machine that could build copies of itself and succeeded when he found a mathematical model for such a machine with very complicated rules on a rectangular grid. The Game of Life emerged as Conway’s successful attempt to simplify von Neumann’s ideas.\nThe game made its first public appearance in the October 1970 issue of Scientific American, in Martin Gardner’s “Mathematical Games” column, under the title “The fantastic combinations of John Conway’s new solitaire game ‘Life’”.\nSince its publication, Conway’s Game of Life has attracted significant interest due to the surprising ways patterns can evolve.\nLife is an example of emergence and self-organization. It is of interest to physicists, biologists, economists, mathematicians, philosophers, generative scientists, and others, as it shows how complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counterintuitive notion that “design” and “organization” can spontaneously arise in the absence of a designer.\nConway carefully selected the rules, after considerable experimentation, to meet three criteria:\n\nThere should be no initial pattern for which a simple proof exists that the population can grow without limit.\nThere must be initial patterns that appear to grow indefinitely.\nThere should be simple initial patterns that evolve and change for a considerable period before ending in one of the following ways:\n\ndying out completely (due to overcrowding or becoming too sparse); or\nsettling into a stable configuration that remains unchanged thereafter, or entering an oscillating phase in which they repeat a cycle endlessly of two or more periods.\n\n\n\n\n3.5.7 Patterns\nMany different types of patterns occur in the Game of Life, including static patterns (“still lifes”), repeating patterns (“oscillators” – a superset of still lifes), and patterns that move across the board (“spaceships”). Common examples of these three classes are shown below, with live cells in black and dead cells in white.\n\n\n\nGosper glider gun\n\n\nUsing the provided rules, you can investigate the evolution of simple patterns:\n\n\n\n3 cells\n\n\n\n\n\n4 cells\n\n\nSource\nPatterns that evolve for long periods before stabilizing are called Methuselahs, the first of which discovered was the R-pentomino.\n\n\n\nR-pendomino\n\n\nDiehard is a pattern that eventually disappears, rather than stabilizing, after 130 generations, which is believed to be the maximum for initial patterns with seven or fewer cells.\n\n\n\nDiehard\n\n\nAcorn takes 5,206 generations to produce 633 cells, including 13 escaping gliders.\n\n\n\nAcorn\n\n\nConway originally conjectured that no pattern could grow indefinitely; that is, for any initial configuration with a finite number of live cells, the population could not grow beyond some finite upper bound. The Gosper glider gun pattern produces its first glider in the 15th generation, and another every 30 generations thereafter.\n\n\n\nGosper’s glider gun\n\n\n\n\n\nGosper glider gun\n\n\nFor many years, this pattern was the smallest known. In 2015, a gun called Simkin glider gun was discovered, which emits a glider every 120 generations, and has fewer live cells but is spread across a larger bounding box at its ends.\n\n\n\nSimkin glider gun\n\n\nSource \nSource\n\n\n3.5.8 Python Implementation of Game of Life\nimport time\nimport pygame\nimport numpy as np\n\nCOLOR_BG = (10, 10, 10,)  # Color de fondo\nCOLOR_GRID = (40, 40, 40)  # Color de la cuadrícula\nCOLOR_DIE_NEXT = (170, 170, 170)  # Color de las células que mueren en la siguiente generación\nCOLOR_ALIVE_NEXT = (255, 255, 255)  # Color de las células que siguen vivas en la siguiente generación\n\npygame.init()\npygame.display.set_caption(\"conway's game of life\")  # Título de la ventana del juego\n\n# Función para actualizar la pantalla con las células\ndef update(screen, cells, size, with_progress=False):\n    updated_cells = np.zeros((cells.shape[0], cells.shape[1]))  # Matriz para almacenar las células actualizadas\n\n    for row, col in np.ndindex(cells.shape):\n        alive = np.sum(cells[row-1:row+2, col-1:col+2]) - cells[row, col]  # Cálculo de células vecinas vivas\n        color = COLOR_BG if cells[row, col] == 0 else COLOR_ALIVE_NEXT  # Color de la célula actual\n\n        if cells[row, col] == 1:  # Si la célula actual está viva\n            if alive &lt; 2 or alive &gt; 3:  # Si tiene menos de 2 o más de 3 vecinos vivos, muere\n                if with_progress:\n                    color = COLOR_DIE_NEXT\n            elif 2 &lt;= alive &lt;= 3:  # Si tiene 2 o 3 vecinos vivos, sigue viva\n                updated_cells[row, col] = 1\n                if with_progress:\n                    color = COLOR_ALIVE_NEXT\n        else:  # Si la célula actual está muerta\n            if alive == 3:  # Si tiene exactamente 3 vecinos vivos, revive\n                updated_cells[row, col] = 1\n                if with_progress:\n                    color = COLOR_ALIVE_NEXT\n\n        pygame.draw.rect(screen, color, (col * size, row * size, size - 1, size - 1))  # Dibuja la célula en la pantalla\n\n    return updated_cells  # Devuelve las células actualizadas\n\n# Función principal del programa\ndef main():\n    pygame.init()\n    screen = pygame.display.set_mode((800, 600))  # Crea la ventana del juego\n\n    cells = np.zeros((60, 80))  # Crea una matriz de células muertas\n    screen.fill(COLOR_GRID)  # Rellena la pantalla con el color de la cuadrícula\n    update(screen, cells, 10)  # Actualiza la pantalla con las células\n\n    pygame.display.flip()\n    pygame.display.update()\n\n    running = False  # Variable para controlar si el juego está en ejecución\n\n    while True:\n        for Q in pygame.event.get():\n            if Q.type == pygame.QUIT:  # Si se cierra la ventana, termina el programa\n                pygame.quit()\n                return\n            elif Q.type == pygame.KEYDOWN:\n                if Q.key == pygame.K_SPACE:  # Si se presiona la tecla espacio, se inicia o pausa el juego\n                    running = not running\n                    update(screen, cells, 10)\n                    pygame.display.update()\n            if pygame.mouse.get_pressed()[0]:  # Si se presiona el botón izquierdo del ratón\n                pos = pygame.mouse.get_pos()  # Obtiene la posición del ratón\n                cells[pos[1] // 10, pos[0] // 10] = 1  # Marca la célula correspondiente como viva\n                update(screen, cells, 10)\n                pygame.display.update()\n\n        screen.fill(COLOR_GRID)  # Rellena la pantalla con el color de la cuadrícula\n\n        if running:  # Si el juego está en ejecución\n            cells = update(screen, cells, 10, with_progress=True)  # Actualiza las células con progreso\n            pygame.display.update()\n\n        time.sleep(0.001)  # Espera un breve tiempo para controlar la velocidad del juego\n\nif __name__ == \"__main__\":\n    main()\n\nThis Python script implements Conway’s Game of Life, a cellular automaton devised by British mathematician John Horton Conway. It is a zero-player game, meaning its evolution is determined entirely by its initial state, with no further input required.\nThe script begins by importing the necessary modules: time, pygame for the graphical interface, and numpy to handle the game grid as a 2D matrix. It then defines color constants used for visualizing the game.\nThe pygame.init() function is called to initialize all imported Pygame modules. The window title is set to “Conway’s Game of Life” using pygame.display.set_caption().\nThe update() function updates the game state and redraws the grid. It takes four arguments: screen (the Pygame surface to draw on), cells (the current game state as a 2D numpy array), size (the pixel size of each cell), and with_progress (a boolean indicating whether to display cells that will change in the next generation).\nThe function creates a new 2D matrix updated_cells filled with zeros, matching the shape of cells. It then iterates over each cell, calculates the number of live neighbors, and applies the game rules to determine whether the cell will be alive in the next generation. The function draws each cell on the screen using the appropriate color and returns updated_cells.\nThe main() function initializes Pygame, creates the game window, and initializes the game state as a 2D numpy array of zeros (representing dead cells). It then enters the main loop, which handles Pygame events (such as closing the window or key presses), updates the game state if it is running, and redraws the grid. The game can be started or paused by pressing the spacebar, and cells can be toggled manually by clicking on them.\nFinally, the script calls main() to launch the game. Press the spacebar to begin.\n\n\n3.5.9 Further Reading\n\nThe Game Of Life – Emergence In Generative Art (2020)\nConway’s Game of Life – Life on Computer by Swaraj Kalbande\nWikipedia – Conway’s Game of Life\n\n\n\n3.5.10 Interactive Websites\n\nJohn Conway’s Game of Life – An Introduction to Cellular Automata\nconwaylife.com\nCellular Automata Megathread",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequencers</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html",
    "href": "chapters/conclusion.html",
    "title": "7  Conclusion",
    "section": "",
    "text": "7.1 Key Takeaways\nIn this chapter, we summarize the key points discussed throughout the book. We reflect on the main topics introduced in the first chapter and the detailed discussions in the second chapter.\nThank you for reading! We hope this book has provided valuable insights and knowledge.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html#key-takeaways",
    "href": "chapters/conclusion.html#key-takeaways",
    "title": "7  Conclusion",
    "section": "",
    "text": "Highlight the main objectives of the book.\nDiscuss the significance of the topics covered.\nEncourage further exploration and learning in the subject area.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Creative Coding with Pure Data",
    "section": "Contributing",
    "text": "Contributing\nIf you would like to contribute to this book, please feel free to fork the repository and submit a pull request. I welcome any suggestions, corrections, or improvements to the content. You can also report issues or request features by opening an issue in the repository. You can find the source code for this book on GitHub.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#recommended-pure-data-distributions",
    "href": "index.html#recommended-pure-data-distributions",
    "title": "Creative Coding with Pure Data",
    "section": "Recommended Pure Data distributions",
    "text": "Recommended Pure Data distributions\nThis book is based on Pure Data Vanilla distribution, which is the most widely used version. You can download it from the official website:\nhttps://puredata.info/downloads/pure-data\nThis version is maintained by the original author, Miller Puckette, and is the most stable supported version of Pure Data. It includes all the core features and libraries needed for most projects, making it a great starting point for beginners and experienced users alike. Pure Data Vanilla is a free and open-source visual programming language for multimedia applications, widely used in the fields of sound synthesis, interactive installations, and live performance. It is available for Windows, macOS, and Linux operating systems.\nHowever, there are several other distributions that you may find useful for specific projects or needs:\n\nPurr Data – A fork of Pd-l2ork that focuses on usability and accessibility, with a more polished interface and additional features. Purr Data serves the same purpose, but offers a new and much improved graphical user interface and includes many 3rd party plug-ins. Like Pd, it runs on Linux, macOS and Windows, and is open-source throughout.\nPlugdata – plugdata is a plugin wrapper designed for Miller Puckette’s Pure Data (Pd), featuring an enhanced graphical user interface (GUI) created using JUCE, headed by Timothy Schoen. this project is still a work in progress, and may still have some bugs. By default, plugdata comes with the cyclone and ELSE collections of externals and abstractions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-following-libraries-are-recommended-for-use-with-pure-data",
    "href": "index.html#the-following-libraries-are-recommended-for-use-with-pure-data",
    "title": "Creative Coding with Pure Data",
    "section": "The following libraries are recommended for use with Pure Data:",
    "text": "The following libraries are recommended for use with Pure Data:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/introduction.html#for-an-in-depth-exploration-of-creative-coding-consider-checking-out-these-resources",
    "href": "chapters/introduction.html#for-an-in-depth-exploration-of-creative-coding-consider-checking-out-these-resources",
    "title": "1  Introduction",
    "section": "1.4 For an in-depth exploration of creative coding, consider checking out these resources:",
    "text": "1.4 For an in-depth exploration of creative coding, consider checking out these resources:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#references",
    "href": "chapters/playrec.html#references",
    "title": "2  Rec & Play",
    "section": "References",
    "text": "References\n\n\n\n\nHasse, Sarah. 2012. “I Am Sitting in a Room.” Body, Space & Technology 11. https://doi.org/10.16995/bst.71.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#references.",
    "href": "chapters/playrec.html#references.",
    "title": "2  Rec & Play",
    "section": "2.2 References{.}",
    "text": "2.2 References{.}\n\n\n\n\nHasse, Sarah. 2012. “I Am Sitting in a Room.” Body, Space & Technology 11. https://doi.org/10.16995/bst.71.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#i-am-sitting-in-a-room-freeverb-patch-walkthrough",
    "href": "chapters/playrec.html#i-am-sitting-in-a-room-freeverb-patch-walkthrough",
    "title": "2  Rec & Play",
    "section": "2.2 I am sitting in a room (freeverb) — Patch Walkthrough",
    "text": "2.2 I am sitting in a room (freeverb) — Patch Walkthrough\nThis Pure Data patch extends the original I am sitting in a room concept by introducing a virtual acoustic environment using reverb and allowing for pre-recorded audio input instead of just live microphone input. This adaptation provides more creative flexibility and consistent results compared to using a physical room.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#conceptual-overview-1",
    "href": "chapters/playrec.html#conceptual-overview-1",
    "title": "2  Rec & Play",
    "section": "2.3 Conceptual Overview",
    "text": "2.3 Conceptual Overview\nThe patch enables you to: - Use a pre-recorded audio file OR live input as your source material - Add artificial reverb to simulate room characteristics - Follow the same recursive recording process as the original - Achieve more controlled, repeatable results",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#system-diagram-1",
    "href": "chapters/playrec.html#system-diagram-1",
    "title": "2  Rec & Play",
    "section": "2.3 System Diagram",
    "text": "2.3 System Diagram\n\n\n\n\n\nflowchart TD\n    AudioFile[Audio File&lt;br&gt;readsf~] --&gt; Reverb1[freeverb~] --&gt; AudioInput[s~ audio.input]\n    AudioInput --&gt; RecordA[Record A&lt;br&gt;writesf~ record-A.wav]\n    RecordA --&gt; PlayA[Play A&lt;br&gt;readsf~ record-A.wav]\n    PlayA --&gt; Reverb2[freeverb~] \n    PlayA --&gt; PlayerA[s~ player.A]\n    Reverb2 --&gt; ThrowAudio1[throw~ audio]\n    PlayerA --&gt; RecordB[Record B&lt;br&gt;writesf~ record-B.wav]\n    RecordB --&gt; PlayB[Play B&lt;br&gt;readsf~ record-B.wav]\n    PlayB --&gt; Reverb3[freeverb~]\n    PlayB --&gt; PlayerB[s~ player.B]\n    Reverb3 --&gt; ThrowAudio2[throw~ audio]\n    PlayerB --&gt; RecordA\n    ThrowAudio1 --&gt; CatchAudio[catch~ audio]\n    ThrowAudio2 --&gt; CatchAudio\n    CatchAudio --&gt; Output[out~]\n    \n    classDef reverb fill:#e1f5fe,stroke:#0277bd\n    classDef files fill:#e8f5e9,stroke:#2e7d32\n    classDef audio fill:#fff3e0,stroke:#e65100\n    \n    class Reverb1,Reverb2,Reverb3 reverb\n    class AudioFile,RecordA,PlayA,RecordB,PlayB files\n    class AudioInput,PlayerA,PlayerB,ThrowAudio1,ThrowAudio2,CatchAudio audio\n\n\n\n\n\n\n\n\n2.3.1 Unique Features of the [freeverb~] Version\n\n2.3.1.1 1. Audio File Input\nUnlike the original patch that only uses microphone input, this version can:\n\nPlay a pre-recorded audio file (speech.wav) as the initial source\nProcess this file through reverb before recording\nTrigger playback with a button (labeled “2. Playback audio”)\n\n\n\n\n\n\nflowchart LR\n    Bang((bng)) --&gt; Msg[/msg open speech.wav, 1/]\n    Msg --&gt; Reader[readsf~]\n    Reader --&gt; Reverb[freeverb~]\n    Reverb --&gt; Send[s~ audio.input]\n    \n    style Bang fill:#f9f,stroke:#333,stroke-width:2px\n    style Msg fill:#fff,stroke:#333\n    style Reader fill:#e8f5e9,stroke:#333\n    style Reverb fill:#e1f5fe,stroke:#333\n    style Send fill:#fff3e0,stroke:#333\n\n\n\n\n\n\n\n\n2.3.1.2 2. Virtual Acoustic Environment\nInstead of relying on a physical room’s acoustics, this patch uses freeverb~ objects to create a simulated acoustic space:\n\nEvery audio signal (input and playback) passes through a freeverb~ object\nThis creates consistent reverberation that can be adjusted and controlled\nMultiple freeverb~ objects process different stages of the audio for cumulative effects\n\n\n\n2.3.1.3 3. Enhanced Control Flow\nThe patch includes numbered controls for better workflow:\n\n\n\n\n\n\n\n\nButton\nLabel\nFunction\n\n\n\n\n1\nStart recording\nBegins recording the input source to record-A.wav\n\n\n2\nPlayback audio\nPlays the speech.wav file through reverb into the system\n\n\n3\nStop recording\nStops the current recording process\n\n\n\n\n\n2.3.1.4 4. Multiple Send/Receive Paths\nThe patch uses additional send/receive pairs to manage signal routing:\n\ns~/r~ audio.input - Routes input signals to the recording object\ns~/r~ player.A - Routes playback from file A to recorder B\ns~/r~ player.B - Routes playback from file B back to recorder A\nthrow~/catch~ audio - Collects all signals to be sent to the output\n\n\n\n\n2.3.2 Step-by-Step Explanation\n\n2.3.2.1 Step 1: Initial Audio Source Options\nThis patch provides two possible sources for the initial recording:\n\nPre-recorded file input:\n\nClick “2. Playback audio” button to play speech.wav\nThe file is played through readsf~, processed by freeverb~, and sent to s~ audio.input\n\nLive input: (not explicitly shown in this version but could be added using adc~)\n\nAny signal sent to s~ audio.input will be recorded when recording starts\n\n\n\n\n2.3.2.2 Step 2: Recording the First Generation\n\nClick “1. Start recording” to begin recording to record-A.wav\nThe signal from s~ audio.input is captured by r~ audio.input and sent to writesf~\nThe resulting file contains your source material with initial reverb applied\n\n\n\n2.3.2.3 Step 3: Playback and Re-recording\n\nAfter stopping recording with “3. Stop recording”, the patch automatically:\n\nOpens and plays record-A.wav through readsf~\nProcesses it through another freeverb~ for additional reverb\nSends it to both throw~ audio (for monitoring) and s~ player.A\nThe player.A signal is received and recorded to record-B.wav\n\n\n\n\n2.3.2.4 Step 4: Recursive Process\n\nWhen record-B.wav is created, it can be played back through another readsf~\nThis signal is also processed by freeverb~\nThe output is sent to both throw~ audio and s~ player.B\nThe player.B signal could be received and recorded back to record-A.wav\nThis cycle can continue, with each generation accumulating more of the virtual room’s characteristics\n\n\n\n2.3.2.5 Step 5: Output\n\nAll audio signals sent to throw~ audio are collected by catch~ audio\nThe mixed signal is sent to out~ for playback through speakers/headphones\n\n\n\n\n\n2.3.3 Key Objects Table\n\n\n\n\n\n\n\nObject\nPurpose in This Patch\n\n\n\n\nfreeverb~\nCreates virtual room acoustics by adding reverberation\n\n\nreadsf~\nPlays audio files (source material or previous generations)\n\n\nwritesf~\nRecords audio to file (for each generation)\n\n\ns~/r~\nRoutes audio between different parts of the patch\n\n\nthrow~/catch~\nMixes and outputs all audio signals\n\n\ndel\nAdds small delays to ensure proper sequencing of operations\n\n\nbng\nProvides buttons for user interaction\n\n\nout~\nSends audio to speakers/headphones\n\n\n\n\n\n2.3.4 Creative Applications\n\nExperiment with reverb settings: Try different room sizes, damping, and wet/dry mix\nUse different source materials: Test various speech samples, musical phrases, or sound effects\nCreate hybrid processes: Record the first generation in a real room, then switch to the virtual environment\nBuild compositional sequences: Layer different generations to create evolving textures\nCompare real vs. virtual: Process the same source through both the original and freeverb patches to contrast physical and virtual acoustics\n\nThis virtual approach offers a controlled laboratory for exploring the core concepts of Lucier’s piece, making it accessible even without an ideal acoustic space, while also opening new creative possibilities that wouldn’t be possible in a physical implementation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#unique-features-of-the-freeverb-version",
    "href": "chapters/playrec.html#unique-features-of-the-freeverb-version",
    "title": "2  Rec & Play",
    "section": "2.4 Unique Features of the Freeverb Version",
    "text": "2.4 Unique Features of the Freeverb Version\n\n2.4.1 1. Audio File Input\nUnlike the original patch that only uses microphone input, this version can:\n\nPlay a pre-recorded audio file (speech.wav) as the initial source\nProcess this file through reverb before recording\nTrigger playback with a button (labeled “2. Playback audio”)\n\n[bng] ---&gt; [msg open speech.wav, 1] ---&gt; [readsf~] ---&gt; [freeverb~] ---&gt; [s~ audio.input]\n\n\n2.4.2 2. Virtual Acoustic Environment\nInstead of relying on a physical room’s acoustics, this patch uses freeverb~ objects to create a simulated acoustic space:\n\nEvery audio signal (input and playback) passes through a freeverb~ object\nThis creates consistent reverberation that can be adjusted and controlled\nMultiple freeverb~ objects process different stages of the audio for cumulative effects\n\n\n\n2.4.3 3. Enhanced Control Flow\nThe patch includes numbered controls for better workflow:\n\n\n\n\n\n\n\n\nButton\nLabel\nFunction\n\n\n\n\n1\nStart recording\nBegins recording the input source to record-A.wav\n\n\n2\nPlayback audio\nPlays the speech.wav file through reverb into the system\n\n\n3\nStop recording\nStops the current recording process\n\n\n\n\n\n2.4.4 4. Multiple Send/Receive Paths\nThe patch uses additional send/receive pairs to manage signal routing:\n\ns~/r~ audio.input - Routes input signals to the recording object\ns~/r~ player.A - Routes playback from file A to recorder B\ns~/r~ player.B - Routes playback from file B back to recorder A\nthrow~/catch~ audio - Collects all signals to be sent to the output",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#step-by-step-explanation",
    "href": "chapters/playrec.html#step-by-step-explanation",
    "title": "2  Rec & Play",
    "section": "2.5 Step-by-Step Explanation",
    "text": "2.5 Step-by-Step Explanation\n\n2.5.1 Step 1: Initial Audio Source Options\nThis patch provides two possible sources for the initial recording:\n\nPre-recorded file input:\n\nClick “2. Playback audio” button to play speech.wav\nThe file is played through readsf~, processed by freeverb~, and sent to s~ audio.input\n\nLive input: (not explicitly shown in this version but could be added using adc~)\n\nAny signal sent to s~ audio.input will be recorded when recording starts\n\n\n\n\n2.5.2 Step 2: Recording the First Generation\n\nClick “1. Start recording” to begin recording to record-A.wav\nThe signal from s~ audio.input is captured by r~ audio.input and sent to writesf~\nThe resulting file contains your source material with initial reverb applied\n\n\n\n2.5.3 Step 3: Playback and Re-recording\n\nAfter stopping recording with “3. Stop recording”, the patch automatically:\n\nOpens and plays record-A.wav through readsf~\nProcesses it through another freeverb~ for additional reverb\nSends it to both throw~ audio (for monitoring) and s~ player.A\nThe player.A signal is received and recorded to record-B.wav\n\n\n\n\n2.5.4 Step 4: Recursive Process\n\nWhen record-B.wav is created, it can be played back through another readsf~\nThis signal is also processed by freeverb~\nThe output is sent to both throw~ audio and s~ player.B\nThe player.B signal could be received and recorded back to record-A.wav\nThis cycle can continue, with each generation accumulating more of the virtual room’s characteristics\n\n\n\n2.5.5 Step 5: Output\n\nAll audio signals sent to throw~ audio are collected by catch~ audio\nThe mixed signal is sent to out~ for playback through speakers/headphones",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#key-objects-table-1",
    "href": "chapters/playrec.html#key-objects-table-1",
    "title": "2  Rec & Play",
    "section": "2.4 Key Objects Table",
    "text": "2.4 Key Objects Table\n\n\n\n\n\n\n\nObject\nPurpose in This Patch\n\n\n\n\nfreeverb~\nCreates virtual room acoustics by adding reverberation\n\n\nreadsf~\nPlays audio files (source material or previous generations)\n\n\nwritesf~\nRecords audio to file (for each generation)\n\n\ns~/r~\nRoutes audio between different parts of the patch\n\n\nthrow~/catch~\nMixes and outputs all audio signals\n\n\ndel\nAdds small delays to ensure proper sequencing of operations\n\n\nbng\nProvides buttons for user interaction\n\n\nout~\nSends audio to speakers/headphones",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#creative-applications",
    "href": "chapters/playrec.html#creative-applications",
    "title": "2  Rec & Play",
    "section": "2.4 Creative Applications",
    "text": "2.4 Creative Applications\n\nExperiment with reverb settings: Try different room sizes, damping, and wet/dry mix\nUse different source materials: Test various speech samples, musical phrases, or sound effects\nCreate hybrid processes: Record the first generation in a real room, then switch to the virtual environment\nBuild compositional sequences: Layer different generations to create evolving textures\nCompare real vs. virtual: Process the same source through both the original and freeverb patches to contrast physical and virtual acoustics\n\n\nThis virtual approach offers a controlled laboratory for exploring the core concepts of Lucier’s piece, making it accessible even without an ideal acoustic space, while also opening new creative possibilities that wouldn’t be possible in a physical implementation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#i-am-sitting-in-a-room-patch-walkthrough",
    "href": "chapters/playrec.html#i-am-sitting-in-a-room-patch-walkthrough",
    "title": "2  Rec & Play",
    "section": "2.2 I am sitting in a room — Patch Walkthrough",
    "text": "2.2 I am sitting in a room — Patch Walkthrough\nThis Pure Data patch extends the original I am sitting in a room concept by introducing a virtual acoustic environment using reverb and allowing for pre-recorded audio input instead of just live microphone input. This adaptation provides more creative flexibility and consistent results compared to using a physical room.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#i-am-sitting-in-a-room-patch-walkthrough-with",
    "href": "chapters/playrec.html#i-am-sitting-in-a-room-patch-walkthrough-with",
    "title": "2  Rec & Play",
    "section": "2.2 I am sitting in a room — Patch Walkthrough with []",
    "text": "2.2 I am sitting in a room — Patch Walkthrough with []\nThis Pure Data patch extends the original I am sitting in a room concept by introducing a virtual acoustic environment using reverb and allowing for pre-recorded audio input instead of just live microphone input. This adaptation provides more creative flexibility and consistent results compared to using a physical room.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#i-am-sitting-in-a-room-patch-walkthrough-with-freeverb",
    "href": "chapters/playrec.html#i-am-sitting-in-a-room-patch-walkthrough-with-freeverb",
    "title": "2  Rec & Play",
    "section": "2.2 I am sitting in a room — Patch Walkthrough with [freeverb~]",
    "text": "2.2 I am sitting in a room — Patch Walkthrough with [freeverb~]\nThe patch I-am-sitting-in-a-room-freeverb.pd extends the original I am sitting in a room concept by introducing a virtual acoustic environment using reverb and allowing for pre-recorded audio input instead of just live microphone input. This adaptation provides more creative flexibility and consistent results compared to using a physical room.\n\n\n\nPure Data implementation of I am sitting in a room with freeverb\n\n\nThis patch is designed to simulate the recursive recording process of Lucier’s piece while allowing for more control over the sound environment. It uses the freeverb~ object to create a virtual room effect, enabling you to manipulate the acoustic characteristics of the sound without relying on a physical space.\n\n2.2.1 Conceptual Overview\nThe patch enables you to: - Use a pre-recorded audio file OR live input as your source material - Add artificial reverb to simulate room characteristics - Follow the same recursive recording process as the original - Achieve more controlled, repeatable results\n\n\n2.2.2 System Diagram\n\n\n\n\n\nflowchart TD\n    AudioFile[Audio File&lt;br&gt;readsf~] --&gt; Reverb1[freeverb~] --&gt; AudioInput[s~ audio.input]\n    AudioInput --&gt; RecordA[Record A&lt;br&gt;writesf~ record-A.wav]\n    RecordA --&gt; PlayA[Play A&lt;br&gt;readsf~ record-A.wav]\n    PlayA --&gt; Reverb2[freeverb~] \n    PlayA --&gt; PlayerA[s~ player.A]\n    Reverb2 --&gt; ThrowAudio1[throw~ audio]\n    PlayerA --&gt; RecordB[Record B&lt;br&gt;writesf~ record-B.wav]\n    RecordB --&gt; PlayB[Play B&lt;br&gt;readsf~ record-B.wav]\n    PlayB --&gt; Reverb3[freeverb~]\n    PlayB --&gt; PlayerB[s~ player.B]\n    Reverb3 --&gt; ThrowAudio2[throw~ audio]\n    PlayerB --&gt; RecordA\n    ThrowAudio1 --&gt; CatchAudio[catch~ audio]\n    ThrowAudio2 --&gt; CatchAudio\n    CatchAudio --&gt; Output[out~]\n    \n    classDef reverb fill:#e1f5fe,stroke:#0277bd\n    classDef files fill:#e8f5e9,stroke:#2e7d32\n    classDef audio fill:#fff3e0,stroke:#e65100\n    \n    class Reverb1,Reverb2,Reverb3 reverb\n    class AudioFile,RecordA,PlayA,RecordB,PlayB files\n    class AudioInput,PlayerA,PlayerB,ThrowAudio1,ThrowAudio2,CatchAudio audio\n\n\n\n\n\n\n\n\n\n2.2.3 Unique Features of the [freeverb~] Version\n\n2.2.3.1 1. Audio File Input\nUnlike the original patch that only uses microphone input, this version can:\n\nPlay a pre-recorded audio file (speech.wav) as the initial source\nProcess this file through reverb before recording\nTrigger playback with a button (labeled “2. Playback audio”)\n\n\n\n\n\n\nflowchart LR\n    Bang((bng)) --&gt; Msg[/msg open speech.wav, 1/]\n    Msg --&gt; Reader[readsf~]\n    Reader --&gt; Reverb[freeverb~]\n    Reverb --&gt; Send[s~ audio.input]\n    \n    style Bang fill:#f9f,stroke:#333,stroke-width:2px\n    style Msg fill:#fff,stroke:#333\n    style Reader fill:#e8f5e9,stroke:#333\n    style Reverb fill:#e1f5fe,stroke:#333\n    style Send fill:#fff3e0,stroke:#333\n\n\n\n\n\n\n\n\n2.2.3.2 2. Virtual Acoustic Environment\nInstead of relying on a physical room’s acoustics, this patch uses freeverb~ objects to create a simulated acoustic space:\n\nEvery audio signal (input and playback) passes through a freeverb~ object\nThis creates consistent reverberation that can be adjusted and controlled\nMultiple freeverb~ objects process different stages of the audio for cumulative effects\n\n\n\n2.2.3.3 3. Enhanced Control Flow\nThe patch includes numbered controls for better workflow:\n\n\n\n\n\n\n\n\nButton\nLabel\nFunction\n\n\n\n\n1\nStart recording\nBegins recording the input source to record-A.wav\n\n\n2\nPlayback audio\nPlays the speech.wav file through reverb into the system\n\n\n3\nStop recording\nStops the current recording process\n\n\n\n\n\n2.2.3.4 4. Multiple Send/Receive Paths\nThe patch uses additional send/receive pairs to manage signal routing:\n\ns~/r~ audio.input - Routes input signals to the recording object\ns~/r~ player.A - Routes playback from file A to recorder B\ns~/r~ player.B - Routes playback from file B back to recorder A\nthrow~/catch~ audio - Collects all signals to be sent to the output\n\n\n\n\n2.2.4 Step-by-Step Explanation\n\n2.2.4.1 Step 1: Initial Audio Source Options\nThis patch provides two possible sources for the initial recording:\n\nPre-recorded file input:\n\nClick “2. Playback audio” button to play speech.wav\nThe file is played through readsf~, processed by freeverb~, and sent to s~ audio.input\n\nLive input: (not explicitly shown in this version but could be added using adc~)\n\nAny signal sent to s~ audio.input will be recorded when recording starts\n\n\n\n\n2.2.4.2 Step 2: Recording the First Generation\n\nClick “1. Start recording” to begin recording to record-A.wav\nThe signal from s~ audio.input is captured by r~ audio.input and sent to writesf~\nThe resulting file contains your source material with initial reverb applied\n\n\n\n2.2.4.3 Step 3: Playback and Re-recording\n\nAfter stopping recording with “3. Stop recording”, the patch automatically:\n\nOpens and plays record-A.wav through readsf~\nProcesses it through another freeverb~ for additional reverb\nSends it to both throw~ audio (for monitoring) and s~ player.A\nThe player.A signal is received and recorded to record-B.wav\n\n\n\n\n2.2.4.4 Step 4: Recursive Process\n\nWhen record-B.wav is created, it can be played back through another readsf~\nThis signal is also processed by freeverb~\nThe output is sent to both throw~ audio and s~ player.B\nThe player.B signal could be received and recorded back to record-A.wav\nThis cycle can continue, with each generation accumulating more of the virtual room’s characteristics\n\n\n\n2.2.4.5 Step 5: Output\n\nAll audio signals sent to throw~ audio are collected by catch~ audio\nThe mixed signal is sent to out~ for playback through speakers/headphones\n\n\n\n\n\n2.2.5 Key Objects Table\n\n\n\n\n\n\n\nObject\nPurpose in This Patch\n\n\n\n\nfreeverb~\nCreates virtual room acoustics by adding reverberation\n\n\nreadsf~\nPlays audio files (source material or previous generations)\n\n\nwritesf~\nRecords audio to file (for each generation)\n\n\ns~/r~\nRoutes audio between different parts of the patch\n\n\nthrow~/catch~\nMixes and outputs all audio signals\n\n\ndel\nAdds small delays to ensure proper sequencing of operations\n\n\nbng\nProvides buttons for user interaction\n\n\nout~\nSends audio to speakers/headphones\n\n\n\n\n\n2.2.6 Creative Applications\n\nExperiment with reverb settings: Try different room sizes, damping, and wet/dry mix\nUse different source materials: Test various speech samples, musical phrases, or sound effects\nCreate hybrid processes: Record the first generation in a real room, then switch to the virtual environment\nBuild compositional sequences: Layer different generations to create evolving textures\nCompare real vs. virtual: Process the same source through both the original and freeverb patches to contrast physical and virtual acoustics\n\nThis virtual approach offers a controlled laboratory for exploring the core concepts of Lucier’s piece, making it accessible even without an ideal acoustic space, while also opening new creative possibilities that wouldn’t be possible in a physical implementation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#i-am-sitting-in-a-freeverb-patch-walkthrough-with-freeverb",
    "href": "chapters/playrec.html#i-am-sitting-in-a-freeverb-patch-walkthrough-with-freeverb",
    "title": "2  Rec & Play",
    "section": "2.2 I am sitting in a freeverb — Patch Walkthrough with [freeverb~]",
    "text": "2.2 I am sitting in a freeverb — Patch Walkthrough with [freeverb~]\nThe patch I-am-sitting-in-a-room-freeverb.pd extends the original I am sitting in a room concept by introducing a virtual acoustic environment using reverb and allowing for pre-recorded audio input instead of just live microphone input. This adaptation provides more creative flexibility and consistent results compared to using a physical room.\n\n\n\nPure Data implementation of I am sitting in a room with freeverb\n\n\nThis patch is designed to simulate the recursive recording process of Lucier’s piece while allowing for more control over the sound environment. It uses the freeverb~ object to create a virtual room effect, enabling you to manipulate the acoustic characteristics of the sound without relying on a physical space.\n\n2.2.1 Conceptual Overview\nThe patch enables you to: - Use a pre-recorded audio file OR live input as your source material - Add artificial reverb to simulate room characteristics - Follow the same recursive recording process as the original - Achieve more controlled, repeatable results\n\n\n2.2.2 System Diagram\n\n\n\n\n\nflowchart TD\n    AudioFile[Audio File&lt;br&gt;readsf~] --&gt; Reverb1[freeverb~] --&gt; AudioInput[s~ audio.input]\n    AudioInput --&gt; RecordA[Record A&lt;br&gt;writesf~ record-A.wav]\n    RecordA --&gt; PlayA[Play A&lt;br&gt;readsf~ record-A.wav]\n    PlayA --&gt; Reverb2[freeverb~] \n    PlayA --&gt; PlayerA[s~ player.A]\n    Reverb2 --&gt; ThrowAudio1[throw~ audio]\n    PlayerA --&gt; RecordB[Record B&lt;br&gt;writesf~ record-B.wav]\n    RecordB --&gt; PlayB[Play B&lt;br&gt;readsf~ record-B.wav]\n    PlayB --&gt; Reverb3[freeverb~]\n    PlayB --&gt; PlayerB[s~ player.B]\n    Reverb3 --&gt; ThrowAudio2[throw~ audio]\n    PlayerB --&gt; RecordA\n    ThrowAudio1 --&gt; CatchAudio[catch~ audio]\n    ThrowAudio2 --&gt; CatchAudio\n    CatchAudio --&gt; Output[out~]\n    \n    classDef reverb fill:#e1f5fe,stroke:#0277bd\n    classDef files fill:#e8f5e9,stroke:#2e7d32\n    classDef audio fill:#fff3e0,stroke:#e65100\n    \n    class Reverb1,Reverb2,Reverb3 reverb\n    class AudioFile,RecordA,PlayA,RecordB,PlayB files\n    class AudioInput,PlayerA,PlayerB,ThrowAudio1,ThrowAudio2,CatchAudio audio\n\n\n\n\n\n\n\n\n\n2.2.3 Unique Features of the [freeverb~] Version\n\n2.2.3.1 1. Audio File Input\nUnlike the original patch that only uses microphone input, this version can:\n\nPlay a pre-recorded audio file (speech.wav) as the initial source\nProcess this file through reverb before recording\nTrigger playback with a button (labeled “2. Playback audio”)\n\n\n\n\n\n\nflowchart LR\n    Bang((bng)) --&gt; Msg[/msg open speech.wav, 1/]\n    Msg --&gt; Reader[readsf~]\n    Reader --&gt; Reverb[freeverb~]\n    Reverb --&gt; Send[s~ audio.input]\n    \n    style Bang fill:#f9f,stroke:#333,stroke-width:2px\n    style Msg fill:#fff,stroke:#333\n    style Reader fill:#e8f5e9,stroke:#333\n    style Reverb fill:#e1f5fe,stroke:#333\n    style Send fill:#fff3e0,stroke:#333\n\n\n\n\n\n\n\n\n2.2.3.2 2. Virtual Acoustic Environment\nInstead of relying on a physical room’s acoustics, this patch uses freeverb~ objects to create a simulated acoustic space:\n\nEvery audio signal (input and playback) passes through a freeverb~ object\nThis creates consistent reverberation that can be adjusted and controlled\nMultiple freeverb~ objects process different stages of the audio for cumulative effects\n\n\n\n2.2.3.3 3. Enhanced Control Flow\nThe patch includes numbered controls for better workflow:\n\n\n\n\n\n\n\n\nButton\nLabel\nFunction\n\n\n\n\n1\nStart recording\nBegins recording the input source to record-A.wav\n\n\n2\nPlayback audio\nPlays the speech.wav file through reverb into the system\n\n\n3\nStop recording\nStops the current recording process\n\n\n\n\n\n2.2.3.4 4. Multiple Send/Receive Paths\nThe patch uses additional send/receive pairs to manage signal routing:\n\ns~/r~ audio.input - Routes input signals to the recording object\ns~/r~ player.A - Routes playback from file A to recorder B\ns~/r~ player.B - Routes playback from file B back to recorder A\nthrow~/catch~ audio - Collects all signals to be sent to the output\n\n\n\n\n2.2.4 Step-by-Step Explanation\n\n2.2.4.1 Step 1: Initial Audio Source Options\nThis patch provides two possible sources for the initial recording:\n\nPre-recorded file input:\n\nClick “2. Playback audio” button to play speech.wav\nThe file is played through readsf~, processed by freeverb~, and sent to s~ audio.input\n\nLive input: (not explicitly shown in this version but could be added using adc~)\n\nAny signal sent to s~ audio.input will be recorded when recording starts\n\n\n\n\n2.2.4.2 Step 2: Recording the First Generation\n\nClick “1. Start recording” to begin recording to record-A.wav\nThe signal from s~ audio.input is captured by r~ audio.input and sent to writesf~\nThe resulting file contains your source material with initial reverb applied\n\n\n\n2.2.4.3 Step 3: Playback and Re-recording\n\nAfter stopping recording with “3. Stop recording”, the patch automatically:\n\nOpens and plays record-A.wav through readsf~\nProcesses it through another freeverb~ for additional reverb\nSends it to both throw~ audio (for monitoring) and s~ player.A\nThe player.A signal is received and recorded to record-B.wav\n\n\n\n\n2.2.4.4 Step 4: Recursive Process\n\nWhen record-B.wav is created, it can be played back through another readsf~\nThis signal is also processed by freeverb~\nThe output is sent to both throw~ audio and s~ player.B\nThe player.B signal could be received and recorded back to record-A.wav\nThis cycle can continue, with each generation accumulating more of the virtual room’s characteristics\n\n\n\n2.2.4.5 Step 5: Output\n\nAll audio signals sent to throw~ audio are collected by catch~ audio\nThe mixed signal is sent to out~ for playback through speakers/headphones\n\n\n\n\n\n2.2.5 Key Objects Table\n\n\n\n\n\n\n\nObject\nPurpose in This Patch\n\n\n\n\nfreeverb~\nCreates virtual room acoustics by adding reverberation\n\n\nreadsf~\nPlays audio files (source material or previous generations)\n\n\nwritesf~\nRecords audio to file (for each generation)\n\n\ns~/r~\nRoutes audio between different parts of the patch\n\n\nthrow~/catch~\nMixes and outputs all audio signals\n\n\ndel\nAdds small delays to ensure proper sequencing of operations\n\n\nbng\nProvides buttons for user interaction\n\n\nout~\nSends audio to speakers/headphones\n\n\n\n\n\n2.2.6 Creative Applications\n\nExperiment with reverb settings: Try different room sizes, damping, and wet/dry mix\nUse different source materials: Test various speech samples, musical phrases, or sound effects\nCreate hybrid processes: Record the first generation in a real room, then switch to the virtual environment\nBuild compositional sequences: Layer different generations to create evolving textures\nCompare real vs. virtual: Process the same source through both the original and freeverb patches to contrast physical and virtual acoustics\n\nThis virtual approach offers a controlled laboratory for exploring the core concepts of Lucier’s piece, making it accessible even without an ideal acoustic space, while also opening new creative possibilities that wouldn’t be possible in a physical implementation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  },
  {
    "objectID": "chapters/playrec.html#i-am-sitting-in-a-freeverb-patch-walkthrough",
    "href": "chapters/playrec.html#i-am-sitting-in-a-freeverb-patch-walkthrough",
    "title": "2  Rec & Play",
    "section": "2.2 I am sitting in a [freeverb~] — Patch Walkthrough",
    "text": "2.2 I am sitting in a [freeverb~] — Patch Walkthrough\nThe patch I-am-sitting-in-a-room-freeverb.pd extends the original I am sitting in a room concept by introducing a virtual acoustic environment using reverb and allowing for pre-recorded audio input instead of just live microphone input. This adaptation provides more creative flexibility and consistent results compared to using a physical room.\n\n\n\nPure Data implementation of I am sitting in a room with freeverb\n\n\nThis patch is designed to simulate the recursive recording process of Lucier’s piece while allowing for more control over the sound environment. It uses the freeverb~ object to create a virtual room effect, enabling you to manipulate the acoustic characteristics of the sound without relying on a physical space.\n\n2.2.1 Conceptual Overview\nThe patch enables you to: - Use a pre-recorded audio file OR live input as your source material - Add artificial reverb to simulate room characteristics - Follow the same recursive recording process as the original - Achieve more controlled, repeatable results\n\n\n2.2.2 System Diagram\n\n\n\n\n\nflowchart TB\n  %% Top level source\n  AudioFile[Audio File]\n  \n  %% Horizontal arrangement of recorders A and B\n  subgraph PLAYBACK RECORDERS\n    direction LR\n    subgraph RecorderA [DEVICE A]\n      RecordA[writesf~ A.wav]\n      PlayA[readsf~ A.wav]\n      RecordA ==&gt; PlayA\n    end\n    \n    subgraph RecorderB [DEVICE B]\n      RecordB[writesf~ B.wav]\n      PlayB[readsf~ B.wav]\n      RecordB ==&gt; PlayB\n    end\n  end\n  \n  %% Reverb block in the middle\n  VirtualRoom[freeverb~&lt;br&gt;&lt;br&gt;Virtual&lt;br&gt;&lt;br&gt;Room's&lt;br&gt;&lt;br&gt;Reflections]\n  \n  %% Output at the bottom  \n  Output[dac~]\n  \n  %% Clear connections showing signal flow\n  AudioFile --&gt; RecordA\n  AudioFile --&gt; Output\n  \n  PlayA ==&gt; VirtualRoom\n  PlayB ==&gt; VirtualRoom\n  \n  VirtualRoom ==&gt; RecordB\n  VirtualRoom ==&gt; RecordA\n  VirtualRoom --&gt; Output\n  \n  %% Feedback path showing recursion\n  PlayB -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordA\n  PlayA -.-&gt;|\"Trigger to&lt;br&gt; start recording\"| RecordB\n\n\n\n\n\n\n\n\n2.2.3 Unique Features of the [freeverb~] Version\n\n2.2.3.1 1. Audio File Input\nUnlike the original patch that only uses microphone input, this version can:\n\nPlay a pre-recorded audio file (speech.wav) as the initial source\nProcess this file through reverb before recording\nTrigger playback with a button (labeled “2. Playback audio”)\n\n\n\n\n\n\nflowchart LR\n    bng([bng]) --&gt; msg[\"msg open speech.wav, 1\"]\n    msg --&gt; readsf[\"readsf~\"]\n    readsf --&gt; freeverb[\"freeverb~\"]\n    freeverb --&gt; send[\"s~ audio.input\"]\n\n\n\n\n\n\n\n\n2.2.3.2 2. Virtual Acoustic Environment\nInstead of relying on a physical room’s acoustics, this patch uses freeverb~ objects to create a simulated acoustic space:\n\nEvery audio signal (input and playback) passes through a freeverb~ object\nThis creates consistent reverberation that can be adjusted and controlled\nMultiple freeverb~ objects process different stages of the audio for cumulative effects\n\n\n\n2.2.3.3 3. Enhanced Control Flow\nThe patch includes numbered controls for better workflow:\n\n\n\n\n\n\n\n\nButton\nLabel\nFunction\n\n\n\n\n1\nStart recording\nBegins recording the input source to record-A.wav\n\n\n2\nPlayback audio\nPlays the speech.wav file through reverb into the system\n\n\n3\nStop recording\nStops the current recording process\n\n\n\n\n\n2.2.3.4 4. Multiple Send/Receive Paths\nThe patch uses additional send/receive pairs to manage signal routing:\n\ns~/r~ audio.input - Routes input signals to the recording object\ns~/r~ player.A - Routes playback from file A to recorder B\ns~/r~ player.B - Routes playback from file B back to recorder A\nthrow~/catch~ audio - Collects all signals to be sent to the output\n\n\n\n\n2.2.4 Step-by-Step Explanation\n\n2.2.4.1 Step 1: Initial Audio Source Options\nThis patch provides two possible sources for the initial recording:\n\nPre-recorded file input:\n\nClick “2. Playback audio” button to play speech.wav\nThe file is played through readsf~, processed by freeverb~, and sent to s~ audio.input\n\nLive input: (not explicitly shown in this version but could be added using adc~)\n\nAny signal sent to s~ audio.input will be recorded when recording starts\n\n\n\n\n2.2.4.2 Step 2: Recording the First Generation\n\nClick “1. Start recording” to begin recording to record-A.wav\nThe signal from s~ audio.input is captured by r~ audio.input and sent to writesf~\nThe resulting file contains your source material with initial reverb applied\n\n\n\n2.2.4.3 Step 3: Playback and Re-recording\n\nAfter stopping recording with “3. Stop recording”, the patch automatically:\n\nOpens and plays record-A.wav through readsf~\nProcesses it through another freeverb~ for additional reverb\nSends it to both throw~ audio (for monitoring) and s~ player.A\nThe player.A signal is received and recorded to record-B.wav\n\n\n\n\n2.2.4.4 Step 4: Recursive Process\n\nWhen record-B.wav is created, it can be played back through another readsf~\nThis signal is also processed by freeverb~\nThe output is sent to both throw~ audio and s~ player.B\nThe player.B signal could be received and recorded back to record-A.wav\nThis cycle can continue, with each generation accumulating more of the virtual room’s characteristics\n\n\n\n2.2.4.5 Step 5: Output\n\nAll audio signals sent to throw~ audio are collected by catch~ audio\nThe mixed signal is sent to out~ for playback through speakers/headphones\n\n\n\n\n\n2.2.5 Key Objects Table\n\n\n\n\n\n\n\nObject\nPurpose in This Patch\n\n\n\n\nfreeverb~\nCreates virtual room acoustics by adding reverberation\n\n\nreadsf~\nPlays audio files (source material or previous generations)\n\n\nwritesf~\nRecords audio to file (for each generation)\n\n\ns~/r~\nRoutes audio between different parts of the patch\n\n\nthrow~/catch~\nMixes and outputs all audio signals\n\n\ndel\nAdds small delays to ensure proper sequencing of operations\n\n\nbng\nProvides buttons for user interaction\n\n\nout~\nSends audio to speakers/headphones\n\n\n\n\n\n2.2.6 Creative Applications\n\nExperiment with reverb settings: Try different room sizes, damping, and wet/dry mix\nUse different source materials: Test various speech samples, musical phrases, or sound effects\nCreate hybrid processes: Record the first generation in a real room, then switch to the virtual environment\nBuild compositional sequences: Layer different generations to create evolving textures\nCompare real vs. virtual: Process the same source through both the original and freeverb patches to contrast physical and virtual acoustics\n\nThis virtual approach offers a controlled laboratory for exploring the core concepts of Lucier’s piece, making it accessible even without an ideal acoustic space, while also opening new creative possibilities that wouldn’t be possible in a physical implementation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Rec & Play</span>"
    ]
  }
]